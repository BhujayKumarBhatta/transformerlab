{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a9eba75-163c-4611-a780-9d44f78e6644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers huggingface_hub\n",
    "# !pip install tf-keras\n",
    "import os\n",
    "# Suppress specific TensorFlow warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # 3 means to filter out all INFO and WARNING logs\n",
    "import warnings\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, message='.*OUT_OF_RANGE.*')\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "import random\n",
    "import re\n",
    "import pickle\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from lr_schedular import CustomSchedule\n",
    "from transformer_encoder import TransformerEncoderV3  \n",
    "from positional_encoding import encode_pos_sin_cosine\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertTokenizerFast\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Input, Dense, Dropout, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, BinaryCrossentropy\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "from transformers import TFPreTrainedModel, BertConfig\n",
    "from transformers.utils import ModelOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ad68f8-3bde-409e-a925-bfcfca23c706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f5cf2ee-98ea-4000-a7ba-19ac8c943270",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexibleGPTDecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, d_model, dff, rate=0.1, use_bbmha=True, use_masked_softmax=False, row_mask_to_zero=False, **kwargs):\n",
    "        super(FlexibleGPTDecoderLayer, self).__init__(**kwargs)\n",
    "        self.use_bbmha = use_bbmha\n",
    "        if use_bbmha:\n",
    "            self.mha = MultiHeadAttentionV3(num_heads=num_heads, d_model=d_model, use_masked_softmax=use_masked_softmax, row_mask_to_sero=row_mask_to_zero)\n",
    "        else:\n",
    "            self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        \n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask=None):\n",
    "        # Apply multi-head attention\n",
    "        if self.use_bbmha:\n",
    "            attn_output = self.mha(x, x, x, mask=mask, use_causal_mask=True)\n",
    "        else:\n",
    "            attn_output = self.mha(x, x, use_mask=mask)\n",
    "\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # Apply residual connection after dropout\n",
    "\n",
    "        # Feed-forward network\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # Second residual connection\n",
    "\n",
    "        return out2\n",
    "\n",
    "class FlexibleGPTDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, vocab_size, max_position_encoding, rate=0.1, use_bbmha=True, use_masked_softmax=False, row_mask_to_zero=False):\n",
    "        super(FlexibleGPTDecoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(max_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [FlexibleGPTDecoderLayer(num_heads, d_model, dff, rate, use_bbmha, use_masked_softmax, row_mask_to_zero) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # Adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i, dec_layer in enumerate(self.dec_layers):\n",
    "            x = dec_layer(x, training, mask=look_ahead_mask(seq_len))\n",
    "\n",
    "        return x  # (batch_size, target_seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d488fb-5f21-4791-a5a3-48b47c3c1579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdb3bb6-3cfd-4ede-8bdc-34598491e35f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f4bdf4-831b-42fa-b321-f5e0591939b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab93ac2-9010-41e6-b53e-f11a5792c563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02780166-6570-4644-9c84-4923eb00afb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ae220f-348c-4471-a040-b2bb1fa120f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
