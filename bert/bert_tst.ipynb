{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "086051b2-2d8f-4bb7-bf24-0e4dd05fdc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import re\n",
    "sys.path.append('../src')\n",
    "import pandas as pd\n",
    "from transformer_encoder import TransformerEncoderV3  \n",
    "from positional_encoding import encode_pos_sin_cosine\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Input, Dense, Dropout, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, BinaryCrossentropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63c535dd-000d-42b6-904a-4f2d3faba92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how the dataset looks: dict_keys(['id', 'url', 'title', 'text'])\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# Load an example dataset, 'wikipedia' for English, 2020-03-01 version\n",
    "dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\")\n",
    "print('how the dataset looks:', dataset[0].keys())\n",
    "num_of_articles = 1000\n",
    "# article_texts_dataset = dataset[0]['text'][:num_of_articles]\n",
    "# # Extract text and write to a file\n",
    "# with open('input_text.txt', 'w', encoding='utf-8') as f:\n",
    "#     for article  in article_texts_dataset:\n",
    "#         # Write each Wikipedia article on a new line\n",
    "#         f.write(article.replace('\\n', ' ') + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eec8e7b0-449e-4c0f-9c78-d050fc6c4344",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words in first_article: 43985\n",
      "how the article look:\n",
      " Anarchism is a political philosophy and movement that is sceptical of authority and rejects all involuntary, coercive forms of hierarchy. Anarchism calls for the abolition of the state, which it holds to be unnecessary, undesirable, and harmful. As a historically left-wing movement, placed on the farthest left of the political spectrum, it is usually described alongside communalism and libertarian Marxism as the libertarian wing (libertarian socialism) of the socialist movement, and has a strong historical association with anti-capitalism and socialism.\n",
      "\n",
      "Humans lived in societies without formal hierarchies long before the establishment of formal states, realms, or empires. With the rise of organised hierarchical bodies, scepticism toward authority also rose. Although traces of anarchist thought are found throughout history, modern anarchism emerged from the Enlightenment. During the latter half of the 19th and the first decades of the 20th century, the anarchist movement flourished in most parts of the world and had a significant role in workers' struggles for emancipation. Various anarchist schools of thought formed during this period. Anarchists have taken part in several revolutions, most notably in the Paris Commune, the Russian Civil War and the Spanish Civil War, whose end marked the end of the classical era of anarchism. In the last decades of the 20th and into the 21st century, the anarchist movement has been resurgent once more.\n",
      "\n",
      "Anarchism employs a diversity of tact\n",
      "..........................................\n",
      "........................\n",
      "phical and political anarchism] are philosophical and political claims.\" (p. 137)\n",
      "  Anarchistic popular fiction novel.\n",
      " \n",
      " \n",
      " \n",
      "  An argument for philosophical anarchism.\n",
      "\n",
      "External links \n",
      " Anarchy Archives. Anarchy Archives is an online research center on the history and theory of anarchism.\n",
      "\n",
      " \n",
      "Anti-capitalism\n",
      "Anti-fascism\n",
      "Economic ideologies\n",
      "Left-wing politics\n",
      "Libertarian socialism\n",
      "Libertarianism\n",
      "Political culture\n",
      "Political movements\n",
      "Political ideologies\n",
      "Social theories\n",
      "Socialism\n",
      "Far-left politics\n"
     ]
    }
   ],
   "source": [
    "first_article = dataset[0]['text']\n",
    "print('words in first_article:', len(first_article))\n",
    "print('how the article look:\\n',first_article[:1500])\n",
    "print('..........................................\\n........................')\n",
    "print(first_article[-500:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5f1620-8982-4f04-97a3-edd3d029d29e",
   "metadata": {},
   "source": [
    "Document and Sentence Segmentation:\n",
    "\n",
    "Document Boundary: Each Wikipedia article can be treated as a single document. This aligns with the BERT requirement where each document is separated by blank lines.\n",
    "\n",
    "Sentence Tokenization: Use a sentence tokenizer to convert each paragraph into distinct sentences. This is crucial because BERT's NSP task assumes that two consecutive sentences in the data might be used as training pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66192cf9-6269-4a60-b20b-e9324877385b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/bhujay/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def save_articles_with_doc_boundary(dataset_name, config_name, split_name, output_file_path, num_of_articles=1000):\n",
    "    nltk.download('punkt')\n",
    "    dataset = load_dataset(dataset_name, config_name, split=split_name)\n",
    "    articles_to_process = dataset.select(range(num_of_articles))\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "        for i, article in enumerate(articles_to_process):\n",
    "            title = article.get('title', f\"No Title Available for Article {i}\")\n",
    "            art_id = article.get('id', \"No ID\")\n",
    "            art_url = article.get('url', \"No URL\")\n",
    "            sentences = nltk.sent_tokenize(article['text'])\n",
    "            full_article_text = f\"ARTICLE-{i}-{art_id}-{art_url}-{title}\\n\" + '\\n'.join(sentences) + '\\n\\n'\n",
    "            file.write(full_article_text)\n",
    "# Call the function to process and save articles\n",
    "output_file_path = 'wiki_articles_with_seperator.txt'\n",
    "save_articles_with_doc_boundary('wikipedia', '20220301.en', 'train', output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2be5185-e2e9-4851-83a6-3badc245f2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARTICLE-0-12-https://en.wikipedia.org/wiki/Anarchism-Anarchism\n",
      "\n",
      "Anarchism is a political philosophy and movement that is sceptical of authority and rejects all involuntary, coercive forms of hierarchy.\n",
      "\n",
      "Anarchism calls for the abolition of the state, which it holds to be unnecessary, undesirable, and harmful.\n",
      "\n",
      "As a historically left-wing movement, placed on the farthest left of the political spectrum, it is usually described alongside communalism and libertarian Marxism as the libertarian wing (libertarian socialism) of the socialist movement, and has a strong historical association with anti-capitalism and socialism.\n",
      "\n",
      "Humans lived in societies without formal hierarchies long before the establishment of formal states, realms, or empires.\n",
      "\n",
      "ARTICLE-1-25-https://en.wikipedia.org/wiki/Autism-Autism\n",
      "\n",
      "Autism is a neurodevelopmental disorder characterized by difficulties with social interaction and communication, and by restricted and repetitive behavior.\n",
      "\n",
      "Parents often notice signs during the first three years of their child's life.\n",
      "\n",
      "These signs often develop gradually, though some autistic children experience regression in their communication and social skills after reaching developmental milestones at a normal pace.\n",
      "\n",
      "Autism is associated with a combination of genetic and environmental factors.\n",
      "\n",
      "ARTICLE-2-39-https://en.wikipedia.org/wiki/Albedo-Albedo\n",
      "\n",
      "Albedo (; ) is the measure of the diffuse reflection of solar radiation out of the total solar radiation and measured on a scale from 0, corresponding to a black body that absorbs all incident radiation, to 1, corresponding to a body that reflects all incident radiation.\n",
      "\n",
      "Surface albedo is defined as the ratio of radiosity Je to the irradiance Ee (flux per unit area) received by a surface.\n",
      "\n",
      "The proportion reflected is not only determined by properties of the surface itself, but also by the spectral and angular distribution of solar radiation reaching the Earth's surface.\n",
      "\n",
      "These factors vary with atmospheric composition, geographic location, and time (see position of the Sun).\n",
      "\n",
      "ARTICLE-3-290-https://en.wikipedia.org/wiki/A-A\n",
      "\n",
      "A, or a, is the first letter and the first vowel of the modern English alphabet and the ISO basic Latin alphabet.\n",
      "\n",
      "Its name in English is a (pronounced ), plural aes.\n",
      "\n",
      "It is similar in shape to the Ancient Greek letter alpha, from which it derives.\n",
      "\n",
      "The uppercase version consists of the two slanting sides of a triangle, crossed in the middle by a horizontal bar.\n",
      "\n",
      "ARTICLE-4-303-https://en.wikipedia.org/wiki/Alabama-Alabama\n",
      "\n",
      "Alabama () is a state in the Southeastern region of the United States, bordered by Tennessee to the north; Georgia to the east; Florida and the Gulf of Mexico to the south; and Mississippi to the west.\n",
      "\n",
      "Alabama is the 30th largest by area and the 24th-most populous of the U.S. states.\n",
      "\n",
      "With a total of  of inland waterways, Alabama has among the most of any state.\n",
      "\n",
      "Alabama is nicknamed the Yellowhammer State, after the state bird.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def display_article_lines(file_path, num_lines=5, num_articles=5):    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        article_count = 0\n",
    "        line_count = 0\n",
    "        for line in file:\n",
    "            if line.startswith('ARTICLE-'):  # New article detected\n",
    "                if article_count >= num_articles:\n",
    "                    break                          \n",
    "                article_count += 1\n",
    "                line_count = 0  # Reset line count for the new article\n",
    "            if line_count < num_lines:\n",
    "                print(line)\n",
    "                line_count += 1\n",
    "            else:\n",
    "                continue  # Skip further lines until the next article starts\n",
    "output_file_path = 'wiki_articles_with_seperator.txt'\n",
    "display_article_lines(output_file_path, num_lines=5, num_articles=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fa7466b-b8a7-48e7-b4e0-35af92b1f186",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingInstance:\n",
    "    \"\"\"A single training instance (sentence pair).\"\"\"\n",
    "    def __init__(self, tokens, segment_ids, masked_lm_positions, masked_lm_labels, is_random_next):\n",
    "        self.tokens = tokens\n",
    "        self.segment_ids = segment_ids\n",
    "        self.masked_lm_positions = masked_lm_positions\n",
    "        self.masked_lm_labels = masked_lm_labels\n",
    "        self.is_random_next = is_random_next\n",
    "\n",
    "    def __str__(self):\n",
    "        tokens_str = \" \".join([str(token) for token in self.tokens])\n",
    "        segment_ids_str = \" \".join(map(str, self.segment_ids))\n",
    "        masked_lm_positions_str = \" \".join(map(str, self.masked_lm_positions))\n",
    "        masked_lm_labels_str = \" \".join([str(label) for label in self.masked_lm_labels])\n",
    "        return f\"Tokens: {tokens_str}\\nSegment IDs: {segment_ids_str}\\n\" \\\n",
    "               f\"Is Random Next: {self.is_random_next}\\n\" \\\n",
    "               f\"Masked LM Positions: {masked_lm_positions_str}\\n\" \\\n",
    "               f\"Masked LM Labels: {masked_lm_labels_str}\\n\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "def mask_tokens(tokens, tokenizer, max_predictions_per_seq, rng):\n",
    "    \"\"\"Masks tokens and returns masked tokens and corresponding labels.\"\"\"\n",
    "    output_tokens = tokens[:]\n",
    "    output_labels = [-1] * len(tokens)  # Initialize labels with -1 (no change)\n",
    "\n",
    "    # Determine which tokens can be masked\n",
    "    candidate_indices = [\n",
    "        i for i, token in enumerate(tokens) \n",
    "        if token not in [tokenizer.cls_token, tokenizer.sep_token]\n",
    "    ]\n",
    "    rng.shuffle(candidate_indices)\n",
    "    num_masked = min(max_predictions_per_seq, len(candidate_indices) * 15 // 100)\n",
    "    \n",
    "    for index in candidate_indices[:num_masked]:\n",
    "        random_choice = rng.random()\n",
    "        # 80% replace with [MASK], 10% random token, 10% unchanged\n",
    "        if random_choice < 0.8:\n",
    "            output_tokens[index] = tokenizer.mask_token\n",
    "        elif random_choice < 0.9:\n",
    "            output_tokens[index] = random.choice(list(tokenizer.vocab.keys()))\n",
    "        \n",
    "        output_labels[index] = tokenizer.convert_tokens_to_ids(tokens[index])\n",
    "\n",
    "    return output_tokens, output_labels\n",
    "\n",
    "def truncate_and_process(tokens_a, tokens_b, max_seq_length, tokenizer, max_predictions_per_seq, instances, rng, is_random_next):\n",
    "    # Truncate tokens_a and tokens_b if their combined length is too long\n",
    "    while len(tokens_a) + len(tokens_b) + 3 > max_seq_length:\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "    tokens = ['[CLS]'] + tokens_a + ['[SEP]'] + tokens_b + ['[SEP]']\n",
    "    segment_ids = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "    masked_tokens, masked_labels = mask_tokens(tokens, tokenizer, max_predictions_per_seq, rng)\n",
    "\n",
    "    # Convert masked_tokens to IDs\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(masked_tokens)  # Ensure this returns integers\n",
    "    instance = TrainingInstance(\n",
    "        tokens=token_ids,\n",
    "        segment_ids=segment_ids,\n",
    "        masked_lm_positions=[i for i, label in enumerate(masked_labels) if label != -1],\n",
    "        masked_lm_labels=[label for label in masked_labels if label != -1],\n",
    "        is_random_next=int(is_random_next)\n",
    "    )\n",
    "    # instance = {\n",
    "    #     'tokens': token_ids,\n",
    "    #     'segment_ids': segment_ids,\n",
    "    #     'masked_lm_positions': [i for i, label in enumerate(masked_labels) if label != -1],\n",
    "    #     'masked_lm_labels': [label for label in masked_labels if label != -1],\n",
    "    #     'is_random_next': int(is_random_next)\n",
    "    # }\n",
    "    instances.append(instance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6eeb5833-0ffe-475d-b799-3e5fa64f7f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens A: ['ana', '##rch', '##ism', 'is', 'a', 'political', 'philosophy', 'and', 'movement', 'that', 'is', 'sc', '##ept', '##ical', 'of', 'authority', 'and', 'rejects', 'all', 'involuntary', ',', 'coe', '##rc', '##ive', 'forms', 'of', 'hierarchy', '.', 'ana', '##rch', '##ism', 'calls', 'for', 'the', 'abolition', 'of', 'the', 'state', ',', 'which', 'it', 'holds', 'to', 'be', 'unnecessary', ',', 'und', '##es', '##ira', '##ble', ',', 'and', 'harmful', '.', 'as', 'a', 'historically', 'left', '-', 'wing', 'movement', ',', 'placed'], len: 63\n",
      "Tokens B: [',', 'or', 'empires', '.', 'with', 'the', 'rise', 'of', 'organised', 'hierarchical']\n",
      "Is random next: False\n",
      "\n",
      "Tokens A: [',', 'or', 'empires', '.', 'with', 'the', 'rise', 'of', 'organised', 'hierarchical', 'bodies', ',', 'sc', '##ept', '##ici', '##sm', 'toward', 'authority', 'also', 'rose', '.', 'although', 'traces', 'of', 'anarchist', 'thought', 'are', 'found', 'throughout', 'history', ',', 'modern', 'ana', '##rch', '##ism', 'emerged', 'from', 'the', 'enlightenment', '.', 'during', 'the', 'latter', 'half', 'of', 'the', '19th', 'and', 'the', 'first', 'decades', 'of', 'the', '20th', 'century', ',', 'the', 'anarchist', 'movement', 'flourished', 'in', 'most'], len: 62\n",
      "Tokens B: ['in', 'the', 'last', 'decades', 'of', 'the', '20th', 'and', 'into', 'the']\n",
      "Is random next: False\n",
      "\n",
      "Tokens A: ['in', 'the', 'last', 'decades', 'of', 'the', '20th', 'and', 'into', 'the', '21st', 'century', ',', 'the', 'anarchist', 'movement', 'has', 'been', 'res', '##urgent', 'once', 'more', '.', 'ana', '##rch', '##ism', 'employs', 'a', 'diversity', 'of', 'tactics', 'in', 'order', 'to', 'meet', 'its', 'ideal', 'ends', 'which', 'can', 'be', 'broadly', 'separated', 'into', 'revolutionary', 'and', 'evolutionary', 'tactics', ';', 'there', 'is', 'significant', 'overlap', 'between', 'the', 'two', ',', 'which', 'are', 'merely', 'descriptive', '.', 'revolutionary'], len: 63\n",
      "Tokens B: ['internally', 'inconsistent', ',', 'violent', ',', 'or', 'utopia', '##n', '.', 'etymology']\n",
      "Is random next: False\n",
      "\n",
      "Tokens A: ['internally', 'inconsistent', ',', 'violent', ',', 'or', 'utopia', '##n', '.', 'etymology', ',', 'terminology', ',', 'and', 'definition', 'the', 'et', '##ym', '##ological', 'origin', 'of', 'ana', '##rch', '##ism', 'is', 'from', 'the', 'ancient', 'greek', 'ana', '##rk', '##hia', ',', 'meaning', '\"', 'without', 'a', 'ruler', '\"', ',', 'composed', 'of', 'the', 'prefix', 'an', '-', '(', '\"', 'without', '\"', ')', 'and', 'the', 'word', 'ark', '##hos', '(', '\"', 'leader', '\"', 'or', '\"'], len: 62\n",
      "Tokens B: ['conscience', '.', 'cy', '##nic', '##s', 'dismissed', 'human', 'law', '(', 'no']\n",
      "Is random next: True\n",
      "\n",
      "Tokens A: ['many', 'views', 'with', 'later', 'anarchist', '##s', '.', 'many', 'revolutionaries', 'of', 'the', '19th', 'century', 'such', 'as', 'william', 'god', '##win', '(', '1756', '–', '1836', ')', 'and', 'wilhelm', 'wei', '##tling', '(', '1808', '–', '1871', ')', 'would', 'contribute', 'to', 'the', 'anarchist', 'doctrines', 'of', 'the', 'next', 'generation', 'but', 'did', 'not', 'use', 'anarchist', 'or', 'ana', '##rch', '##ism', 'in', 'describing', 'themselves', 'or', 'their', 'beliefs', '.', 'the', 'first', 'political', 'philosopher', 'to'], len: 63\n",
      "Tokens B: ['and', 'anarchist', '##s', 'at', 'a', 'series', 'of', 'events', 'named', 'may']\n",
      "Is random next: True\n",
      "\n",
      "Tokens A: ['united', 'states', '.', 'some', 'usage', '##s', 'of', 'libertarian', '##ism', 'refer', 'to', 'individual', '##istic', 'free', '-', 'market', 'philosophy', 'only', ',', 'and', 'free', '-', 'market', 'ana', '##rch', '##ism', 'in', 'particular', 'is', 'termed', 'libertarian', 'ana', '##rch', '##ism', '.', 'while', 'the', 'term', 'libertarian', 'has', 'been', 'largely', 'synonymous', 'with', 'ana', '##rch', '##ism', ',', 'its', 'meaning', 'has', 'more', 'recently', 'dil', '##uted', 'with', 'wider', 'adoption', 'from', 'ideological', '##ly', 'di', '##spar'], len: 63\n",
      "Tokens B: ['.', 'ana', '##rch', '##ism', 'is', 'broadly', 'used', 'to', 'describe', 'the']\n",
      "Is random next: False\n",
      "\n",
      "Tokens A: ['.', 'ana', '##rch', '##ism', 'is', 'broadly', 'used', 'to', 'describe', 'the', 'anti', '-', 'authoritarian', 'wing', 'of', 'the', 'socialist', 'movement', '.', 'ana', '##rch', '##ism', 'is', 'contrasted', 'to', 'socialist', 'forms', 'which', 'are', 'state', '-', 'oriented', 'or', 'from', 'above', '.', 'scholars', 'of', 'ana', '##rch', '##ism', 'generally', 'highlight', 'ana', '##rch', '##ism', \"'\", 's', 'socialist', 'credentials', 'and', 'critic', '##ise', 'attempts', 'at', 'creating', 'di', '##cho', '##tom', '##ies', 'between', 'the'], len: 62\n",
      "Tokens B: ['is', 'a', 'lot', 'of', 'discussion', 'among', 'scholars', 'and', 'anarchist', '##s']\n",
      "Is random next: False\n",
      "\n",
      "Tokens A: ['is', 'a', 'lot', 'of', 'discussion', 'among', 'scholars', 'and', 'anarchist', '##s', 'on', 'the', 'matter', ',', 'and', 'various', 'currents', 'perceive', 'ana', '##rch', '##ism', 'slightly', 'differently', '.', 'major', 'definition', '##al', 'elements', 'include', 'the', 'will', 'for', 'a', 'non', '-', 'coe', '##rc', '##ive', 'society', ',', 'the', 'rejection', 'of', 'the', 'state', 'apparatus', ',', 'the', 'belief', 'that', 'human', 'nature', 'allows', 'humans', 'to', 'exist', 'in', 'or', 'progress', 'toward', 'such', 'a', 'non'], len: 63\n",
      "Tokens B: ['between', 'authority', 'and', 'autonomy', 'would', 'mean', 'the', 'state', 'could', 'never']\n",
      "Is random next: True\n",
      "\n",
      "Tokens A: ['to', 'ana', '##rch', '##ism', 'in', 'the', 'ancient', 'world', 'were', 'in', 'china', 'and', 'greece', '.', 'in', 'china', ',', 'philosophical', 'ana', '##rch', '##ism', '(', 'the', 'discussion', 'on', 'the', 'legitimacy', 'of', 'the', 'state', ')', 'was', 'del', '##ine', '##ated', 'by', 'tao', '##ist', 'philosophers', 'zhu', '##ang', 'zhou', 'and', 'lao', '##zi', '.', 'alongside', 'st', '##oic', '##ism', ',', 'tao', '##ism', 'has', 'been', 'said', 'to', 'have', 'had', '\"', 'significant', 'anticipation', '##s'], len: 63\n",
      "Tokens B: ['##used', 'philosophical', 'ana', '##rch', '##ism', 'in', 'england', ',', 'morally', 'del']\n",
      "Is random next: True\n",
      "\n",
      "Tokens A: ['conscience', '.', 'cy', '##nic', '##s', 'dismissed', 'human', 'law', '(', 'no', '##mos', ')', 'and', 'associated', 'authorities', 'while', 'trying', 'to', 'live', 'according', 'to', 'nature', '(', 'ph', '##ysis', ')', '.', 'st', '##oic', '##s', 'were', 'supportive', 'of', 'a', 'society', 'based', 'on', 'unofficial', 'and', 'friendly', 'relations', 'among', 'its', 'citizens', 'without', 'the', 'presence', 'of', 'a', 'state', '.', 'in', 'medieval', 'europe', ',', 'there', 'was', 'no', 'anarchist', '##ic', 'activity', 'except', 'some'], len: 63\n",
      "Tokens B: ['community', 'and', 'individual', '##ity', 'sets', 'it', 'apart', 'from', 'ana', '##rch']\n",
      "Is random next: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_bert_pretraining_instances_in_chunks(file_path, chunk_size=1048576, \n",
    "                          doc_boundary_pattern=r'ARTICLE-\\d+-\\d+-https:\\/\\/\\S+',\n",
    "                         test_print=10, max_seq_length=128, max_predictions_per_seq=20, \n",
    "                         dupe_factor=5, random_seed=12345, nsp_enabled=True):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    rng = random.Random(random_seed)\n",
    "    buffer = ''\n",
    "    instances = []    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        while True:\n",
    "            chunk = file.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            buffer += chunk\n",
    "            documents = re.split(doc_boundary_pattern, buffer, flags=re.MULTILINE)            \n",
    "            if documents and not re.match(doc_boundary_pattern, documents[-1]):\n",
    "                buffer = documents.pop()\n",
    "            else:\n",
    "                buffer = ''            \n",
    "            for doc in documents:\n",
    "                if not doc.strip():\n",
    "                    continue\n",
    "                tokenized_doc = tokenizer.tokenize(doc)\n",
    "                sequences = [tokenized_doc[i:i+max_seq_length] for i in range(0, len(tokenized_doc), max_seq_length)]                \n",
    "                for j in range(len(sequences) - 1):\n",
    "                    tokens_a = sequences[j]\n",
    "                    if rng.random() > 0.5 or not nsp_enabled:\n",
    "                        is_random_next = True\n",
    "                        tokens_b = sequences[rng.randint(0, len(sequences) - 1)]\n",
    "                    else:\n",
    "                        is_random_next = False\n",
    "                        tokens_b = sequences[j + 1]\n",
    "\n",
    "                    truncate_and_process(tokens_a, tokens_b, max_seq_length, tokenizer, max_predictions_per_seq, instances, rng, is_random_next)\n",
    "\n",
    "                    if test_print > 0:\n",
    "                        print(f\"Tokens A: {tokens_a}, len: {len(tokens_a)}\")\n",
    "                        print(f\"Tokens B: {tokens_b[:10]}\")\n",
    "                        print(f\"Is random next: {is_random_next}\\n\")\n",
    "                        test_print -= 1\n",
    "                        if test_print == 0:\n",
    "                            return instances\n",
    "    return instances  # Return all instances for further processing or training\n",
    "file_path = 'wiki_articles_with_seperator.txt'\n",
    "res = create_bert_pretraining_instances_in_chunks(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c44c0535-65cd-4d1b-8fd4-72937bd3b4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 101 9617 11140 2964 2003 103 2576 4695 103 2929 2008 2003 8040 23606 7476 1997 3691 1998 103 2035 26097 1010 24873 11890 3512 3596 4090 12571 1012 9617 11140 2964 4455 2005 1996 15766 1997 1996 2110 1010 103 2009 4324 2000 2022 14203 1010 103 2229 7895 3468 1010 1998 17631 1012 2004 1037 7145 2187 1011 3358 30221 1010 2872 102 1010 2030 23560 1012 2007 1996 4125 1997 7362 25835 103 1010 8040 23606 103 6491 2646 3691 2036 3123 1012 103 103 1997 103 2245 2024 2179 103 103 1010 2715 9617 11140 2964 6003 2013 1996 16724 1012 2076 1996 3732 2431 1997 17571 3708 1998 1996 2034 5109 1997 1996 3983 2301 1010 1996 18448 103 17893 1999 2087 102\n",
      "Segment IDs: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "Is Random Next: 0\n",
      "Masked LM Positions: 5 8 18 26 40 47 51 61 63 75 79 86 87 89 93 94 110 123\n",
      "Masked LM Labels: 1037 1998 19164 1997 2029 6151 1010 2929 2872 4230 28775 2348 10279 18448 2802 2381 1996 2929\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1dee5bc5-7c45-4723-91f8-b7d40e1998de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_instances_as_parquet(instances, file_path):\n",
    "    data = [instance.__dict__ for instance in instances]  # Convert instance objects to dictionaries\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_parquet(file_path, engine='pyarrow')  # Save as Parquet\n",
    "\n",
    "# Example usage\n",
    "instances = create_bert_pretraining_instances_in_chunks(file_path, test_print=0)\n",
    "pres = save_instances_as_parquet(instances, 'pretraining_bert_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a8022e-e523-4054-bfb6-56c671635e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aeddd2-64f5-49c8-b563-224d66e4ffac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ecd859cf-88b2-4da8-9eaa-b71c0cb1ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert these articles to pretraining data \n",
    "# !python create_pretraining_data.py --vocab_file vocab.txt --input_text input_text.txt --output_tfrecord output.tfrecord --do_lower_case --nsp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d246b53-8a4b-4e74-b802-da85d3f58f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 30000  # Smaller vocabulary size for simplicity\n",
    "num_layers = 2  # Fewer layers\n",
    "d_model = 256  # Smaller dimensionality\n",
    "num_heads = 4\n",
    "dff = 512\n",
    "segment_size = 2\n",
    "max_seq_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8669e1e8-12af-429a-93db-afd3470d8205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-12 17:40:22.878046: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "def load_and_print_dataset(filepath):\n",
    "    raw_dataset = tf.data.TFRecordDataset(filepath)\n",
    "    for i, raw_record in enumerate(raw_dataset):  # Adjust the number based on how many you want to check\n",
    "        # print(\"Raw record:\", raw_record.numpy())\n",
    "        try:\n",
    "            example = tf.io.parse_single_example(\n",
    "                raw_record,\n",
    "                {\n",
    "                    # 'input_ids': tf.io.FixedLenFeature([128], tf.int64),\n",
    "                    'input_ids': tf.io.VarLenFeature(tf.int64),\n",
    "                    'segment_ids': tf.io.VarLenFeature(tf.int64),\n",
    "                    'masked_lm_positions': tf.io.VarLenFeature(tf.int64),\n",
    "                    'masked_lm_labels': tf.io.VarLenFeature(tf.int64),\n",
    "                    'next_sentence_labels': tf.io.FixedLenFeature([], tf.int64)\n",
    "                }\n",
    "            )\n",
    "            # print(\"Parsed example:\", example)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to parse record {i}: {e}\")\n",
    "            print(\"Raw record:\", raw_record.numpy())\n",
    "            break  # Or continue based on how you want to handle errors\n",
    "\n",
    "# Example usage\n",
    "load_and_print_dataset('output.tfrecord')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b9ee814-a624-41e5-a3e4-3bd2f5b1d416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfrecord(serialized_example):\n",
    "    feature_description = {\n",
    "        'input_ids': tf.io.VarLenFeature(tf.int64),\n",
    "        'segment_ids': tf.io.VarLenFeature(tf.int64),\n",
    "        'masked_lm_positions': tf.io.VarLenFeature(tf.int64),\n",
    "        'masked_lm_labels': tf.io.VarLenFeature(tf.int64),\n",
    "        'next_sentence_labels': tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    # print(serialized_example)\n",
    "    example = tf.io.parse_single_example(serialized_example, feature_description)\n",
    "    # print(example)\n",
    "    input_ids = tf.cast(example['input_ids'], tf.int32)\n",
    "    input_ids = tf.sparse.to_dense(input_ids)\n",
    "    segment_ids = tf.cast(example['segment_ids'], tf.int32)\n",
    "    segment_ids = tf.sparse.to_dense(segment_ids)\n",
    "    masked_lm_positions = tf.sparse.to_dense(example['masked_lm_positions'])\n",
    "    masked_lm_labels = tf.sparse.to_dense(example['masked_lm_labels'])\n",
    "    next_sentence_labels = tf.cast(example['next_sentence_labels'], tf.int32)\n",
    "    inputs = {'input_ids': input_ids, 'segment_ids': segment_ids}\n",
    "    labels = {'masked_lm_positions': masked_lm_positions,\n",
    "              'mlm_labels': masked_lm_labels, 'nsp_labels': next_sentence_labels}\n",
    "    return (inputs, labels)\n",
    "\n",
    "def load_dataset(filepath, batch_size):\n",
    "    raw_dataset = tf.data.TFRecordDataset(filepath)\n",
    "    parsed_dataset = raw_dataset.map(parse_tfrecord)\n",
    "    # Define padding shapes for each component of the dataset\n",
    "    padded_shapes = ({\n",
    "        'input_ids': [None],  # Dynamic padding for input_ids\n",
    "        'segment_ids': [None]  # Dynamic padding for segment_ids\n",
    "    }, {\n",
    "        'masked_lm_positions': [None],  # Dynamic padding for positions\n",
    "        'mlm_labels': [None],  # Dynamic padding for mlm labels\n",
    "        'nsp_labels': []  # No padding needed for scalar labels\n",
    "    })\n",
    "\n",
    "    # Use padded_batch to handle variable sequence lengths\n",
    "    batched_dataset = parsed_dataset.padded_batch(batch_size, padded_shapes=padded_shapes)\n",
    "    # batched_dataset = parsed_dataset.batch(batch_size)    \n",
    "    return batched_dataset\n",
    "\n",
    "# Usage\n",
    "batch_size = 32\n",
    "train_dataset = load_dataset('output.tfrecord', batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84d75159-e176-4d89-80a9-fa6b52aeec18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(32, 128), dtype=int32, numpy=\n",
      "array([[  101,  9617, 11140, ...,  2591,  4813,   102],\n",
      "       [  101, 19465,  2003, ...,   103,  1997,   102],\n",
      "       [  101,  2632, 28759, ...,  3774,  1997,   102],\n",
      "       ...,\n",
      "       [  101,   103, 11332, ..., 12626,  1997,   102],\n",
      "       [  101,  1996, 16951, ...,  1996,  7842,   102],\n",
      "       [  101, 17694, 15396, ...,   103, 24988,   102]], dtype=int32)>, 'segment_ids': <tf.Tensor: shape=(32, 128), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 1, 1, 1],\n",
      "       [0, 0, 0, ..., 1, 1, 1],\n",
      "       [0, 0, 0, ..., 1, 1, 1],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 1, 1, 1],\n",
      "       [0, 0, 0, ..., 1, 1, 1],\n",
      "       [0, 0, 0, ..., 1, 1, 1]], dtype=int32)>}, {'masked_lm_positions': <tf.Tensor: shape=(32, 18), dtype=int64, numpy=\n",
      "array([[  5,   8,  18,  26,  40,  47,  51,  61,  63,  75,  79,  86,  87,\n",
      "         89,  93,  94, 110, 123],\n",
      "       [ 19,  22,  31,  33,  37,  45,  46,  49,  56,  61,  64,  85,  95,\n",
      "         98, 108, 111, 112, 125],\n",
      "       [  3,   4,  12,  14,  18,  37,  51,  58,  60,  70,  76,  91, 106,\n",
      "        107, 111, 112, 117, 120],\n",
      "       [ 23,  26,  34,  38,  44,  47,  52,  60,  66,  71,  76,  77,  79,\n",
      "         89,  98,  99, 106, 116],\n",
      "       [  2,   5,   6,  11,  30,  35,  39,  45,  56,  82,  83,  86,  91,\n",
      "         95,  99, 113, 114, 125],\n",
      "       [  5,   8,  13,  15,  17,  23,  33,  36,  38,  39,  44,  51,  77,\n",
      "         78,  80,  87, 101, 102],\n",
      "       [  7,   9,  27,  34,  39,  52,  59,  69,  74,  77,  85,  94,  96,\n",
      "        107, 112, 113, 114, 125],\n",
      "       [  3,   4,   6,  23,  29,  34,  35,  50,  51,  69,  72,  74,  75,\n",
      "         81,  86, 107, 108, 113],\n",
      "       [  4,   5,  14,  23,  29,  48,  54,  69,  75,  82,  94, 100, 104,\n",
      "        108, 109, 117, 120, 126],\n",
      "       [ 13,  15,  16,  29,  31,  34,  47,  66,  76,  79,  85,  89, 100,\n",
      "        104, 105, 107, 108, 109],\n",
      "       [ 12,  17,  35,  48,  49,  59,  63,  70,  72,  84,  95,  98, 100,\n",
      "        102, 116, 118, 121, 123],\n",
      "       [  2,   7,  15,  16,  27,  44,  51,  73,  80,  84,  90, 101, 102,\n",
      "        106, 114, 119, 121, 122],\n",
      "       [ 16,  20,  21,  22,  45,  48,  49,  58,  77,  80,  81,  84,  85,\n",
      "         91, 104, 115, 116, 118],\n",
      "       [ 12,  23,  33,  37,  40,  42,  49,  50,  61,  62,  72,  74,  79,\n",
      "         83,  92, 105, 116, 117],\n",
      "       [ 21,  33,  44,  54,  58,  62,  63,  65,  71,  72,  73,  87,  92,\n",
      "         93, 108, 113, 119, 125],\n",
      "       [  6,  10,  11,  21,  31,  36,  39,  40,  47,  56,  67,  71,  74,\n",
      "         88,  95, 112, 116, 118],\n",
      "       [  9,  12,  19,  20,  27,  30,  39,  46,  49,  57,  79,  83,  84,\n",
      "         87,  98, 104, 109, 119],\n",
      "       [ 21,  27,  35,  43,  49,  50,  52,  57,  69,  81,  84, 102, 105,\n",
      "        108, 111, 113, 118, 124],\n",
      "       [ 17,  18,  28,  44,  46,  48,  56,  62,  66,  69,  77,  81,  85,\n",
      "         88,  92, 106, 123, 126],\n",
      "       [  2,  12,  17,  19,  20,  28,  34,  40,  42,  60,  77,  81,  93,\n",
      "         99, 101, 107, 114, 115],\n",
      "       [ 16,  30,  37,  45,  51,  54,  65,  71,  83,  84,  95,  97,  99,\n",
      "        103, 107, 110, 113, 126],\n",
      "       [  9,  11,  13,  20,  38,  48,  54,  66,  67,  72,  85,  86,  92,\n",
      "         97, 103, 110, 111, 113],\n",
      "       [  6,   7,  33,  45,  48,  52,  56,  58,  63,  67,  71,  74,  78,\n",
      "         88, 112, 114, 115, 119],\n",
      "       [  2,   4,  17,  20,  30,  32,  36,  38,  39,  40,  43,  50,  67,\n",
      "         70,  87,  99, 115, 118],\n",
      "       [  3,   6,   7,  14,  15,  21,  48,  52,  56,  60,  62,  66,  69,\n",
      "         71,  78, 108, 111, 122],\n",
      "       [  2,  12,  19,  32,  41,  43,  46,  47,  69,  73,  77,  83,  88,\n",
      "         94, 103, 107, 118, 120],\n",
      "       [  1,   2,   4,   5,   7,  20,  50,  57,  63,  85,  86,  98,  99,\n",
      "        107, 113, 115, 121, 125],\n",
      "       [  6,   8,  14,  16,  17,  23,  32,  40,  54,  73,  87,  96,  99,\n",
      "        100, 102, 104, 106, 111],\n",
      "       [  5,   8,   9,  15,  16,  17,  19,  24,  48,  53,  60,  64,  66,\n",
      "         75,  79,  80,  90, 117],\n",
      "       [  1,  12,  14,  20,  21,  26,  27,  38,  52,  59,  84,  88,  92,\n",
      "        100, 101, 108, 115, 121],\n",
      "       [  3,   4,  12,  20,  37,  38,  48,  50,  58,  60,  66,  84,  86,\n",
      "         87,  94, 110, 114, 117],\n",
      "       [  5,   9,  13,  25,  32,  49,  52,  59,  70,  74,  85,  90, 108,\n",
      "        117, 118, 119, 125, 126]])>, 'mlm_labels': <tf.Tensor: shape=(32, 18), dtype=int64, numpy=\n",
      "array([[ 1037,  1998, 19164,  1997,  2029,  6151,  1010,  2929,  2872,\n",
      "         7356,  2591,  7775,  1998,  5248,  5060,  5751,  4503,  4807],\n",
      "       [ 1010,  7775,  2076,  2034,  2037,  2411,  4503,  2295, 26237,\n",
      "         2591,  2632,  1998,  1037,  2008,  7978,  2303,  2008,  6463],\n",
      "       [ 1006,  1025,  9185,  5943,  1996,  2015,  2035,  2003,  2004,\n",
      "         2003,  2034,  2171,  2714,  1999,  3418,  3306,  2029,  1012],\n",
      "       [ 3763,  2049,  1007,  2015,  4338,  3418,  2013,  2544,  1997,\n",
      "         1516,  2124,  2036,  2632, 23848,  2923,  2013,  2573, 24268],\n",
      "       [ 1006,  1037,  2110,  1997,  1996,  1996,  5900,  2003, 20151,\n",
      "         2006,  1996,  3167,  7526,  2019,  3656,  1997,  1996,  4322],\n",
      "       [23167,  2030,  1007,  1037,  1997,  4602,  2430, 11525,  1055,\n",
      "         6335,  1996,  1996,  6725,  1007,  2019,  2004,  6725,  1012],\n",
      "       [ 1010,  1516,  1997,  2010,  5367,  1996,  1010,  9834,  2549,\n",
      "         4647, 18900,  1012,  2011,  2819,  3686,  4588,  2082,  1012],\n",
      "       [ 1025, 10488,  9834,  2232,  3418, 18858,  1010,  4588,  2082,\n",
      "         1024,  2012,  1010,  2019,  2070,  1996,  1037,  2373,  1006],\n",
      "       [ 3000,  2003,  4543,  2001, 25600,  4776, 25600,  2190,  2396,\n",
      "         1005,  2000,  2005,  2914,  2689,  4504,  1997,  1997,  1006],\n",
      "       [ 1999,  1012,  1996,  2904,  2049,  1999,  1996,  2982,  2982,\n",
      "         1998,  3068,  5240,  1996,  1012,  2445,  2011,  1996,  2914],\n",
      "       [ 2024,  4087,  2982,  4367,  3861,  2982,  5038, 13245,  2003,\n",
      "        21151,  2377,  1054,  2011,  2361,  1010,  2035,  2011,  1012],\n",
      "       [ 1006,  1007,  2143,  2550,  2400, 26573,  5889,  2001,  2001,\n",
      "         1010,  2429, 17365,  2581,  4242,  2003,  3273,  1998,  4695],\n",
      "       [ 2761,  1010,  2628,  2011,  4969,  2569,  8597,  2727,  1037,\n",
      "        11718,  9593,  3115,  2241,  1997,  7142,  1996,  4054,  6648],\n",
      "       [ 2003,  1996,  1055,  2009,  7142,  1997,  2009,  2003,  4964,\n",
      "        16396,  3218,  5142,  2529,  4176,  3430,  1037,  1012,  2174],\n",
      "       [ 1010,  2009,  7814,  2174,  1006,  5142,  8137,  1999,  2019,\n",
      "         9896,  1006,  2000,  3563,  3471,  1998,  2437, 13792,  2015],\n",
      "       [ 2141,  5740, 10736,  1020,  2171,  1007,  1037,  2845,  1012,\n",
      "         4975,  2003,  7621, 16744,  1037,  2181,  2030,  4066,  4198],\n",
      "       [ 4006,  1037, 13702,  1010,  2817, 11208,  2002,  2139,  1045,\n",
      "         2118,  4053,  4655,  1012,  5320,  2036, 21434,  2030,  3732],\n",
      "       [ 3261,  1011,  1010,  2632,  4361,  1010,  1010,  1010,  2111,\n",
      "         1996,  2497,  1999,  2088,  2003,  1996,  2011,  2264,  4643],\n",
      "       [ 1996, 23848,  2003,  2003,  2000,  4794, 12917, 16842,  1037,\n",
      "         3494,  3117,  2350,  2024,  2013, 21989,  3117, 15758,  2014],\n",
      "       [ 2003,  1055,  1012,  3494,  1996,  1012,  4830, 10191,  1996,\n",
      "        15758,  1010,  8578,  2627,  1012, 12795,  2096,  2164, 17606],\n",
      "       [ 1010,  1010, 12795, 12795, 17606,  1012, 22703,  3567,  2658,\n",
      "         5093,  3570,  2014,  1997,  5093,  2012,  1997,  1010,  2028],\n",
      "       [ 2005,  1007,  1037,  2492,  2109,  1997,  4910,  9593,  2051,\n",
      "         1996,  2241,  2006,  5372,  1055,  1037, 11679,  3823,  2009],\n",
      "       [ 5640,  1024, 11020,  1010,  2088,  1012,  2433,  2632, 18470,\n",
      "         1006,  2137,  1007,  4800, 15187,  2891,  2003,  2241,  4174],\n",
      "       [ 3952,  2000,  2003,  2120,  2335,  2162, 14482,  1010,  2166,\n",
      "         2029, 21754,  3334,  2080,  1025,  3619,  1006,  9516,  3417],\n",
      "       [ 2003,  1999,  1996,  2913,  2006,  2648,  2011,  1007,  1012,\n",
      "         2030, 26357,  6895,  1007, 12066,  8989,  1011,  2024,  3176],\n",
      "       [ 6895,  2005, 17181,  7588,  2087,  2839, 11683,  2024,  3146,\n",
      "         2163,  2036,  2660,  2710,  4561,  4904,  5899,  1040,  1051],\n",
      "       [ 5899,  2003,  3007,  1997,  1999,  2660,  2226,  1051,  4290,\n",
      "        10808,  1037,  1007,  1010, 12702,  3430,  4776, 13729, 23569],\n",
      "       [ 2029,  2024,  4871,  1999,  3151,  2030,  8697,  1012,  1039,\n",
      "         4556,  1997,  1037, 21383,  1010,  1998,  1010,  1998,  7870],\n",
      "       [ 1996,  1999,  4556,  3306,  1998,  3142,  1012,  1996,  1010,\n",
      "         1010,  1997,  7213, 12943,  3359,  2137,  2280,  2019, 12943],\n",
      "       [ 7213,  3359,  2003,  1012,  1015,  2003,  2019, 12968,  8504,\n",
      "         2034,  2549,  1997, 16444,  4958, 26013,  1007,  3656, 13710],\n",
      "       [15396,  4588, 19472,  8240,  1010,  7269,  2105,  2454,  1997,\n",
      "         4155, 15396, 20604,  4100,  3406, 22123,  9197,  1010,  1010],\n",
      "       [17694,  1010, 21358, 10654, 11219,  4021,  3088,  3033,  1997,\n",
      "         2003, 21988,  2789,  2042,  2011,  1996,  4175,  2620,  1010]])>, 'nsp_labels': <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
      "array([0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,\n",
      "       1, 1, 0, 0, 1, 0, 0, 1, 0, 0], dtype=int32)>})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "single_test_instance = iter(train_dataset.take(1)).next()\n",
    "single_input_tuple = single_test_instance[0]['input_ids'], single_test_instance[0]['segment_ids']\n",
    "print(single_test_instance)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed3d1efc-c514-4806-9707-3be670c4fc30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (32, 128, 256)\n",
      "Output shape: tf.Tensor(\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]], shape=(32, 128), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "class PositionalAndSegmentEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, segment_size, d_model, max_pos=2048, pos_dropout=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)  # Initialize the superclass (Layer)\n",
    "        self.d_model = d_model  # Store the dimensionality of the model embeddings\n",
    "        self.token_embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.segment_embedding = tf.keras.layers.Embedding(segment_size, d_model)\n",
    "        self.pos_encoding = encode_pos_sin_cosine(max_pos, d_model, debug=False)\n",
    "        self.dropout = tf.keras.layers.Dropout(pos_dropout)\n",
    "\n",
    "    def compute_mask(self, inputs, *args, **kwargs):\n",
    "        # Assuming the input structure is a tuple of (tokens, segments)\n",
    "        token_inputs, _ = inputs\n",
    "        return self.token_embedding.compute_mask(token_inputs, *args, **kwargs)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # Expect inputs to be a tuple (token_inputs, segment_inputs)\n",
    "        token_inputs, segment_inputs = inputs\n",
    "        tokens = self.token_embedding(token_inputs)  # Token embeddings\n",
    "        segments = self.segment_embedding(segment_inputs)  # Segment embeddings       \n",
    "        x = tokens + segments\n",
    "        # Scale the embeddings by the square root of the embedding dimension size\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        # Add positional encoding to the combined embeddings, sliced to match the input length\n",
    "        length = tf.shape(x)[1]\n",
    "        # pos_encodings = tf.reshape(self.pos_encoding, (1, -1, self.d_model))[:, :length, :]\n",
    "        pos_encodings = tf.cast(tf.reshape(self.pos_encoding, (1, -1, self.d_model))[:, :tf.shape(x)[1], :], tf.float32)\n",
    "        x += pos_encodings\n",
    "        x = self.dropout(x, training=training)\n",
    "        return x\n",
    "\n",
    "embedding_layer = PositionalAndSegmentEmbedding(vocab_size=vocab_size, segment_size=2, d_model=256)\n",
    "\n",
    "# Extract a single batch from the dataset\n",
    "for inputs, labels in train_dataset.take(1):\n",
    "    # The inputs dictionary contains 'input_ids' and 'segment_ids'\n",
    "    input_ids = inputs['input_ids']\n",
    "    segment_ids = inputs['segment_ids']\n",
    "\n",
    "    # Call the embedding layer\n",
    "    embeddings = embedding_layer((input_ids, segment_ids))\n",
    "\n",
    "    # Print the output shape\n",
    "    print(\"Output shape:\", embeddings.shape)\n",
    "    print(\"Output shape:\", embeddings._keras_mask)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0ee2176-1307-4de3-b836-7f4762537010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 128, 256)\n"
     ]
    }
   ],
   "source": [
    "class TransformerEncoderV4(TransformerEncoderV3):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, vocab_size, segment_size, max_pos=2048, pos_dropout=0.1, **kwargs):\n",
    "        super(TransformerEncoderV4, self).__init__(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff, vocab_size=vocab_size, max_pos=max_pos, **kwargs)\n",
    "        # Use the custom embedding layer that handles tokens, segments, and positional encodings\n",
    "        self.embedding_layer = PositionalAndSegmentEmbedding(vocab_size, segment_size, d_model, max_pos, pos_dropout)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_ids, segment_ids = inputs\n",
    "        # The embedding layer now handles everything including token, segment, and positional embeddings\n",
    "        x = self.embedding_layer((input_ids, segment_ids), training=training)\n",
    "        x = self.enc_layers_0(x, training=training)\n",
    "        for i in range(self.remaining_layers):\n",
    "            x = self.enc_layers[i](x, training=training)\n",
    "        return x\n",
    "tren = TransformerEncoderV4(num_layers, d_model, num_heads, dff, vocab_size, segment_size, max_pos=max_seq_length)\n",
    "\n",
    "encoder_out = tren(single_input_tuple)\n",
    "print(encoder_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "028da786-f0f6-4280-8228-4d3b80ce27cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mlm_output': <tf.Tensor: shape=(32, 128, 30000), dtype=float32, numpy=\n",
      "array([[[3.8340859e-05, 3.7446305e-05, 2.8395381e-05, ...,\n",
      "         2.6639331e-05, 3.7298159e-05, 3.4331169e-05],\n",
      "        [3.5365043e-05, 3.5282104e-05, 3.7169062e-05, ...,\n",
      "         2.2325734e-05, 3.5428879e-05, 3.0616156e-05],\n",
      "        [3.5435303e-05, 3.2537922e-05, 3.5552261e-05, ...,\n",
      "         2.3641700e-05, 3.7744659e-05, 3.3079072e-05],\n",
      "        ...,\n",
      "        [3.8897797e-05, 3.2174790e-05, 3.5057983e-05, ...,\n",
      "         2.0151998e-05, 3.3009997e-05, 3.7184676e-05],\n",
      "        [3.8864127e-05, 2.8284470e-05, 3.2879325e-05, ...,\n",
      "         2.0593974e-05, 3.2062126e-05, 4.0074581e-05],\n",
      "        [4.0870749e-05, 3.0497569e-05, 3.2746706e-05, ...,\n",
      "         1.9698917e-05, 3.7209582e-05, 3.7094494e-05]],\n",
      "\n",
      "       [[3.8267041e-05, 3.6553694e-05, 2.8442262e-05, ...,\n",
      "         2.6696673e-05, 3.8556202e-05, 3.5093311e-05],\n",
      "        [3.3343262e-05, 3.7650851e-05, 3.7500002e-05, ...,\n",
      "         2.4165498e-05, 3.2304510e-05, 3.7988360e-05],\n",
      "        [3.5531571e-05, 3.4948869e-05, 3.2075324e-05, ...,\n",
      "         2.1441134e-05, 3.7092130e-05, 3.3637887e-05],\n",
      "        ...,\n",
      "        [3.8908034e-05, 3.2001451e-05, 3.0565996e-05, ...,\n",
      "         2.1982078e-05, 3.3278833e-05, 3.5821147e-05],\n",
      "        [4.1329899e-05, 3.2053867e-05, 3.2572167e-05, ...,\n",
      "         2.2254239e-05, 3.4770706e-05, 3.6197362e-05],\n",
      "        [4.1736821e-05, 3.1131854e-05, 3.3270349e-05, ...,\n",
      "         1.9692614e-05, 3.6545713e-05, 3.7423240e-05]],\n",
      "\n",
      "       [[3.8384944e-05, 3.7300528e-05, 2.7708664e-05, ...,\n",
      "         2.6305384e-05, 3.7537651e-05, 3.3667657e-05],\n",
      "        [4.1632200e-05, 3.9185899e-05, 3.0581661e-05, ...,\n",
      "         2.4036859e-05, 3.3238783e-05, 3.0437408e-05],\n",
      "        [3.4502766e-05, 3.4785950e-05, 3.1502605e-05, ...,\n",
      "         2.3415003e-05, 3.5155812e-05, 3.2552554e-05],\n",
      "        ...,\n",
      "        [4.3554519e-05, 3.0483128e-05, 2.9599249e-05, ...,\n",
      "         2.2188186e-05, 3.4779139e-05, 4.1666441e-05],\n",
      "        [4.0897674e-05, 3.1536409e-05, 3.1543219e-05, ...,\n",
      "         2.1879205e-05, 3.4441269e-05, 3.5778747e-05],\n",
      "        [4.2222469e-05, 3.0489366e-05, 3.2068663e-05, ...,\n",
      "         1.9387504e-05, 3.6191577e-05, 3.7251746e-05]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[4.0369618e-05, 3.6432561e-05, 2.7565522e-05, ...,\n",
      "         2.5920044e-05, 3.7305486e-05, 3.3638342e-05],\n",
      "        [3.7296966e-05, 3.4359044e-05, 2.9409130e-05, ...,\n",
      "         2.5062487e-05, 3.4500139e-05, 2.9945128e-05],\n",
      "        [4.2680727e-05, 3.1275235e-05, 2.8359695e-05, ...,\n",
      "         2.5282194e-05, 3.9887571e-05, 3.1117946e-05],\n",
      "        ...,\n",
      "        [4.0022253e-05, 3.1457053e-05, 3.2297379e-05, ...,\n",
      "         2.2535862e-05, 3.2225715e-05, 3.7428927e-05],\n",
      "        [4.1111543e-05, 3.1459767e-05, 3.1317588e-05, ...,\n",
      "         2.2015029e-05, 3.4947137e-05, 3.5476398e-05],\n",
      "        [4.2125215e-05, 3.0098117e-05, 3.1983309e-05, ...,\n",
      "         1.9385750e-05, 3.6863250e-05, 3.7321373e-05]],\n",
      "\n",
      "       [[3.9081766e-05, 3.6956069e-05, 2.7707294e-05, ...,\n",
      "         2.6533280e-05, 3.8100632e-05, 3.3457203e-05],\n",
      "        [4.0066912e-05, 3.3975768e-05, 3.0732012e-05, ...,\n",
      "         2.5087022e-05, 3.5351644e-05, 3.1470379e-05],\n",
      "        [3.9770755e-05, 3.3331078e-05, 3.2376174e-05, ...,\n",
      "         2.5248119e-05, 3.1461521e-05, 2.8341643e-05],\n",
      "        ...,\n",
      "        [4.1766787e-05, 2.9246972e-05, 3.1107375e-05, ...,\n",
      "         2.1901249e-05, 3.4004643e-05, 3.9108039e-05],\n",
      "        [4.0607236e-05, 3.0588966e-05, 3.2712895e-05, ...,\n",
      "         2.0346504e-05, 3.3356078e-05, 3.9278093e-05],\n",
      "        [4.1660398e-05, 3.0389723e-05, 3.2573927e-05, ...,\n",
      "         1.9765024e-05, 3.7451824e-05, 3.6718247e-05]],\n",
      "\n",
      "       [[3.9453593e-05, 3.7783735e-05, 2.8366981e-05, ...,\n",
      "         2.6708973e-05, 3.7036432e-05, 3.3726810e-05],\n",
      "        [3.4346514e-05, 3.1883330e-05, 3.0936513e-05, ...,\n",
      "         2.3902670e-05, 3.4056371e-05, 3.3554687e-05],\n",
      "        [3.6541449e-05, 3.6453835e-05, 3.5595604e-05, ...,\n",
      "         2.4008288e-05, 3.7813763e-05, 3.4381275e-05],\n",
      "        ...,\n",
      "        [3.8372822e-05, 3.0996394e-05, 2.9803323e-05, ...,\n",
      "         2.1743881e-05, 3.3167766e-05, 3.6474266e-05],\n",
      "        [3.7692094e-05, 3.1488395e-05, 3.5858080e-05, ...,\n",
      "         1.8802806e-05, 3.5655325e-05, 3.8000591e-05],\n",
      "        [4.1824500e-05, 3.0451160e-05, 3.2356467e-05, ...,\n",
      "         1.9408011e-05, 3.6730191e-05, 3.7507070e-05]]], dtype=float32)>, 'nsp_output': <tf.Tensor: shape=(32, 1), dtype=float32, numpy=\n",
      "array([[0.355136  ],\n",
      "       [0.3213798 ],\n",
      "       [0.35239238],\n",
      "       [0.314295  ],\n",
      "       [0.36076358],\n",
      "       [0.33442888],\n",
      "       [0.32500687],\n",
      "       [0.34099397],\n",
      "       [0.35146984],\n",
      "       [0.33107892],\n",
      "       [0.32160512],\n",
      "       [0.30575266],\n",
      "       [0.3073671 ],\n",
      "       [0.42170575],\n",
      "       [0.35086858],\n",
      "       [0.31313616],\n",
      "       [0.33483458],\n",
      "       [0.3903778 ],\n",
      "       [0.3403566 ],\n",
      "       [0.31576318],\n",
      "       [0.34178865],\n",
      "       [0.36401507],\n",
      "       [0.37044027],\n",
      "       [0.37676662],\n",
      "       [0.3459438 ],\n",
      "       [0.3838137 ],\n",
      "       [0.37607267],\n",
      "       [0.34416476],\n",
      "       [0.34559724],\n",
      "       [0.37360963],\n",
      "       [0.313964  ],\n",
      "       [0.30483413]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "class BERT(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, vocab_size, segment_size, max_seq_length=128, rate=0.1):\n",
    "        super(BERT, self).__init__()\n",
    "        self.encoder = TransformerEncoderV4(num_layers=num_layers, d_model=d_model, num_heads=num_heads,\n",
    "                                            dff=dff, vocab_size=vocab_size, segment_size=segment_size,\n",
    "                                            max_pos=max_seq_length, pos_dropout=rate)\n",
    "        self.mlm_dense = tf.keras.layers.Dense(vocab_size, activation='softmax')  # Ensures output shape is [batch, seq_length, vocab_size]\n",
    "        self.nsp_dense = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.encoder((inputs['input_ids'], inputs['segment_ids']), training=training)\n",
    "        mlm_output = self.mlm_dense(x)  # Check shapes here\n",
    "        nsp_output = self.nsp_dense(x[:, 0, :])\n",
    "        return {'mlm_output': mlm_output, 'nsp_output': nsp_output}\n",
    "\n",
    "\n",
    "bert_model = BERT(num_layers, d_model, num_heads, \n",
    "                  dff, vocab_size, segment_size)\n",
    "bert_out = bert_model(single_test_instance[0])\n",
    "print(bert_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bfe9d7d7-b7a2-433a-a67e-4909874953c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0, Total Loss: 11.0329, MLM Loss: 10.3090, NSP Loss: 0.7240\n",
      "Epoch 1, Step 10, Total Loss: 11.0875, MLM Loss: 10.3090, NSP Loss: 0.7785\n",
      "Epoch 1, Step 20, Total Loss: 11.0651, MLM Loss: 10.3090, NSP Loss: 0.7562\n",
      "Epoch 1, Step 30, Total Loss: 10.9832, MLM Loss: 10.3090, NSP Loss: 0.6742\n",
      "Epoch 1, Step 40, Total Loss: 11.0547, MLM Loss: 10.3090, NSP Loss: 0.7458\n",
      "Epoch 1, Step 50, Total Loss: 11.1379, MLM Loss: 10.3090, NSP Loss: 0.8290\n",
      "Epoch 1, Step 60, Total Loss: 11.2094, MLM Loss: 10.3090, NSP Loss: 0.9005\n",
      "Epoch 1, Step 70, Total Loss: 11.1596, MLM Loss: 10.3090, NSP Loss: 0.8507\n",
      "Epoch 1, Step 80, Total Loss: 11.0163, MLM Loss: 10.3090, NSP Loss: 0.7074\n",
      "Epoch 1, Step 90, Total Loss: 11.0615, MLM Loss: 10.3090, NSP Loss: 0.7526\n",
      "Epoch 1, Step 100, Total Loss: 11.0120, MLM Loss: 10.3090, NSP Loss: 0.7030\n",
      "Epoch 1, Step 110, Total Loss: 11.0890, MLM Loss: 10.3089, NSP Loss: 0.7801\n",
      "Epoch 1, Step 120, Total Loss: 11.0855, MLM Loss: 10.3089, NSP Loss: 0.7765\n",
      "Epoch 1, Step 130, Total Loss: 11.1073, MLM Loss: 10.3089, NSP Loss: 0.7984\n",
      "Epoch 1, Step 140, Total Loss: 11.1070, MLM Loss: 10.3089, NSP Loss: 0.7981\n",
      "Epoch 1, Step 150, Total Loss: 11.1046, MLM Loss: 10.3089, NSP Loss: 0.7956\n",
      "Epoch 1, Step 160, Total Loss: 10.9980, MLM Loss: 10.3089, NSP Loss: 0.6891\n",
      "Epoch 1, Step 170, Total Loss: 11.0680, MLM Loss: 10.3089, NSP Loss: 0.7591\n",
      "Epoch 1, Step 180, Total Loss: 11.0447, MLM Loss: 10.3089, NSP Loss: 0.7357\n",
      "Epoch 1, Step 190, Total Loss: 11.1043, MLM Loss: 10.3089, NSP Loss: 0.7953\n",
      "Epoch 1, Step 200, Total Loss: 11.0274, MLM Loss: 10.3089, NSP Loss: 0.7184\n",
      "Epoch 1, Step 210, Total Loss: 10.9339, MLM Loss: 10.3089, NSP Loss: 0.6249\n",
      "Epoch 1, Step 220, Total Loss: 11.1278, MLM Loss: 10.3089, NSP Loss: 0.8189\n",
      "Epoch 1, Step 230, Total Loss: 11.0738, MLM Loss: 10.3089, NSP Loss: 0.7648\n",
      "Epoch 1, Step 240, Total Loss: 11.1705, MLM Loss: 10.3089, NSP Loss: 0.8616\n",
      "Epoch 1, Step 250, Total Loss: 11.0642, MLM Loss: 10.3089, NSP Loss: 0.7553\n",
      "Epoch 1, Step 260, Total Loss: 11.2062, MLM Loss: 10.3089, NSP Loss: 0.8973\n",
      "Epoch 1, Step 270, Total Loss: 10.9137, MLM Loss: 10.3089, NSP Loss: 0.6048\n",
      "Epoch 1, Step 280, Total Loss: 10.9877, MLM Loss: 10.3089, NSP Loss: 0.6788\n",
      "Epoch 1, Step 290, Total Loss: 10.9139, MLM Loss: 10.3089, NSP Loss: 0.6049\n",
      "Epoch 1, Step 300, Total Loss: 11.0796, MLM Loss: 10.3089, NSP Loss: 0.7707\n",
      "Epoch 1, Step 310, Total Loss: 11.0568, MLM Loss: 10.3089, NSP Loss: 0.7479\n",
      "Epoch 1, Step 320, Total Loss: 11.0881, MLM Loss: 10.3089, NSP Loss: 0.7792\n",
      "Epoch 1, Step 330, Total Loss: 10.9293, MLM Loss: 10.3089, NSP Loss: 0.6204\n",
      "Epoch 1, Step 340, Total Loss: 11.1032, MLM Loss: 10.3089, NSP Loss: 0.7942\n",
      "Epoch 1, Step 350, Total Loss: 11.0516, MLM Loss: 10.3089, NSP Loss: 0.7427\n",
      "Epoch 1, Step 360, Total Loss: 11.0182, MLM Loss: 10.3089, NSP Loss: 0.7092\n",
      "Epoch 1, Step 370, Total Loss: 11.0871, MLM Loss: 10.3089, NSP Loss: 0.7781\n",
      "Epoch 1, Step 380, Total Loss: 10.9865, MLM Loss: 10.3089, NSP Loss: 0.6775\n",
      "Epoch 1, Step 390, Total Loss: 10.9752, MLM Loss: 10.3089, NSP Loss: 0.6663\n",
      "Epoch 1, Step 400, Total Loss: 11.1608, MLM Loss: 10.3089, NSP Loss: 0.8518\n",
      "Epoch 1, Step 410, Total Loss: 10.9989, MLM Loss: 10.3089, NSP Loss: 0.6900\n",
      "Epoch 1, Step 420, Total Loss: 10.9988, MLM Loss: 10.3089, NSP Loss: 0.6898\n",
      "Epoch 1, Step 430, Total Loss: 11.1358, MLM Loss: 10.3089, NSP Loss: 0.8268\n",
      "Epoch 1, Step 440, Total Loss: 11.0617, MLM Loss: 10.3089, NSP Loss: 0.7528\n",
      "Epoch 1, Step 450, Total Loss: 11.0223, MLM Loss: 10.3089, NSP Loss: 0.7134\n",
      "Epoch 1, Step 460, Total Loss: 11.0444, MLM Loss: 10.3089, NSP Loss: 0.7355\n",
      "Epoch 1, Step 470, Total Loss: 10.9692, MLM Loss: 10.3089, NSP Loss: 0.6603\n",
      "Epoch 1, Step 480, Total Loss: 11.0644, MLM Loss: 10.3089, NSP Loss: 0.7555\n",
      "Epoch 1, Step 490, Total Loss: 11.0689, MLM Loss: 10.3089, NSP Loss: 0.7600\n",
      "Epoch 1, Step 500, Total Loss: 11.0517, MLM Loss: 10.3089, NSP Loss: 0.7428\n",
      "Epoch 1, Step 510, Total Loss: 11.0791, MLM Loss: 10.3089, NSP Loss: 0.7702\n",
      "Epoch 1, Step 520, Total Loss: 10.9969, MLM Loss: 10.3089, NSP Loss: 0.6880\n",
      "Epoch 1, Step 530, Total Loss: 11.0088, MLM Loss: 10.3089, NSP Loss: 0.6999\n",
      "Epoch 1, Step 540, Total Loss: 11.0130, MLM Loss: 10.3089, NSP Loss: 0.7042\n",
      "Epoch 1, Step 550, Total Loss: 11.0350, MLM Loss: 10.3089, NSP Loss: 0.7262\n",
      "Epoch 1, Step 560, Total Loss: 10.9638, MLM Loss: 10.3089, NSP Loss: 0.6549\n",
      "Epoch 1, Step 570, Total Loss: 11.0500, MLM Loss: 10.3089, NSP Loss: 0.7412\n",
      "Epoch 1, Step 580, Total Loss: 11.0538, MLM Loss: 10.3089, NSP Loss: 0.7449\n",
      "Epoch 1, Step 590, Total Loss: 11.0548, MLM Loss: 10.3089, NSP Loss: 0.7459\n",
      "Epoch 1, Step 600, Total Loss: 10.9581, MLM Loss: 10.3089, NSP Loss: 0.6492\n",
      "Epoch 1, Step 610, Total Loss: 10.9503, MLM Loss: 10.3089, NSP Loss: 0.6414\n",
      "Epoch 1, Step 620, Total Loss: 11.0426, MLM Loss: 10.3088, NSP Loss: 0.7337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-12 18:16:21.953513: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Step 0, Total Loss: 11.0036, MLM Loss: 10.3088, NSP Loss: 0.6948\n",
      "Epoch 2, Step 10, Total Loss: 10.9292, MLM Loss: 10.3088, NSP Loss: 0.6204\n",
      "Epoch 2, Step 20, Total Loss: 11.0623, MLM Loss: 10.3088, NSP Loss: 0.7535\n",
      "Epoch 2, Step 30, Total Loss: 11.0720, MLM Loss: 10.3087, NSP Loss: 0.7633\n",
      "Epoch 2, Step 40, Total Loss: 10.9847, MLM Loss: 10.3088, NSP Loss: 0.6759\n",
      "Epoch 2, Step 50, Total Loss: 10.9817, MLM Loss: 10.3087, NSP Loss: 0.6730\n",
      "Epoch 2, Step 60, Total Loss: 11.0598, MLM Loss: 10.3087, NSP Loss: 0.7510\n",
      "Epoch 2, Step 70, Total Loss: 10.9881, MLM Loss: 10.3087, NSP Loss: 0.6794\n",
      "Epoch 2, Step 80, Total Loss: 11.0788, MLM Loss: 10.3086, NSP Loss: 0.7701\n",
      "Epoch 2, Step 90, Total Loss: 11.0113, MLM Loss: 10.3086, NSP Loss: 0.7026\n",
      "Epoch 2, Step 100, Total Loss: 11.0149, MLM Loss: 10.3086, NSP Loss: 0.7062\n",
      "Epoch 2, Step 110, Total Loss: 11.1322, MLM Loss: 10.3085, NSP Loss: 0.8236\n",
      "Epoch 2, Step 120, Total Loss: 10.9869, MLM Loss: 10.3085, NSP Loss: 0.6784\n",
      "Epoch 2, Step 130, Total Loss: 10.9497, MLM Loss: 10.3084, NSP Loss: 0.6413\n",
      "Epoch 2, Step 140, Total Loss: 10.9953, MLM Loss: 10.3083, NSP Loss: 0.6870\n",
      "Epoch 2, Step 150, Total Loss: 11.0428, MLM Loss: 10.3083, NSP Loss: 0.7346\n",
      "Epoch 2, Step 160, Total Loss: 11.0303, MLM Loss: 10.3080, NSP Loss: 0.7223\n",
      "Epoch 2, Step 170, Total Loss: 11.0163, MLM Loss: 10.3081, NSP Loss: 0.7082\n",
      "Epoch 2, Step 180, Total Loss: 11.0238, MLM Loss: 10.3080, NSP Loss: 0.7157\n",
      "Epoch 2, Step 190, Total Loss: 10.9171, MLM Loss: 10.3080, NSP Loss: 0.6091\n",
      "Epoch 2, Step 200, Total Loss: 11.0049, MLM Loss: 10.3076, NSP Loss: 0.6973\n",
      "Epoch 2, Step 210, Total Loss: 11.0074, MLM Loss: 10.3071, NSP Loss: 0.7002\n",
      "Epoch 2, Step 220, Total Loss: 11.0259, MLM Loss: 10.3069, NSP Loss: 0.7190\n",
      "Epoch 2, Step 230, Total Loss: 11.0738, MLM Loss: 10.3070, NSP Loss: 0.7668\n",
      "Epoch 2, Step 240, Total Loss: 10.9660, MLM Loss: 10.3066, NSP Loss: 0.6594\n",
      "Epoch 2, Step 250, Total Loss: 11.0027, MLM Loss: 10.3064, NSP Loss: 0.6963\n",
      "Epoch 2, Step 260, Total Loss: 11.1325, MLM Loss: 10.3053, NSP Loss: 0.8272\n",
      "Epoch 2, Step 270, Total Loss: 10.9468, MLM Loss: 10.3053, NSP Loss: 0.6415\n",
      "Epoch 2, Step 280, Total Loss: 11.0030, MLM Loss: 10.3041, NSP Loss: 0.6989\n",
      "Epoch 2, Step 290, Total Loss: 11.0061, MLM Loss: 10.3037, NSP Loss: 0.7024\n",
      "Epoch 2, Step 300, Total Loss: 11.0518, MLM Loss: 10.3029, NSP Loss: 0.7489\n",
      "Epoch 2, Step 310, Total Loss: 11.1253, MLM Loss: 10.3045, NSP Loss: 0.8207\n",
      "Epoch 2, Step 320, Total Loss: 10.9808, MLM Loss: 10.3021, NSP Loss: 0.6787\n",
      "Epoch 2, Step 330, Total Loss: 10.9593, MLM Loss: 10.2999, NSP Loss: 0.6595\n",
      "Epoch 2, Step 340, Total Loss: 10.9825, MLM Loss: 10.2968, NSP Loss: 0.6856\n",
      "Epoch 2, Step 350, Total Loss: 11.0242, MLM Loss: 10.2984, NSP Loss: 0.7259\n",
      "Epoch 2, Step 360, Total Loss: 11.0186, MLM Loss: 10.2967, NSP Loss: 0.7219\n",
      "Epoch 2, Step 370, Total Loss: 11.0294, MLM Loss: 10.2934, NSP Loss: 0.7360\n",
      "Epoch 2, Step 380, Total Loss: 11.0346, MLM Loss: 10.2868, NSP Loss: 0.7478\n",
      "Epoch 2, Step 390, Total Loss: 10.9668, MLM Loss: 10.2828, NSP Loss: 0.6840\n",
      "Epoch 2, Step 400, Total Loss: 10.9713, MLM Loss: 10.2830, NSP Loss: 0.6882\n",
      "Epoch 2, Step 410, Total Loss: 10.8833, MLM Loss: 10.2805, NSP Loss: 0.6028\n",
      "Epoch 2, Step 420, Total Loss: 11.0774, MLM Loss: 10.2715, NSP Loss: 0.8059\n",
      "Epoch 2, Step 430, Total Loss: 11.0091, MLM Loss: 10.2854, NSP Loss: 0.7237\n",
      "Epoch 2, Step 440, Total Loss: 10.9285, MLM Loss: 10.2796, NSP Loss: 0.6489\n",
      "Epoch 2, Step 450, Total Loss: 11.0459, MLM Loss: 10.2870, NSP Loss: 0.7589\n",
      "Epoch 2, Step 460, Total Loss: 10.9219, MLM Loss: 10.2767, NSP Loss: 0.6452\n",
      "Epoch 2, Step 470, Total Loss: 11.0034, MLM Loss: 10.2694, NSP Loss: 0.7340\n",
      "Epoch 2, Step 480, Total Loss: 10.9572, MLM Loss: 10.2770, NSP Loss: 0.6802\n",
      "Epoch 2, Step 490, Total Loss: 11.0023, MLM Loss: 10.2757, NSP Loss: 0.7266\n",
      "Epoch 2, Step 500, Total Loss: 10.9840, MLM Loss: 10.2651, NSP Loss: 0.7190\n",
      "Epoch 2, Step 510, Total Loss: 10.9555, MLM Loss: 10.2684, NSP Loss: 0.6871\n",
      "Epoch 2, Step 520, Total Loss: 11.0657, MLM Loss: 10.2735, NSP Loss: 0.7922\n",
      "Epoch 2, Step 530, Total Loss: 11.0366, MLM Loss: 10.2647, NSP Loss: 0.7719\n",
      "Epoch 2, Step 540, Total Loss: 10.9272, MLM Loss: 10.2626, NSP Loss: 0.6647\n",
      "Epoch 2, Step 550, Total Loss: 11.0820, MLM Loss: 10.2542, NSP Loss: 0.8278\n",
      "Epoch 2, Step 560, Total Loss: 10.9919, MLM Loss: 10.2699, NSP Loss: 0.7220\n",
      "Epoch 2, Step 570, Total Loss: 11.0179, MLM Loss: 10.2509, NSP Loss: 0.7670\n",
      "Epoch 2, Step 580, Total Loss: 11.0619, MLM Loss: 10.2680, NSP Loss: 0.7940\n",
      "Epoch 2, Step 590, Total Loss: 10.9614, MLM Loss: 10.2707, NSP Loss: 0.6906\n",
      "Epoch 2, Step 600, Total Loss: 10.9571, MLM Loss: 10.2636, NSP Loss: 0.6935\n",
      "Epoch 2, Step 610, Total Loss: 11.0292, MLM Loss: 10.2726, NSP Loss: 0.7565\n",
      "Epoch 2, Step 620, Total Loss: 10.9644, MLM Loss: 10.2655, NSP Loss: 0.6989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-12 18:22:21.464438: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Step 0, Total Loss: 10.9019, MLM Loss: 10.2695, NSP Loss: 0.6324\n",
      "Epoch 3, Step 10, Total Loss: 10.9768, MLM Loss: 10.2712, NSP Loss: 0.7056\n",
      "Epoch 3, Step 20, Total Loss: 10.9484, MLM Loss: 10.2651, NSP Loss: 0.6833\n",
      "Epoch 3, Step 30, Total Loss: 10.9837, MLM Loss: 10.2471, NSP Loss: 0.7367\n",
      "Epoch 3, Step 40, Total Loss: 10.9918, MLM Loss: 10.2748, NSP Loss: 0.7170\n",
      "Epoch 3, Step 50, Total Loss: 10.9657, MLM Loss: 10.2524, NSP Loss: 0.7132\n",
      "Epoch 3, Step 60, Total Loss: 11.0250, MLM Loss: 10.2734, NSP Loss: 0.7517\n",
      "Epoch 3, Step 70, Total Loss: 10.9326, MLM Loss: 10.2591, NSP Loss: 0.6735\n",
      "Epoch 3, Step 80, Total Loss: 10.9131, MLM Loss: 10.2483, NSP Loss: 0.6648\n",
      "Epoch 3, Step 90, Total Loss: 10.9206, MLM Loss: 10.2701, NSP Loss: 0.6505\n",
      "Epoch 3, Step 100, Total Loss: 10.9700, MLM Loss: 10.2643, NSP Loss: 0.7057\n",
      "Epoch 3, Step 110, Total Loss: 10.9996, MLM Loss: 10.2656, NSP Loss: 0.7340\n",
      "Epoch 3, Step 120, Total Loss: 10.9620, MLM Loss: 10.2717, NSP Loss: 0.6904\n",
      "Epoch 3, Step 130, Total Loss: 11.0171, MLM Loss: 10.2615, NSP Loss: 0.7556\n",
      "Epoch 3, Step 140, Total Loss: 10.9329, MLM Loss: 10.2453, NSP Loss: 0.6875\n",
      "Epoch 3, Step 150, Total Loss: 10.9487, MLM Loss: 10.2584, NSP Loss: 0.6903\n",
      "Epoch 3, Step 160, Total Loss: 10.9469, MLM Loss: 10.2526, NSP Loss: 0.6943\n",
      "Epoch 3, Step 170, Total Loss: 10.9261, MLM Loss: 10.2617, NSP Loss: 0.6643\n",
      "Epoch 3, Step 180, Total Loss: 10.9333, MLM Loss: 10.2662, NSP Loss: 0.6671\n",
      "Epoch 3, Step 190, Total Loss: 10.9843, MLM Loss: 10.2705, NSP Loss: 0.7138\n",
      "Epoch 3, Step 200, Total Loss: 10.9847, MLM Loss: 10.2549, NSP Loss: 0.7299\n",
      "Epoch 3, Step 210, Total Loss: 10.9032, MLM Loss: 10.2636, NSP Loss: 0.6395\n",
      "Epoch 3, Step 220, Total Loss: 10.9784, MLM Loss: 10.2527, NSP Loss: 0.7257\n",
      "Epoch 3, Step 230, Total Loss: 11.0387, MLM Loss: 10.2650, NSP Loss: 0.7737\n",
      "Epoch 3, Step 240, Total Loss: 10.9764, MLM Loss: 10.2708, NSP Loss: 0.7056\n",
      "Epoch 3, Step 250, Total Loss: 11.0098, MLM Loss: 10.2537, NSP Loss: 0.7561\n",
      "Epoch 3, Step 260, Total Loss: 11.1120, MLM Loss: 10.2623, NSP Loss: 0.8496\n",
      "Epoch 3, Step 270, Total Loss: 10.9147, MLM Loss: 10.2543, NSP Loss: 0.6604\n",
      "Epoch 3, Step 280, Total Loss: 10.9389, MLM Loss: 10.2494, NSP Loss: 0.6895\n",
      "Epoch 3, Step 290, Total Loss: 10.9244, MLM Loss: 10.2634, NSP Loss: 0.6609\n",
      "Epoch 3, Step 300, Total Loss: 10.9279, MLM Loss: 10.2485, NSP Loss: 0.6794\n",
      "Epoch 3, Step 310, Total Loss: 10.9774, MLM Loss: 10.2765, NSP Loss: 0.7009\n",
      "Epoch 3, Step 320, Total Loss: 10.8862, MLM Loss: 10.2575, NSP Loss: 0.6288\n",
      "Epoch 3, Step 330, Total Loss: 10.9334, MLM Loss: 10.2631, NSP Loss: 0.6703\n",
      "Epoch 3, Step 340, Total Loss: 10.9771, MLM Loss: 10.2589, NSP Loss: 0.7182\n",
      "Epoch 3, Step 350, Total Loss: 10.9391, MLM Loss: 10.2593, NSP Loss: 0.6798\n",
      "Epoch 3, Step 360, Total Loss: 10.9336, MLM Loss: 10.2635, NSP Loss: 0.6701\n",
      "Epoch 3, Step 370, Total Loss: 10.9689, MLM Loss: 10.2516, NSP Loss: 0.7172\n",
      "Epoch 3, Step 380, Total Loss: 11.0232, MLM Loss: 10.2485, NSP Loss: 0.7748\n",
      "Epoch 3, Step 390, Total Loss: 10.9455, MLM Loss: 10.2370, NSP Loss: 0.7084\n",
      "Epoch 3, Step 400, Total Loss: 10.9446, MLM Loss: 10.2464, NSP Loss: 0.6982\n",
      "Epoch 3, Step 410, Total Loss: 10.9063, MLM Loss: 10.2493, NSP Loss: 0.6570\n",
      "Epoch 3, Step 420, Total Loss: 10.9082, MLM Loss: 10.2330, NSP Loss: 0.6752\n",
      "Epoch 3, Step 430, Total Loss: 11.0506, MLM Loss: 10.2660, NSP Loss: 0.7846\n",
      "Epoch 3, Step 440, Total Loss: 10.9346, MLM Loss: 10.2623, NSP Loss: 0.6723\n",
      "Epoch 3, Step 450, Total Loss: 11.0613, MLM Loss: 10.2725, NSP Loss: 0.7888\n",
      "Epoch 3, Step 460, Total Loss: 10.9621, MLM Loss: 10.2548, NSP Loss: 0.7073\n",
      "Epoch 3, Step 470, Total Loss: 10.9573, MLM Loss: 10.2580, NSP Loss: 0.6993\n",
      "Epoch 3, Step 480, Total Loss: 11.0402, MLM Loss: 10.2549, NSP Loss: 0.7853\n",
      "Epoch 3, Step 490, Total Loss: 10.9425, MLM Loss: 10.2563, NSP Loss: 0.6862\n",
      "Epoch 3, Step 500, Total Loss: 10.9839, MLM Loss: 10.2459, NSP Loss: 0.7379\n",
      "Epoch 3, Step 510, Total Loss: 10.8951, MLM Loss: 10.2483, NSP Loss: 0.6468\n",
      "Epoch 3, Step 520, Total Loss: 10.9914, MLM Loss: 10.2636, NSP Loss: 0.7279\n",
      "Epoch 3, Step 530, Total Loss: 10.9257, MLM Loss: 10.2518, NSP Loss: 0.6739\n",
      "Epoch 3, Step 540, Total Loss: 10.8916, MLM Loss: 10.2534, NSP Loss: 0.6382\n",
      "Epoch 3, Step 550, Total Loss: 10.9290, MLM Loss: 10.2350, NSP Loss: 0.6939\n",
      "Epoch 3, Step 560, Total Loss: 10.9207, MLM Loss: 10.2649, NSP Loss: 0.6558\n",
      "Epoch 3, Step 570, Total Loss: 10.9277, MLM Loss: 10.2418, NSP Loss: 0.6859\n",
      "Epoch 3, Step 580, Total Loss: 11.1227, MLM Loss: 10.2626, NSP Loss: 0.8601\n",
      "Epoch 3, Step 590, Total Loss: 10.9641, MLM Loss: 10.2666, NSP Loss: 0.6976\n",
      "Epoch 3, Step 600, Total Loss: 10.9998, MLM Loss: 10.2558, NSP Loss: 0.7439\n",
      "Epoch 3, Step 610, Total Loss: 10.9327, MLM Loss: 10.2674, NSP Loss: 0.6652\n",
      "Epoch 3, Step 620, Total Loss: 10.9683, MLM Loss: 10.2573, NSP Loss: 0.7109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-12 18:28:38.523818: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss_object_mlm = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, \n",
    "                                                                reduction=tf.keras.losses.Reduction.NONE)\n",
    "loss_object_nsp = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "\n",
    "def compute_mlm_loss(masked_positions, masked_labels, logits):\n",
    "    # Gather the logits at the masked positions\n",
    "    masked_logits = tf.gather(logits, masked_positions, batch_dims=1, axis=1)\n",
    "    \n",
    "    # Ensure that the masked_labels used here are the correct length and match the number of masked_positions\n",
    "    mlm_loss = tf.keras.losses.sparse_categorical_crossentropy(masked_labels, masked_logits, from_logits=True)\n",
    "\n",
    "    # Reduce mean across batches if needed or sum as appropriate\n",
    "    return tf.reduce_mean(mlm_loss)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = bert_model(inputs, training=True)  # Predictions will have 'mlm_output' and 'nsp_output'\n",
    "        # Compute the MLM loss using the positions and labels\n",
    "        loss_mlm = compute_mlm_loss(labels['masked_lm_positions'], \n",
    "                                    labels['mlm_labels'], predictions['mlm_output'])\n",
    "        # NSP loss remains the same\n",
    "        loss_nsp = loss_object_nsp(labels['nsp_labels'], predictions['nsp_output'])\n",
    "        total_loss = loss_mlm + loss_nsp\n",
    "\n",
    "    gradients = tape.gradient(total_loss, bert_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, bert_model.trainable_variables))\n",
    "    return total_loss, loss_mlm, loss_nsp\n",
    "\n",
    "# Ensure that your dataset loading function is correctly parsing and returning 'masked_lm_positions' and 'masked_lm_labels'\n",
    "\n",
    "batch_size = 16\n",
    "train_dataset = load_dataset('output.tfrecord', batch_size)\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    step = 0\n",
    "    for inputs, labels in train_dataset:\n",
    "        loss_values = train_step(inputs, labels)\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, Step {step}, Total Loss: {loss_values[0].numpy():.4f}, MLM Loss: {loss_values[1].numpy():.4f}, NSP Loss: {loss_values[2].numpy():.4f}\")\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163078fe-11be-4884-8047-57c33ed6e995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513b7327-059e-4fa8-bf8a-ac238cc62c85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87fabc4-8221-48b3-92e2-b2f5906e321d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422c3408-9a6a-4958-a2c8-b1c1f05c5c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e54f7b-6cc7-4b0b-9904-ecff00f33890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c508e2c-06f7-4f84-9e1b-1ae4c0d8f143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_bert_pretraining_instances_in_chunks(file_path, chunk_size=1048576, \n",
    "#                           doc_boundary_pattern=r'ARTICLE-\\d+-\\d+-https:\\/\\/\\S+',\n",
    "#                          test_print=2):   \n",
    "#     '''\n",
    "#     the chunk can read more than one documents and hence the buffer can hold more that one doc . Hence the documents can also hold more than one doc\n",
    "#     but when chunk is smaller than the smallest  doc in file, the documents is essentially end up hoding one doc at a time \n",
    "#     '''\n",
    "#     tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#     buffer = ''\n",
    "#     with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#         while True:\n",
    "#             chunk = file.read(chunk_size)\n",
    "#             if not chunk:\n",
    "#                 break\n",
    "#             buffer += chunk\n",
    "#             # Split buffer by document boundary and keep the content only\n",
    "#             documents = re.split(doc_boundary_pattern, buffer, flags=re.MULTILINE)            \n",
    "#             # If the last part might not be a complete document, keep it in the buffer\n",
    "#             if documents and not re.match(doc_boundary_pattern, documents[-1]):\n",
    "#                 buffer = documents.pop()\n",
    "#             else:\n",
    "#                 buffer = ''            \n",
    "#             # Process each document found in this chunk\n",
    "#             for i, doc in enumerate(documents):\n",
    "#                 if not doc.strip():  # Skip any empty results from split\n",
    "#                     continue\n",
    "#                 sentences = nltk.sent_tokenize(doc)\n",
    "#                 tokens = tokenizer.tokenize(doc)\n",
    "#                 if test_print > 0:\n",
    "#                     print(f\"First few tokens: {tokens[:10]}\") \n",
    "#                     print(f\"First sentence: {sentences[:3] if sentences else 'No content'}\\n\")\n",
    "#                 if i == test_print: return\n",
    "# file_path = 'wiki_articles_with_seperator.txt'\n",
    "# create_bert_pretraining_instances_in_chunks(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a55b2de0-9fe0-4bea-988a-4284b91408fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens A: ['ana', '##rch', '##ism', 'is', 'a', 'political', 'philosophy', 'and', 'movement', 'that', 'is', 'sc', '##ept', '##ical', 'of', 'authority', 'and', 'rejects', 'all', 'involuntary', ',', 'coe', '##rc', '##ive', 'forms', 'of', 'hierarchy', '.', 'ana', '##rch', '##ism', 'calls', 'for', 'the', 'abolition', 'of', 'the', 'state', ',', 'which', 'it', 'holds', 'to', 'be', 'unnecessary', ',', 'und', '##es', '##ira', '##ble', ',', 'and', 'harmful', '.', 'as', 'a', 'historically', 'left', '-', 'wing', 'movement', ',', 'placed', 'on', 'the', 'far', '##thest', 'left', 'of', 'the', 'political', 'spectrum', ',', 'it', 'is', 'usually', 'described', 'alongside', 'communal', '##ism', 'and', 'libertarian', 'marxism', 'as', 'the', 'libertarian', 'wing', '(', 'libertarian', 'socialism', ')', 'of', 'the', 'socialist', 'movement', ',', 'and', 'has', 'a', 'strong', 'historical', 'association', 'with', 'anti', '-', 'capitalism', 'and', 'socialism', '.', 'humans', 'lived', 'in', 'societies', 'without', 'formal', 'hi', '##era', '##rch', '##ies', 'long', 'before', 'the', 'establishment', 'of', 'formal', 'states', ',', 'realms'], len:: 128\n",
      "Tokens B: [',', 'or', 'empires', '.', 'with', 'the', 'rise', 'of', 'organised', 'hierarchical']\n",
      "Is random next: False\n",
      "\n",
      "Tokens A: [',', 'or', 'empires', '.', 'with', 'the', 'rise', 'of', 'organised', 'hierarchical', 'bodies', ',', 'sc', '##ept', '##ici', '##sm', 'toward', 'authority', 'also', 'rose', '.', 'although', 'traces', 'of', 'anarchist', 'thought', 'are', 'found', 'throughout', 'history', ',', 'modern', 'ana', '##rch', '##ism', 'emerged', 'from', 'the', 'enlightenment', '.', 'during', 'the', 'latter', 'half', 'of', 'the', '19th', 'and', 'the', 'first', 'decades', 'of', 'the', '20th', 'century', ',', 'the', 'anarchist', 'movement', 'flourished', 'in', 'most', 'parts', 'of', 'the', 'world', 'and', 'had', 'a', 'significant', 'role', 'in', 'workers', \"'\", 'struggles', 'for', 'emancipation', '.', 'various', 'anarchist', 'schools', 'of', 'thought', 'formed', 'during', 'this', 'period', '.', 'anarchist', '##s', 'have', 'taken', 'part', 'in', 'several', 'revolutions', ',', 'most', 'notably', 'in', 'the', 'paris', 'commune', ',', 'the', 'russian', 'civil', 'war', 'and', 'the', 'spanish', 'civil', 'war', ',', 'whose', 'end', 'marked', 'the', 'end', 'of', 'the', 'classical', 'era', 'of', 'ana', '##rch', '##ism', '.'], len:: 128\n",
      "Tokens B: ['in', 'the', 'last', 'decades', 'of', 'the', '20th', 'and', 'into', 'the']\n",
      "Is random next: False\n",
      "\n",
      "Tokens A: ['in', 'the', 'last', 'decades', 'of', 'the', '20th', 'and', 'into', 'the', '21st', 'century', ',', 'the', 'anarchist', 'movement', 'has', 'been', 'res', '##urgent', 'once', 'more', '.', 'ana', '##rch', '##ism', 'employs', 'a', 'diversity', 'of', 'tactics', 'in', 'order', 'to', 'meet', 'its', 'ideal', 'ends', 'which', 'can', 'be', 'broadly', 'separated', 'into', 'revolutionary', 'and', 'evolutionary', 'tactics', ';', 'there', 'is', 'significant', 'overlap', 'between', 'the', 'two', ',', 'which', 'are', 'merely', 'descriptive', '.', 'revolutionary', 'tactics', 'aim', 'to', 'bring', 'down', 'authority', 'and', 'state', ',', 'having', 'taken', 'a', 'violent', 'turn', 'in', 'the', 'past', ',', 'while', 'evolutionary', 'tactics', 'aim', 'to', 'pre', '##fi', '##gur', '##e', 'what', 'an', 'anarchist', 'society', 'would', 'be', 'like', '.', 'anarchist', 'thought', ',', 'criticism', ',', 'and', 'pr', '##ax', '##is', 'have', 'played', 'a', 'part', 'in', 'diverse', 'areas', 'of', 'human', 'society', '.', 'criticism', 'of', 'ana', '##rch', '##ism', 'include', 'claims', 'that', 'it', 'is'], len:: 128\n",
      "Tokens B: ['greece', ',', 'and', 'mexico', '.', 'militant', 'black', 'bloc', 'protest', 'groups']\n",
      "Is random next: True\n",
      "\n",
      "Tokens A: ['internally', 'inconsistent', ',', 'violent', ',', 'or', 'utopia', '##n', '.', 'etymology', ',', 'terminology', ',', 'and', 'definition', 'the', 'et', '##ym', '##ological', 'origin', 'of', 'ana', '##rch', '##ism', 'is', 'from', 'the', 'ancient', 'greek', 'ana', '##rk', '##hia', ',', 'meaning', '\"', 'without', 'a', 'ruler', '\"', ',', 'composed', 'of', 'the', 'prefix', 'an', '-', '(', '\"', 'without', '\"', ')', 'and', 'the', 'word', 'ark', '##hos', '(', '\"', 'leader', '\"', 'or', '\"', 'ruler', '\"', ')', '.', 'the', 'suffix', '-', 'is', '##m', 'denotes', 'the', 'ideological', 'current', 'that', 'favour', '##s', 'anarchy', '.', 'ana', '##rch', '##ism', 'appears', 'in', 'english', 'from', '1642', 'as', 'ana', '##rch', '##ism', '##e', 'and', 'anarchy', 'from', '153', '##9', ';', 'early', 'english', 'usage', '##s', 'emphasis', '##ed', 'a', 'sense', 'of', 'disorder', '.', 'various', 'factions', 'within', 'the', 'french', 'revolution', 'labelled', 'their', 'opponents', 'as', 'anarchist', '##s', ',', 'although', 'few', 'such', 'accused', 'shared'], len:: 128\n",
      "Tokens B: ['sect', '##arian', '##ism', 'within', 'the', 'anarchist', 'mil', '##ieu', 'was', 'ana']\n",
      "Is random next: True\n",
      "\n",
      "Tokens A: ['many', 'views', 'with', 'later', 'anarchist', '##s', '.', 'many', 'revolutionaries', 'of', 'the', '19th', 'century', 'such', 'as', 'william', 'god', '##win', '(', '1756', '–', '1836', ')', 'and', 'wilhelm', 'wei', '##tling', '(', '1808', '–', '1871', ')', 'would', 'contribute', 'to', 'the', 'anarchist', 'doctrines', 'of', 'the', 'next', 'generation', 'but', 'did', 'not', 'use', 'anarchist', 'or', 'ana', '##rch', '##ism', 'in', 'describing', 'themselves', 'or', 'their', 'beliefs', '.', 'the', 'first', 'political', 'philosopher', 'to', 'call', 'himself', 'an', 'anarchist', '(', ')', 'was', 'pierre', '-', 'joseph', 'proud', '##hon', '(', '1809', '–', '1865', ')', ',', 'marking', 'the', 'formal', 'birth', 'of', 'ana', '##rch', '##ism', 'in', 'the', 'mid', '-', '19th', 'century', '.', 'since', 'the', '1890s', 'and', 'beginning', 'in', 'france', ',', 'libertarian', '##ism', 'has', 'often', 'been', 'used', 'as', 'a', 'synonym', 'for', 'ana', '##rch', '##ism', 'and', 'its', 'use', 'as', 'a', 'synonym', 'is', 'still', 'common', 'outside', 'the'], len:: 128\n",
      "Tokens B: ['united', 'states', '.', 'some', 'usage', '##s', 'of', 'libertarian', '##ism', 'refer']\n",
      "Is random next: False\n",
      "\n",
      "Tokens A: ['united', 'states', '.', 'some', 'usage', '##s', 'of', 'libertarian', '##ism', 'refer', 'to', 'individual', '##istic', 'free', '-', 'market', 'philosophy', 'only', ',', 'and', 'free', '-', 'market', 'ana', '##rch', '##ism', 'in', 'particular', 'is', 'termed', 'libertarian', 'ana', '##rch', '##ism', '.', 'while', 'the', 'term', 'libertarian', 'has', 'been', 'largely', 'synonymous', 'with', 'ana', '##rch', '##ism', ',', 'its', 'meaning', 'has', 'more', 'recently', 'dil', '##uted', 'with', 'wider', 'adoption', 'from', 'ideological', '##ly', 'di', '##spar', '##ate', 'groups', ',', 'including', 'both', 'the', 'new', 'left', 'and', 'libertarian', 'marxist', '##s', ',', 'who', 'do', 'not', 'associate', 'themselves', 'with', 'authoritarian', 'socialists', 'or', 'a', 'vanguard', 'party', ',', 'and', 'extreme', 'cultural', 'liberals', ',', 'who', 'are', 'primarily', 'concerned', 'with', 'civil', 'liberties', '.', 'additionally', ',', 'some', 'anarchist', '##s', 'use', 'libertarian', 'socialist', 'to', 'avoid', 'ana', '##rch', '##ism', \"'\", 's', 'negative', 'con', '##not', '##ations', 'and', 'emphasis', '##e', 'its', 'connections', 'with', 'socialism'], len:: 128\n",
      "Tokens B: ['.', 'ana', '##rch', '##ism', 'is', 'broadly', 'used', 'to', 'describe', 'the']\n",
      "Is random next: False\n",
      "\n",
      "Tokens A: ['.', 'ana', '##rch', '##ism', 'is', 'broadly', 'used', 'to', 'describe', 'the', 'anti', '-', 'authoritarian', 'wing', 'of', 'the', 'socialist', 'movement', '.', 'ana', '##rch', '##ism', 'is', 'contrasted', 'to', 'socialist', 'forms', 'which', 'are', 'state', '-', 'oriented', 'or', 'from', 'above', '.', 'scholars', 'of', 'ana', '##rch', '##ism', 'generally', 'highlight', 'ana', '##rch', '##ism', \"'\", 's', 'socialist', 'credentials', 'and', 'critic', '##ise', 'attempts', 'at', 'creating', 'di', '##cho', '##tom', '##ies', 'between', 'the', 'two', '.', 'some', 'scholars', 'describe', 'ana', '##rch', '##ism', 'as', 'having', 'many', 'influences', 'from', 'liberalism', ',', 'and', 'being', 'both', 'liberals', 'and', 'socialists', 'but', 'more', 'so', ',', 'while', 'most', 'scholars', 'reject', 'ana', '##rch', '##o', '-', 'capitalism', 'as', 'a', 'misunderstanding', 'of', 'anarchist', 'principles', '.', 'while', 'opposition', 'to', 'the', 'state', 'is', 'central', 'to', 'anarchist', 'thought', ',', 'defining', 'ana', '##rch', '##ism', 'is', 'not', 'an', 'easy', 'task', 'for', 'scholars', ',', 'as', 'there'], len:: 128\n",
      "Tokens B: ['is', 'a', 'lot', 'of', 'discussion', 'among', 'scholars', 'and', 'anarchist', '##s']\n",
      "Is random next: False\n",
      "\n",
      "Tokens A: ['is', 'a', 'lot', 'of', 'discussion', 'among', 'scholars', 'and', 'anarchist', '##s', 'on', 'the', 'matter', ',', 'and', 'various', 'currents', 'perceive', 'ana', '##rch', '##ism', 'slightly', 'differently', '.', 'major', 'definition', '##al', 'elements', 'include', 'the', 'will', 'for', 'a', 'non', '-', 'coe', '##rc', '##ive', 'society', ',', 'the', 'rejection', 'of', 'the', 'state', 'apparatus', ',', 'the', 'belief', 'that', 'human', 'nature', 'allows', 'humans', 'to', 'exist', 'in', 'or', 'progress', 'toward', 'such', 'a', 'non', '-', 'coe', '##rc', '##ive', 'society', ',', 'and', 'a', 'suggestion', 'on', 'how', 'to', 'act', 'to', 'pursue', 'the', 'ideal', 'of', 'anarchy', '.', 'history', 'pre', '-', 'modern', 'era', 'before', 'the', 'establishment', 'of', 'towns', 'and', 'cities', ',', 'an', 'established', 'authority', 'did', 'not', 'exist', '.', 'it', 'was', 'after', 'the', 'creation', 'of', 'institutions', 'of', 'authority', 'that', 'anarchist', '##ic', 'ideas', 'es', '##po', '##used', 'as', 'a', 'reaction', '.', 'the', 'most', 'notable', 'precursor', '##s'], len:: 128\n",
      "Tokens B: ['oppose', 'the', 'establishment', 'and', 'secondly', 'to', 'promote', 'anarchist', 'ethics', 'and']\n",
      "Is random next: True\n",
      "\n",
      "Tokens A: ['to', 'ana', '##rch', '##ism', 'in', 'the', 'ancient', 'world', 'were', 'in', 'china', 'and', 'greece', '.', 'in', 'china', ',', 'philosophical', 'ana', '##rch', '##ism', '(', 'the', 'discussion', 'on', 'the', 'legitimacy', 'of', 'the', 'state', ')', 'was', 'del', '##ine', '##ated', 'by', 'tao', '##ist', 'philosophers', 'zhu', '##ang', 'zhou', 'and', 'lao', '##zi', '.', 'alongside', 'st', '##oic', '##ism', ',', 'tao', '##ism', 'has', 'been', 'said', 'to', 'have', 'had', '\"', 'significant', 'anticipation', '##s', '\"', 'of', 'ana', '##rch', '##ism', '.', 'ana', '##rch', '##ic', 'attitudes', 'were', 'also', 'articulated', 'by', 'tr', '##aged', '##ians', 'and', 'philosophers', 'in', 'greece', '.', 'ae', '##sch', '##ylus', 'and', 'so', '##ph', '##oc', '##les', 'used', 'the', 'myth', 'of', 'anti', '##gon', '##e', 'to', 'illustrate', 'the', 'conflict', 'between', 'rules', 'set', 'by', 'the', 'state', 'and', 'personal', 'autonomy', '.', 'socrates', 'questioned', 'athenian', 'authorities', 'constantly', 'and', 'insisted', 'on', 'the', 'right', 'of', 'individual', 'freedom', 'of'], len:: 128\n",
      "Tokens B: ['and', 'post', '-', 'ana', '##rch', '##ism', ')', 'developed', 'thereafter', '.']\n",
      "Is random next: True\n",
      "\n",
      "Tokens A: ['conscience', '.', 'cy', '##nic', '##s', 'dismissed', 'human', 'law', '(', 'no', '##mos', ')', 'and', 'associated', 'authorities', 'while', 'trying', 'to', 'live', 'according', 'to', 'nature', '(', 'ph', '##ysis', ')', '.', 'st', '##oic', '##s', 'were', 'supportive', 'of', 'a', 'society', 'based', 'on', 'unofficial', 'and', 'friendly', 'relations', 'among', 'its', 'citizens', 'without', 'the', 'presence', 'of', 'a', 'state', '.', 'in', 'medieval', 'europe', ',', 'there', 'was', 'no', 'anarchist', '##ic', 'activity', 'except', 'some', 'as', '##ce', '##tic', 'religious', 'movements', '.', 'these', ',', 'and', 'other', 'muslim', 'movements', ',', 'later', 'gave', 'birth', 'to', 'religious', 'ana', '##rch', '##ism', '.', 'in', 'the', 'sas', '##anian', 'empire', ',', 'mazda', '##k', 'called', 'for', 'an', 'e', '##gal', '##itarian', 'society', 'and', 'the', 'abolition', 'of', 'monarchy', ',', 'only', 'to', 'be', 'soon', 'executed', 'by', 'emperor', 'ka', '##va', '##d', 'i', '.', 'in', 'bas', '##ra', ',', 'religious', 'sect', '##s', 'preached', 'against', 'the'], len:: 128\n",
      "Tokens B: ['community', 'and', 'individual', '##ity', 'sets', 'it', 'apart', 'from', 'ana', '##rch']\n",
      "Is random next: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# def create_bert_pretraining_instances_in_chunks(file_path, chunk_size=1048576, \n",
    "#                           doc_boundary_pattern=r'ARTICLE-\\d+-\\d+-https:\\/\\/\\S+',\n",
    "#                          test_print=10, max_seq_length=128, max_predictions_per_seq=20, \n",
    "#                          dupe_factor=5, random_seed=12345, nsp_enabled=True):\n",
    "#     tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#     buffer = ''\n",
    "#     rng = random.Random(random_seed)\n",
    "#     instances = []\n",
    "#     with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#         while True:\n",
    "#             chunk = file.read(chunk_size)\n",
    "#             if not chunk:\n",
    "#                 break\n",
    "#             buffer += chunk\n",
    "#             documents = re.split(doc_boundary_pattern, buffer, flags=re.MULTILINE)\n",
    "#             if documents and not re.match(doc_boundary_pattern, documents[-1]):\n",
    "#                 buffer = documents.pop()\n",
    "#             else:\n",
    "#                 buffer = ''\n",
    "            \n",
    "#             for i, doc in enumerate(documents):\n",
    "#                 if not doc.strip():\n",
    "#                     continue\n",
    "#                 tokenized_doc = tokenizer.tokenize(doc)\n",
    "#                 # Split tokenized document into sequences, not implemented here, assumes tokenized_doc is already split\n",
    "#                 sequences = [tokenized_doc[i:i+max_seq_length] for i in range(0, len(tokenized_doc), max_seq_length)]\n",
    "#                 for j in range(len(sequences) - 1):\n",
    "#                     tokens_a = sequences[j]\n",
    "#                     if rng.random() > 0.5 or not nsp_enabled:\n",
    "#                         is_random_next = True\n",
    "#                         # Randomly pick any sequence from any document\n",
    "#                         tokens_b = sequences[rng.randint(0, len(sequences) - 1)]\n",
    "#                     else:\n",
    "#                         is_random_next = False\n",
    "#                         # Ensure tokens_b is the next sequence in the same document\n",
    "#                         tokens_b = sequences[j + 1]\n",
    "                    \n",
    "#                     if test_print > 0:\n",
    "#                         print(f\"Tokens A: {tokens_a}, len:: {len(tokens_a) }\")\n",
    "#                         print(f\"Tokens B: {tokens_b[:10]}\")\n",
    "#                         print(f\"Is random next: {is_random_next}\\n\")\n",
    "#                         test_print -= 1\n",
    "#                         if test_print == 0:\n",
    "#                             return  # Early exit for test purposes\n",
    "\n",
    "# file_path = 'wiki_articles_with_seperator.txt'\n",
    "# res = create_bert_pretraining_instances_in_chunks(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4409844-790b-4695-946a-51f6127b8a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Human:\n",
    "#     def __init__(self, name, legs=2):\n",
    "#         self.name = name\n",
    "#         self.legs = legs\n",
    "\n",
    "# class MaleHuman(Human):\n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         super(MaleHuman, self).__init__(*args, **kwargs)\n",
    "#         # self.beard = beard\n",
    "\n",
    "# m1 = MaleHuman('Bhujay')\n",
    "# m1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848e17e7-f516-4e0e-85de-66c66c143682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed3be30-03bc-46fa-86d3-c2c719bfb359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.padded_batch(\n",
    "    #     batch_size, \n",
    "    #     padded_shapes={'input_ids': [None], \n",
    "    #                    'segment_ids': [128], \n",
    "    #                    'masked_lm_positions': [None], \n",
    "    #                    'masked_lm_labels': [None], \n",
    "    #                    'next_sentence_labels': []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa8e482-617d-43f2-9ea9-63076c6546fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# try:\n",
    "#     for idx, (inputs, labels) in enumerate(train_dataset):\n",
    "#         print(f\"instance {idx + 1}:\")\n",
    "#         # print(f\"Inputs: {inputs}\")\n",
    "#         # print(f\"Labels: {labels}\")\n",
    "#         if idx == 10:  # Limit to 2 batches for demonstration\n",
    "#             break\n",
    "# except Exception as e:\n",
    "#     print(f\"Error while processing the dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da8a988-88a4-419f-aaac-03af4ace604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_dataset(filepath):\n",
    "#     # dataset = load_dataset(filepath, batch_size=16)  # Using batch size of 1 for simplicity\n",
    "#     try:\n",
    "#         for i, (inputs, labels) in enumerate(train_dataset):        \n",
    "#             print(f\"Successfully parsed entry {i+1}\")        \n",
    "#             if i == 100:  # Optionally limit the number of entries to check\n",
    "#                 break\n",
    "#     except Exception as e:\n",
    "#             print(f'record: {i}  , Err: {e}')\n",
    "\n",
    "# test_dataset('output.tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3488dd32-cf4e-46b5-88d9-880fae1b1069",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def parse_tfrecord(serialized_example):\n",
    "#     feature_description = {\n",
    "#         'input_ids': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "#         'segment_ids': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "#         'masked_lm_positions': tf.io.VarLenFeature(tf.int64),\n",
    "#         'masked_lm_labels': tf.io.VarLenFeature(tf.int64),\n",
    "#         'next_sentence_labels': tf.io.FixedLenFeature([], tf.int64)\n",
    "#     }\n",
    "#     try:\n",
    "#         example = tf.io.parse_single_example(serialized_example, feature_description)    \n",
    "#         input_ids = tf.cast(example['input_ids'], tf.int32)\n",
    "#         segment_ids = tf.cast(example['segment_ids'], tf.int32)\n",
    "#         # segment_ids = tf.sparse.to_dense(segment_ids, default_value=0)  # Default to 0\n",
    "#         # segment_ids = tf.reshape(segment_ids, [max_seq_length]) \n",
    "#         masked_lm_positions = tf.sparse.to_dense(example['masked_lm_positions'])\n",
    "#         masked_lm_labels = tf.sparse.to_dense(example['masked_lm_labels'])   \n",
    "#         next_sentence_labels = tf.cast(example['next_sentence_labels'], tf.int32)\n",
    "#         inputs = {'input_ids': input_ids, 'segment_ids': segment_ids, }\n",
    "#         labels = {'masked_lm_positions': masked_lm_positions, \n",
    "#                   'mlm_labels': masked_lm_labels, 'nsp_labels': next_sentence_labels}\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to parse example: {e}\")\n",
    "#         # Return dummy/default data to allow pipeline to continue\n",
    "#         inputs = {\n",
    "#             'input_ids': tf.constant([0]*args.max_seq_length, dtype=tf.int32),\n",
    "#             'segment_ids': tf.constant([0]*args.max_seq_length, dtype=tf.int32)\n",
    "#             }\n",
    "#         labels = {\n",
    "#             'masked_lm_positions': tf.constant([-1]*20, dtype=tf.int32),  # Adjust 20 to your max_predictions_per_seq\n",
    "#             'masked_lm_labels': tf.constant([-1]*20, dtype=tf.int32),\n",
    "#             'nsp_labels': tf.constant([0], dtype=tf.int32)\n",
    "#             } \n",
    "#         return (None, None)\n",
    "#     return inputs, labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def load_dataset(filepath, batch_size):\n",
    "#     raw_dataset = tf.data.TFRecordDataset(filepath)\n",
    "#     parsed_dataset = raw_dataset.map(parse_tfrecord)\n",
    "#     filtered_dataset = parsed_dataset.filter(lambda x: x[0] is not None and x[1] is not None)\n",
    "#     batched_dataset = filtered_dataset.batch(batch_size)\n",
    "#     return batched_dataset\n",
    "# # Usage\n",
    "# batch_size = 16\n",
    "# train_dataset = load_dataset('output.tfrecord', batch_size)\n",
    "\n",
    "# single_test_instance = iter(train_dataset.take(1)).next()\n",
    "# single_input_tuple = single_test_instance[0]['input_ids'], single_test_instance[0]['segment_ids']\n",
    "# print(single_test_instance)\n",
    "# print()\n",
    "# # print(single_input_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b52b52f-f946-4704-9bc0-51cc52a8ba95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def _parse_function(proto):\n",
    "#     # Define your tfrecord again. It must be the same as the one used for saving your data.\n",
    "#     feature_description = {\n",
    "#         'input_ids': tf.io.FixedLenFeature([128], tf.int64),  # Assuming input_ids are of length 128\n",
    "#         'segment_ids': tf.io.FixedLenFeature([128], tf.int64),  # Assuming segment_ids are of length 128\n",
    "#         'masked_lm_positions': tf.io.VarLenFeature(tf.int64),\n",
    "#         'masked_lm_labels': tf.io.VarLenFeature(tf.int64),\n",
    "#         'next_sentence_labels': tf.io.FixedLenFeature([], tf.int64),\n",
    "#     }\n",
    "\n",
    "#     # Load one example\n",
    "#     parsed_features = tf.io.parse_single_example(proto, feature_description)\n",
    "    \n",
    "#     # Turn your sparse array into a dense array with default values as 0\n",
    "#     parsed_features['masked_lm_positions'] = tf.sparse.to_dense(parsed_features['masked_lm_positions'], default_value=0)\n",
    "#     parsed_features['masked_lm_labels'] = tf.sparse.to_dense(parsed_features['masked_lm_labels'], default_value=0)\n",
    "\n",
    "#     return parsed_features\n",
    "# # Read the TFRecord file\n",
    "# def load_dataset(file_path):\n",
    "#     dataset = tf.data.TFRecordDataset(file_path)\n",
    "#     dataset = dataset.map(_parse_function)  # Parse the record into tensors.\n",
    "#     return dataset\n",
    "\n",
    "# # Path to the TFRecord file\n",
    "# tfrecord_file_path = 'output.tfrecord'\n",
    "\n",
    "# # Load the dataset\n",
    "# parsed_dataset = load_dataset(tfrecord_file_path)\n",
    "# # Display a few examples from the dataset\n",
    "# for parsed_record in parsed_dataset.take(2):  # Only take first 5 examples\n",
    "#     print('Input IDs:', parsed_record['input_ids'].numpy())\n",
    "#     print('Segment IDs:', parsed_record['segment_ids'].numpy())\n",
    "#     print('Masked LM Positions:', parsed_record['masked_lm_positions'].numpy())\n",
    "#     print('Masked LM Labels:', parsed_record['masked_lm_labels'].numpy())\n",
    "#     print('Next Sentence Label:', parsed_record['next_sentence_labels'].numpy())\n",
    "#     print('---')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39969389-584a-4602-8847-1472a4155e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_tfrecord(serialized_example):\n",
    "#     feature_description = {\n",
    "#         'input_ids': tf.io.FixedLenFeature([128], tf.int64),\n",
    "#         'segment_ids': tf.io.FixedLenFeature([128], tf.int64),\n",
    "#         'masked_lm_positions': tf.io.VarLenFeature(tf.int64),\n",
    "#         'masked_lm_labels': tf.io.VarLenFeature(tf.int64),\n",
    "#         'next_sentence_labels': tf.io.FixedLenFeature([], tf.int64)\n",
    "#     }\n",
    "#     example = tf.io.parse_single_example(serialized_example, feature_description)\n",
    "\n",
    "#     input_ids = tf.cast(example['input_ids'], tf.int32)\n",
    "#     segment_ids = tf.cast(example['segment_ids'], tf.int32)\n",
    "#     masked_lm_positions = tf.sparse.to_dense(example['masked_lm_positions'])\n",
    "#     masked_lm_labels = tf.sparse.to_dense(example['masked_lm_labels'])\n",
    "#     next_sentence_labels = tf.cast(example['next_sentence_labels'], tf.int32)\n",
    "\n",
    "#     inputs = {'input_ids': input_ids, 'segment_ids': segment_ids}\n",
    "#     labels = {'mlm_output': masked_lm_labels, 'nsp_output': next_sentence_labels}\n",
    "\n",
    "#     return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4058d182-6444-4054-87c9-81511b0ebbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = Adam(learning_rate=2e-5)\n",
    "# loss = {\n",
    "#     'mlm_output': SparseCategoricalCrossentropy(from_logits=True),\n",
    "#     'nsp_output': BinaryCrossentropy(from_logits=True)\n",
    "# }\n",
    "# metrics = {\n",
    "#     'mlm_output': 'accuracy',\n",
    "#     'nsp_output': 'accuracy'\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# def masked_sparse_categorical_crossentropy(y_true, y_pred):\n",
    "#     # Create a mask to ignore `-1` in labels for loss calculation\n",
    "#     mask = tf.cast(tf.not_equal(y_true, -1), tf.float32)\n",
    "#     # Compute sparse categorical crossentropy loss\n",
    "#     loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
    "#     # Apply the mask\n",
    "#     loss *= mask\n",
    "#     # Calculate mean loss only over non-masked elements\n",
    "#     return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "# # Ensure the custom loss function is used when compiling the model\n",
    "# bert_model.compile(\n",
    "#     optimizer=optimizer,\n",
    "#     loss={'mlm_output': masked_sparse_categorical_crossentropy, 'nsp_output': BinaryCrossentropy(from_logits=True)},\n",
    "#     metrics=metrics\n",
    "# )\n",
    "# # epochs = 3 \n",
    "# # bert_model.fit(train_dataset, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bfe004-16cb-4a1d-9991-fcb75ff85137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SmallBERT(Model):\n",
    "#     def __init__(self, vocab_size, num_layers, d_model, num_heads, dff, max_pos=512, rate=0.1):\n",
    "#         super(SmallBERT, self).__init__()\n",
    "#         self.token_embedding = Embedding(vocab_size, d_model)\n",
    "#         self.position_embedding = Embedding(max_pos, d_model)\n",
    "#         self.segment_embedding = Embedding(2, d_model)  # Only 2 segments assumed\n",
    "        \n",
    "#         self.enc_layers = [TransformerEncoderV3(num_layers=1, d_model=d_model, num_heads=num_heads, dff=dff,\n",
    "#                                                 vocab_size=vocab_size, max_pos=max_pos) for _ in range(num_layers)]\n",
    "        \n",
    "#         self.dropout = Dropout(rate)\n",
    "#         self.final_layer = Dense(vocab_size)  # Prediction layer for MLM\n",
    "#         self.nsp_classifier = Dense(2, activation='softmax')  # NSP output\n",
    "\n",
    "#     def call(self, input_ids, segment_ids, training=False):\n",
    "#         seq_length = tf.shape(input_ids)[1]\n",
    "#         position_ids = tf.range(seq_length)\n",
    "        \n",
    "#         x = self.token_embedding(input_ids) + self.position_embedding(position_ids) + self.segment_embedding(segment_ids)\n",
    "#         x = self.dropout(x, training=training)\n",
    "        \n",
    "#         for encoder in self.enc_layers:\n",
    "#             x = encoder(x)\n",
    "        \n",
    "#         logits = self.final_layer(x)\n",
    "#         pooled_output = self.dropout(sequence_output[:, 0, :], training=training)  # Use the output of the [CLS] token\n",
    "#         nsp_output = self.nsp_classifier(pooled_output)\n",
    "#         return logits, nsp_output  \n",
    "\n",
    "# vocab_size = 10000  # Smaller vocabulary size for simplicity\n",
    "# num_layers = 2  # Fewer layers\n",
    "# d_model = 128  # Smaller dimensionality\n",
    "# num_heads = 4\n",
    "# dff = 512\n",
    "# small_bert = SmallBERT(vocab_size, num_layers, d_model, num_heads, dff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e381b9-5878-4250-b99e-8b853640fc3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3dfa73-323e-4930-b9c2-c1e2080a8acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7227c722-9c49-4bdb-926e-7091b49e5f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69ca148-1c24-45e2-904c-94a6c99df05b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a51f6c-f36c-4bf1-97cb-948e5bebaef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d198953-53b2-47b1-bb1a-9d243bf52c02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
