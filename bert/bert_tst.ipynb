{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "086051b2-2d8f-4bb7-bf24-0e4dd05fdc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Suppress specific TensorFlow warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # 3 means to filter out all INFO and WARNING logs\n",
    "import warnings\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, message='.*OUT_OF_RANGE.*')\n",
    "\n",
    "import sys\n",
    "import random\n",
    "import re\n",
    "sys.path.append('../src')\n",
    "import pandas as pd\n",
    "from lr_schedular import CustomSchedule\n",
    "from transformer_encoder import TransformerEncoderV3  \n",
    "from positional_encoding import encode_pos_sin_cosine\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Input, Dense, Dropout, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, BinaryCrossentropy\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63c535dd-000d-42b6-904a-4f2d3faba92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how the dataset looks: dict_keys(['id', 'url', 'title', 'text'])\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# Load an example dataset, 'wikipedia' for English, 2020-03-01 version\n",
    "dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\")\n",
    "print('how the dataset looks:', dataset[0].keys())\n",
    "num_of_articles = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eec8e7b0-449e-4c0f-9c78-d050fc6c4344",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words in first_article: 43985\n",
      "how the article look:\n",
      " Anarchism is a political philosophy and movement that is sceptical of authority and rejects all involuntary, coercive forms of hierarchy. Anarchism calls for the abolition of the state, which it holds to be unnecessary, undesirable, and harmful. As a historically left-wing movement, placed on the farthest left of the political spectrum, it is usually described alongside communalism and libertarian Marxism as the libertarian wing (libertarian socialism) of the socialist movement, and has a strong historical association with anti-capitalism and socialism.\n",
      "\n",
      "Humans lived in societies without formal hierarchies long before the establishment of formal states, realms, or empires. With the rise of organised hierarchical bodies, scepticism toward authority also rose. Although traces of anarchist thought are found throughout history, modern anarchism emerged from the Enlightenment. During the latter half of the 19th and the first decades of the 20th century, the anarchist movement flourished in most parts of the world and had a significant role in workers' struggles for emancipation. Various anarchist schools of thought formed during this period. Anarchists have taken part in several revolutions, most notably in the Paris Commune, the Russian Civil War and the Spanish Civil War, whose end marked the end of the classical era of anarchism. In the last decades of the 20th and into the 21st century, the anarchist movement has been resurgent once more.\n",
      "\n",
      "Anarchism employs a diversity of tact\n",
      "..........................................\n",
      "........................\n",
      "phical and political anarchism] are philosophical and political claims.\" (p.Â 137)\n",
      "  Anarchistic popular fiction novel.\n",
      " \n",
      " \n",
      " \n",
      "  An argument for philosophical anarchism.\n",
      "\n",
      "External links \n",
      " Anarchy Archives. Anarchy Archives is an online research center on the history and theory of anarchism.\n",
      "\n",
      " \n",
      "Anti-capitalism\n",
      "Anti-fascism\n",
      "Economic ideologies\n",
      "Left-wing politics\n",
      "Libertarian socialism\n",
      "Libertarianism\n",
      "Political culture\n",
      "Political movements\n",
      "Political ideologies\n",
      "Social theories\n",
      "Socialism\n",
      "Far-left politics\n"
     ]
    }
   ],
   "source": [
    "first_article = dataset[0]['text']\n",
    "print('words in first_article:', len(first_article))\n",
    "print('how the article look:\\n',first_article[:1500])\n",
    "print('..........................................\\n........................')\n",
    "print(first_article[-500:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5f1620-8982-4f04-97a3-edd3d029d29e",
   "metadata": {},
   "source": [
    "Document and Sentence Segmentation:\n",
    "\n",
    "Document Boundary: Each Wikipedia article can be treated as a single document. This aligns with the BERT requirement where each document is separated by blank lines.\n",
    "\n",
    "Sentence Tokenization: Use a sentence tokenizer to convert each paragraph into distinct sentences. This is crucial because BERT's NSP task assumes that two consecutive sentences in the data might be used as training pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66192cf9-6269-4a60-b20b-e9324877385b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/bhujay/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def save_articles_with_doc_boundary(dataset_name, config_name, split_name, output_file_path, num_of_articles=1000):\n",
    "    nltk.download('punkt')\n",
    "    dataset = load_dataset(dataset_name, config_name, split=split_name)\n",
    "    articles_to_process = dataset.select(range(num_of_articles))\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "        for i, article in enumerate(articles_to_process):\n",
    "            title = article.get('title', f\"No Title Available for Article {i}\")\n",
    "            art_id = article.get('id', \"No ID\")\n",
    "            art_url = article.get('url', \"No URL\")\n",
    "            sentences = nltk.sent_tokenize(article['text'])\n",
    "            full_article_text = f\"ARTICLE-{i}-{art_id}-{art_url}-{title}\\n\" + '\\n'.join(sentences) + '\\n\\n'\n",
    "            file.write(full_article_text)\n",
    "# Call the function to process and save articles\n",
    "output_file_path = 'wiki_articles_with_seperator.txt'\n",
    "save_articles_with_doc_boundary('wikipedia', '20220301.en', 'train', output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2be5185-e2e9-4851-83a6-3badc245f2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARTICLE-0-12-https://en.wikipedia.org/wiki/Anarchism-Anarchism\n",
      "\n",
      "Anarchism is a political philosophy and movement that is sceptical of authority and rejects all involuntary, coercive forms of hierarchy.\n",
      "\n",
      "Anarchism calls for the abolition of the state, which it holds to be unnecessary, undesirable, and harmful.\n",
      "\n",
      "As a historically left-wing movement, placed on the farthest left of the political spectrum, it is usually described alongside communalism and libertarian Marxism as the libertarian wing (libertarian socialism) of the socialist movement, and has a strong historical association with anti-capitalism and socialism.\n",
      "\n",
      "Humans lived in societies without formal hierarchies long before the establishment of formal states, realms, or empires.\n",
      "\n",
      "ARTICLE-1-25-https://en.wikipedia.org/wiki/Autism-Autism\n",
      "\n",
      "Autism is a neurodevelopmental disorder characterized by difficulties with social interaction and communication, and by restricted and repetitive behavior.\n",
      "\n",
      "Parents often notice signs during the first three years of their child's life.\n",
      "\n",
      "These signs often develop gradually, though some autistic children experience regression in their communication and social skills after reaching developmental milestones at a normal pace.\n",
      "\n",
      "Autism is associated with a combination of genetic and environmental factors.\n",
      "\n",
      "ARTICLE-2-39-https://en.wikipedia.org/wiki/Albedo-Albedo\n",
      "\n",
      "Albedo (; ) is the measure of the diffuse reflection of solar radiation out of the total solar radiation and measured on a scale from 0, corresponding to a black body that absorbs all incident radiation, to 1, corresponding to a body that reflects all incident radiation.\n",
      "\n",
      "Surface albedo is defined as the ratio of radiosity Je to the irradiance Ee (flux per unit area) received by a surface.\n",
      "\n",
      "The proportion reflected is not only determined by properties of the surface itself, but also by the spectral and angular distribution of solar radiation reaching the Earth's surface.\n",
      "\n",
      "These factors vary with atmospheric composition, geographic location, and time (see position of the Sun).\n",
      "\n",
      "ARTICLE-3-290-https://en.wikipedia.org/wiki/A-A\n",
      "\n",
      "A, or a, is the first letter and the first vowel of the modern English alphabet and the ISO basic Latin alphabet.\n",
      "\n",
      "Its name in English is a (pronounced ), plural aes.\n",
      "\n",
      "It is similar in shape to the Ancient Greek letter alpha, from which it derives.\n",
      "\n",
      "The uppercase version consists of the two slanting sides of a triangle, crossed in the middle by a horizontal bar.\n",
      "\n",
      "ARTICLE-4-303-https://en.wikipedia.org/wiki/Alabama-Alabama\n",
      "\n",
      "Alabama () is a state in the Southeastern region of the United States, bordered by Tennessee to the north; Georgia to the east; Florida and the Gulf of Mexico to the south; and Mississippi to the west.\n",
      "\n",
      "Alabama is the 30th largest by area and the 24th-most populous of the U.S. states.\n",
      "\n",
      "With a total of  of inland waterways, Alabama has among the most of any state.\n",
      "\n",
      "Alabama is nicknamed the Yellowhammer State, after the state bird.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def display_article_lines(file_path, num_lines=5, num_articles=5):    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        article_count = 0\n",
    "        line_count = 0\n",
    "        for line in file:\n",
    "            if line.startswith('ARTICLE-'):  # New article detected\n",
    "                if article_count >= num_articles:\n",
    "                    break                          \n",
    "                article_count += 1\n",
    "                line_count = 0  # Reset line count for the new article\n",
    "            if line_count < num_lines:\n",
    "                print(line)\n",
    "                line_count += 1\n",
    "            else:\n",
    "                continue  # Skip further lines until the next article starts\n",
    "output_file_path = 'wiki_articles_with_seperator.txt'\n",
    "display_article_lines(output_file_path, num_lines=5, num_articles=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fa7466b-b8a7-48e7-b4e0-35af92b1f186",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingInstance:\n",
    "    \"\"\"A single training instance (sentence pair).\"\"\"\n",
    "    def __init__(self, tokens, segment_ids, masked_lm_positions, masked_lm_labels, is_random_next):\n",
    "        self.tokens = tokens\n",
    "        self.segment_ids = segment_ids\n",
    "        self.masked_lm_positions = masked_lm_positions\n",
    "        self.masked_lm_labels = masked_lm_labels\n",
    "        self.is_random_next = is_random_next\n",
    "\n",
    "    def __str__(self):\n",
    "        tokens_str = \" \".join([str(token) for token in self.tokens])\n",
    "        segment_ids_str = \" \".join(map(str, self.segment_ids))\n",
    "        masked_lm_positions_str = \" \".join(map(str, self.masked_lm_positions))\n",
    "        masked_lm_labels_str = \" \".join([str(label) for label in self.masked_lm_labels])\n",
    "        return f\"Tokens: {tokens_str}\\nSegment IDs: {segment_ids_str}\\n\" \\\n",
    "               f\"Is Random Next: {self.is_random_next}\\n\" \\\n",
    "               f\"Masked LM Positions: {masked_lm_positions_str}\\n\" \\\n",
    "               f\"Masked LM Labels: {masked_lm_labels_str}\\n\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "def mask_tokens(tokens, tokenizer, max_predictions_per_seq, rng):\n",
    "    \"\"\"Masks tokens and returns masked tokens and corresponding labels.\"\"\"\n",
    "    output_tokens = tokens[:]\n",
    "    output_labels = [-1] * len(tokens)  # Initialize labels with -1 (no change)\n",
    "\n",
    "    # Determine which tokens can be masked\n",
    "    candidate_indices = [\n",
    "        i for i, token in enumerate(tokens) \n",
    "        if token not in [tokenizer.cls_token, tokenizer.sep_token]\n",
    "    ]\n",
    "    rng.shuffle(candidate_indices)\n",
    "    num_masked = min(max_predictions_per_seq, len(candidate_indices) * 15 // 100)\n",
    "    \n",
    "    for index in candidate_indices[:num_masked]:\n",
    "        random_choice = rng.random()\n",
    "        # 80% replace with [MASK], 10% random token, 10% unchanged\n",
    "        if random_choice < 0.8:\n",
    "            output_tokens[index] = tokenizer.mask_token\n",
    "        elif random_choice < 0.9:\n",
    "            output_tokens[index] = random.choice(list(tokenizer.vocab.keys()))\n",
    "        \n",
    "        output_labels[index] = tokenizer.convert_tokens_to_ids(tokens[index])\n",
    "\n",
    "    return output_tokens, output_labels\n",
    "\n",
    "def truncate_and_process(tokens_a, tokens_b, max_seq_length, tokenizer, max_predictions_per_seq, instances, rng, is_random_next):\n",
    "    # Truncate tokens_a and tokens_b if their combined length is too long\n",
    "    while len(tokens_a) + len(tokens_b) + 3 > max_seq_length:\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "    tokens = ['[CLS]'] + tokens_a + ['[SEP]'] + tokens_b + ['[SEP]']\n",
    "    segment_ids = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "    masked_tokens, masked_labels = mask_tokens(tokens, tokenizer, max_predictions_per_seq, rng)\n",
    "\n",
    "    # Convert masked_tokens to IDs\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(masked_tokens)  # Ensure this returns integers\n",
    "    instance = TrainingInstance(\n",
    "        tokens=token_ids,\n",
    "        segment_ids=segment_ids,\n",
    "        masked_lm_positions=[i for i, label in enumerate(masked_labels) if label != -1],\n",
    "        masked_lm_labels=[label for label in masked_labels if label != -1],\n",
    "        is_random_next=int(is_random_next)\n",
    "    )\n",
    "    # instance = {\n",
    "    #     'tokens': token_ids,\n",
    "    #     'segment_ids': segment_ids,\n",
    "    #     'masked_lm_positions': [i for i, label in enumerate(masked_labels) if label != -1],\n",
    "    #     'masked_lm_labels': [label for label in masked_labels if label != -1],\n",
    "    #     'is_random_next': int(is_random_next)\n",
    "    # }\n",
    "    instances.append(instance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "20fa8102-4e13-4497-840c-a04204960d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "vocab_size = tokenizer.vocab_size  # Smaller vocabulary size for simplicity\n",
    "print(vocab_size)\n",
    "num_layers = 4  # Fewer layers\n",
    "d_model = 768  # Smaller dimensionality\n",
    "num_heads = 4\n",
    "dff = 3072\n",
    "segment_size = 2\n",
    "max_seq_length = 32\n",
    "max_predictions_per_seq=5\n",
    "batch_size = 16\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6eeb5833-0ffe-475d-b799-3e5fa64f7f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens A: ['ana', '##rch', '##ism', 'is', 'a', 'political', 'philosophy', 'and', 'movement', 'that', 'is', 'sc', '##ept', '##ical', 'of'], len: 15\n",
      "Tokens B: ['for', 'the', 'abolition', 'of', 'the', 'state', ',', 'which', 'it', 'holds']\n",
      "Is random next: False\n",
      "\n",
      "Tokens A: ['for', 'the', 'abolition', 'of', 'the', 'state', ',', 'which', 'it', 'holds', 'to', 'be', 'unnecessary', ','], len: 14\n",
      "Tokens B: ['the', 'far', '##thest', 'left', 'of', 'the', 'political', 'spectrum', ',', 'it']\n",
      "Is random next: False\n",
      "\n",
      "Tokens A: ['the', 'far', '##thest', 'left', 'of', 'the', 'political', 'spectrum', ',', 'it', 'is', 'usually', 'described', 'alongside', 'communal'], len: 15\n",
      "Tokens B: ['and', 'has', 'a', 'strong', 'historical', 'association', 'with', 'anti', '-', 'capitalism']\n",
      "Is random next: False\n",
      "\n",
      "Tokens A: ['and', 'has', 'a', 'strong', 'historical', 'association', 'with', 'anti', '-', 'capitalism', 'and', 'socialism', '.', 'humans'], len: 14\n",
      "Tokens B: ['fiction', 'due', 'to', 'the', 'fact', 'that', 'the', 'ruling', 'class', 'is']\n",
      "Is random next: True\n",
      "\n",
      "Tokens A: [',', 'or', 'empires', '.', 'with', 'the', 'rise', 'of', 'organised', 'hierarchical', 'bodies', ',', 'sc', '##ept', '##ici'], len: 15\n",
      "Tokens B: ['ana', '##rch', '##ism', 'emerged', 'from', 'the', 'enlightenment', '.', 'during', 'the']\n",
      "Is random next: False\n",
      "\n",
      "Tokens A: ['ana', '##rch', '##ism', 'emerged', 'from', 'the', 'enlightenment', '.', 'during', 'the', 'latter', 'half', 'of', 'the'], len: 14\n",
      "Tokens B: ['coe', '##rc', '##ive', 'society', ',', 'and', 'a', 'suggestion', 'on', 'how']\n",
      "Is random next: True\n",
      "\n",
      "Tokens A: ['the', 'world', 'and', 'had', 'a', 'significant', 'role', 'in', 'workers', \"'\", 'struggles', 'for', 'emancipation', '.', 'various'], len: 15\n",
      "Tokens B: ['the', 'ideological', 'development', 'of', 'what', 'became', 'the', 'era', 'of', 'classical']\n",
      "Is random next: True\n",
      "\n",
      "Tokens A: [',', 'most', 'notably', 'in', 'the', 'paris', 'commune', ',', 'the', 'russian', 'civil', 'war', 'and', 'the', 'spanish'], len: 15\n",
      "Tokens B: ['in', 'the', 'last', 'decades', 'of', 'the', '20th', 'and', 'into', 'the']\n",
      "Is random next: False\n",
      "\n",
      "Tokens A: ['in', 'the', 'last', 'decades', 'of', 'the', '20th', 'and', 'into', 'the', '21st', 'century', ',', 'the'], len: 14\n",
      "Tokens B: ['order', 'to', 'meet', 'its', 'ideal', 'ends', 'which', 'can', 'be', 'broadly']\n",
      "Is random next: False\n",
      "\n",
      "Tokens A: ['order', 'to', 'meet', 'its', 'ideal', 'ends', 'which', 'can', 'be', 'broadly', 'separated', 'into', 'revolutionary', 'and', 'evolutionary'], len: 15\n",
      "Tokens B: ['aim', 'to', 'bring', 'down', 'authority', 'and', 'state', ',', 'having', 'taken']\n",
      "Is random next: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_bert_pretraining_instances_in_chunks(file_path, chunk_size=1048576, \n",
    "                          doc_boundary_pattern=r'ARTICLE-\\d+-\\d+-https:\\/\\/\\S+',\n",
    "                         test_print=10, max_seq_length=max_seq_length, \n",
    "                         max_predictions_per_seq=max_predictions_per_seq, \n",
    "                         dupe_factor=5, random_seed=12345, nsp_enabled=True,\n",
    "                                               tokenizer=tokenizer):\n",
    "    # tokenizer = BertTokenizer.from_pretrained(args.vocab_file, do_lower_case=args.do_lower_case)  ## is for custom vocab\n",
    "    # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') ### This loads the default vocab for bert pre-training\n",
    "    rng = random.Random(random_seed)\n",
    "    buffer = ''\n",
    "    instances = []    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        while True:\n",
    "            chunk = file.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            buffer += chunk\n",
    "            documents = re.split(doc_boundary_pattern, buffer, flags=re.MULTILINE)            \n",
    "            if documents and not re.match(doc_boundary_pattern, documents[-1]):\n",
    "                buffer = documents.pop()\n",
    "            else:\n",
    "                buffer = ''            \n",
    "            for doc in documents:\n",
    "                if not doc.strip():\n",
    "                    continue\n",
    "                tokenized_doc = tokenizer.tokenize(doc)\n",
    "                sequences = [tokenized_doc[i:i+max_seq_length] for i in range(0, len(tokenized_doc), max_seq_length)]                \n",
    "                for j in range(len(sequences) - 1):\n",
    "                    tokens_a = sequences[j]\n",
    "                    if rng.random() > 0.5 or not nsp_enabled:\n",
    "                        is_random_next = True\n",
    "                        tokens_b = sequences[rng.randint(0, len(sequences) - 1)]\n",
    "                    else:\n",
    "                        is_random_next = False\n",
    "                        tokens_b = sequences[j + 1]\n",
    "\n",
    "                    truncate_and_process(tokens_a, tokens_b, max_seq_length, tokenizer, max_predictions_per_seq, instances, rng, is_random_next)\n",
    "\n",
    "                    if test_print > 0:\n",
    "                        print(f\"Tokens A: {tokens_a}, len: {len(tokens_a)}\")\n",
    "                        print(f\"Tokens B: {tokens_b[:10]}\")\n",
    "                        print(f\"Is random next: {is_random_next}\\n\")\n",
    "                        test_print -= 1\n",
    "                        if test_print == 0:\n",
    "                            return instances\n",
    "    return instances  # Return all instances for further processing or training\n",
    "file_path = 'wiki_articles_with_seperator.txt'\n",
    "res = create_bert_pretraining_instances_in_chunks(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c44c0535-65cd-4d1b-8fd4-72937bd3b4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(res[0])\n",
    "# # d = res[0]\n",
    "# print(d.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5d6d33c4-5fe7-4be8-a307-927b9a57db6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_instances_as_parquet(instances, file_path, num_instances=1000, small=False):\n",
    "    if small:\n",
    "        instances = instances[:num_instances]\n",
    "    data = {\n",
    "        'input_ids': [],\n",
    "        'segment_ids': [],\n",
    "        'masked_lm_positions': [],\n",
    "        'mlm_labels': [],\n",
    "        'nsp_labels': []\n",
    "    }\n",
    "    for instance in instances:\n",
    "        data['input_ids'].append(instance.tokens)\n",
    "        data['segment_ids'].append(instance.segment_ids)\n",
    "        data['masked_lm_positions'].append(instance.masked_lm_positions)\n",
    "        data['mlm_labels'].append(instance.masked_lm_labels)\n",
    "        data['nsp_labels'].append(instance.is_random_next)\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_parquet(file_path, engine='pyarrow')\n",
    "instances = create_bert_pretraining_instances_in_chunks(file_path, test_print=0)\n",
    "save_instances_as_parquet(instances, 'pretraining_bert_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9869f705-2812-410d-9988-8c6a469b9ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: {'input_ids': <tf.Tensor: shape=(16, 32), dtype=int32, numpy=\n",
      "array([[  101,  9617,   103,  2964,  2003,  1037,  2576,  4695,  1998,\n",
      "         2929,  2008,  2003,  8040,   103,  7476,   103,   102,  2005,\n",
      "          103, 15766,  1997,  1996,  2110,  1010,  2029,  2009,  4324,\n",
      "         2000,  2022, 14203,  1010,   102],\n",
      "       [  101,  2005,  1996, 15766,  1997,   103,  2110,  1010,   103,\n",
      "         2009,  4324,  2000,   103,   103,  1010,   102,  1996,  2521,\n",
      "        20515,  2187,  1997,  1996,  2576,  8674,  1010,  2009,  2003,\n",
      "         2788,  2649,  4077, 15029,   102],\n",
      "       [  101,  1996,  2521, 20515,   103,  1997,  1996,  2576,  8674,\n",
      "         1010,  2009,  2003,  2788,  2649,  4077, 15029,   102,  1998,\n",
      "          103,  1037,  2844,  3439,   103,  2007,  3424,   103, 16498,\n",
      "         1998, 14649,  1012,  4286,   102],\n",
      "       [  101,  1998,  2038,  1037,  2844,  3439,  2523,  2007,  3424,\n",
      "         1011, 16498,  1998, 14649,  1012,  4286,   102,  4349,  2349,\n",
      "          103,  1996,  2755,  2008,  1996,  6996,  2465,  2003,  5664,\n",
      "          103,   103,   103,  1997,   102],\n",
      "       [  101,  1010,  2030,   103,  1012,  2007,  1996,  4125,  1997,\n",
      "         7362,   103,  4230,  1010,  8040, 23606,   103,   102,  9617,\n",
      "        11140,  2964,  6003,  2013,  1996, 16724,  1012,  2076,  1996,\n",
      "          103,  2431,  1997,  1996,   102],\n",
      "       [  101,   103, 11140,  2964,  6003,  2013,  1996, 16724,  1012,\n",
      "         2076,  1996,  3732,  2431,   103,  1996,   102, 24873,   103,\n",
      "         3512,  2554,  1010,  1998,  1037, 10293,  2006,   103,  2000,\n",
      "         2552,  2000,  7323,  1996,   102],\n",
      "       [  101,  1996,  2088,  1998,  2018,  1037,  3278,  2535,  1999,\n",
      "         3667,  1005, 11785,  2005, 22656,   103,   103,   102,  1996,\n",
      "          103,  2458,  1997,  2054,   103,  1996,  3690,  1997,  4556,\n",
      "         9617, 11140,  2964,  1012,   102],\n",
      "       [  101,  1010,  2087,   103,  1999,  1996,  3000,  5715,  1010,\n",
      "         1996,   103,  2942,  2162,  1998,  1996,  3009,   102,  1999,\n",
      "         1996,  2197,  5109,  1997,  1996,  3983,  1998,   103,  1996,\n",
      "         7398,  2301,  1010,   103,   102],\n",
      "       [  101,  1999,  1996,  2197,  5109,  1997,  1996,  3983,   103,\n",
      "         2046,   103,  7398,  2301,  1010,  1996,   102,  2344,  2000,\n",
      "         3113,  2049,   103,  4515,  2029,  2064,  2022, 13644,  5459,\n",
      "          103,  6208,  1998, 12761,   102],\n",
      "       [  101,  2344,  2000,   103,  2049,  7812,  4515,  2029,  2064,\n",
      "         2022,   103,  5459,  2046,  6208,  1998, 12761,   102,  6614,\n",
      "          103,  3288,  2091,  3691,  1998,  2110,  1010,  2383,  2579,\n",
      "          103,  6355,  2735,  1999,   102],\n",
      "       [  101,  6614,  2000,  3288,  2091,   103,  1998,  2110,  1010,\n",
      "         2383,   103,  1037,  6355,  2735,  1999,   102,  2066,  1012,\n",
      "        18448,  2245,  1010,  6256,  1010,  1998, 10975,  8528,  2483,\n",
      "          103,  2209,  1037,  2112,   102],\n",
      "       [  101,  2066,  1012,   103,  2245,  1010,   103,  1010,  1998,\n",
      "        10975,  8528,  2483,  2031,  2209,  1037,  2112,   102,  1996,\n",
      "         4598,  1997,   103,   103,  2110,  2021,  4447,  2008,  4480,\n",
      "         2031,  2053,  7191, 14987,   102],\n",
      "       [  101, 16058,   103,  1010,   103, 24558,  2030, 26425,  2078,\n",
      "         1012, 26803,  1010, 18444,  1010,  1998,  6210,   102,  1010,\n",
      "         3574,  1000,  2302,  1037,  7786,  1000,  1010,  3605,  1997,\n",
      "         1996, 17576,  2019,  1011,   102],\n",
      "       [  101,  1010,  3574,  1000,   103,  1037,  7786,  1000,  1010,\n",
      "         3605,  1997,  1996, 17576,  2019,  1011,   102,  2070, 18448,\n",
      "         2015,   103,  2076,  2023,  2558,  1012,  2750,  5936,  1010,\n",
      "          103,   103, 24935,  4194,   102],\n",
      "       [  101,  1007,  1012,  1996, 16809,  1011,  2003,  2213, 14796,\n",
      "         1996, 17859,  2783,  2008,   103,  2015, 26395,   102,  5483,\n",
      "         1010,  1998,  3290,  1012,   103,   103, 15984,  6186,  2967,\n",
      "         2024,  2124,   103, 13249,   102],\n",
      "       [  101, 16710,  2683,  1025,  2220,  2394,  8192,   103,  7902,\n",
      "         2098,  1037,  3168,  1997,  8761,  1012,  2536,   102,  2116,\n",
      "         5328,  2007,  2101, 18448,  2015,   103,  2116, 24517,   103,\n",
      "         1996,  3708,   103,  2107,   102]], dtype=int32)>, 'segment_ids': <tf.Tensor: shape=(16, 32), dtype=int32, numpy=\n",
      "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "Labels: {'masked_lm_positions': <tf.Tensor: shape=(16, 4), dtype=int32, numpy=\n",
      "array([[ 2, 13, 15, 18],\n",
      "       [ 5,  8, 12, 13],\n",
      "       [ 4, 18, 22, 25],\n",
      "       [18, 27, 28, 29],\n",
      "       [ 3, 10, 15, 27],\n",
      "       [ 1, 13, 17, 25],\n",
      "       [14, 15, 18, 22],\n",
      "       [ 3, 10, 25, 30],\n",
      "       [ 8, 10, 20, 27],\n",
      "       [ 3, 10, 18, 27],\n",
      "       [ 5,  9, 10, 27],\n",
      "       [ 3,  6, 20, 21],\n",
      "       [ 2,  4,  5, 21],\n",
      "       [ 4, 19, 27, 28],\n",
      "       [13, 22, 23, 29],\n",
      "       [ 7, 23, 26, 29]], dtype=int32)>, 'mlm_labels': <tf.Tensor: shape=(16, 4), dtype=int32, numpy=\n",
      "array([[11140, 23606,  1997,  1996],\n",
      "       [ 1996,  2029,  2022, 14203],\n",
      "       [ 2187,  2038,  2523,  1011],\n",
      "       [ 2000,  2013,  1996,  2717],\n",
      "       [23560, 25835, 28775,  3732],\n",
      "       [ 9617,  1997, 11890,  2129],\n",
      "       [ 1012,  2536, 17859,  2150],\n",
      "       [ 5546,  2845,  2046,  1996],\n",
      "       [ 1998,  1996,  7812,  2046],\n",
      "       [ 3113, 13644,  2000,  1037],\n",
      "       [ 3691,  2383,  2579,  2031],\n",
      "       [18448,  6256,  1037, 10124],\n",
      "       [20316,  6355,  1010,  1037],\n",
      "       [ 2302,  4233, 18448,  2015],\n",
      "       [ 7927, 16830,  2304,  2005],\n",
      "       [ 2015,  1012,  1997,  2301]], dtype=int32)>, 'nsp_labels': <tf.Tensor: shape=(16,), dtype=int32, numpy=array([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(file_path, batch_size=32):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    tensor_dict = {}\n",
    "    for col in df.columns:\n",
    "        if isinstance(df[col].values[0], list) or isinstance(df[col].values[0], np.ndarray):\n",
    "            # Handle list or array: pad sequences and convert to tensor\n",
    "            padded_array = tf.keras.preprocessing.sequence.pad_sequences(df[col].tolist(), padding='post', dtype='int32')\n",
    "            tensor_dict[col] = tf.convert_to_tensor(padded_array, dtype=tf.int32)\n",
    "        else:\n",
    "            # Convert scalar values directly to tensor\n",
    "            tensor_dict[col] = tf.convert_to_tensor(df[col].to_numpy(dtype=np.int32), dtype=tf.int32)\n",
    "\n",
    "    # Split the tensors into inputs and labels\n",
    "    inputs = {k: tensor_dict[k] for k in ['input_ids', 'segment_ids']}\n",
    "    labels = {k: tensor_dict[k] for k in ['masked_lm_positions', 'mlm_labels', 'nsp_labels']}\n",
    "    # Combine into a single dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_dataset = load_dataset('pretraining_bert_data.parquet', batch_size=batch_size)\n",
    "\n",
    "for inputs, labels in train_dataset.take(1):\n",
    "    print(\"Inputs:\", inputs)\n",
    "    print(\"Labels:\", labels)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aeddd2-64f5-49c8-b563-224d66e4ffac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ecd859cf-88b2-4da8-9eaa-b71c0cb1ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert these articles to pretraining data \n",
    "# !python create_pretraining_data.py --vocab_file vocab.txt --input_text input_text.txt --output_tfrecord output.tfrecord --do_lower_case --nsp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d246b53-8a4b-4e74-b802-da85d3f58f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "84d75159-e176-4d89-80a9-fa6b52aeec18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[  101  9617   103  2964  2003  1037  2576  4695  1998  2929  2008  2003\n",
      "  8040   103  7476   103   102  2005   103 15766  1997  1996  2110  1010\n",
      "  2029  2009  4324  2000  2022 14203  1010   102], shape=(32,), dtype=int32)\n",
      "tf.Tensor([ 2 13 15 18], shape=(4,), dtype=int32)\n",
      "tf.Tensor([11140 23606  1997  1996], shape=(4,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "single_test_instance = iter(train_dataset.take(1)).next()\n",
    "single_input_tuple = single_test_instance[0]['input_ids'], single_test_instance[0]['segment_ids']\n",
    "# print(single_test_instance)\n",
    "print(single_test_instance[0]['input_ids'][0])\n",
    "print(single_test_instance[1]['masked_lm_positions'][0])\n",
    "print(single_test_instance[1]['mlm_labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ed3d1efc-c514-4806-9707-3be670c4fc30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (16, 32, 256)\n",
      "Output shape: tf.Tensor(\n",
      "[[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]], shape=(16, 32), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "class PositionalAndSegmentEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, segment_size, d_model, max_pos=2048, pos_dropout=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)  # Initialize the superclass (Layer)\n",
    "        self.d_model = d_model  # Store the dimensionality of the model embeddings\n",
    "        self.token_embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.segment_embedding = tf.keras.layers.Embedding(segment_size, d_model)\n",
    "        self.pos_encoding = encode_pos_sin_cosine(max_pos, d_model, debug=False)\n",
    "        self.dropout = tf.keras.layers.Dropout(pos_dropout)\n",
    "\n",
    "    def compute_mask(self, inputs, *args, **kwargs):\n",
    "        # Assuming the input structure is a tuple of (tokens, segments)\n",
    "        token_inputs, _ = inputs\n",
    "        return self.token_embedding.compute_mask(token_inputs, *args, **kwargs)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # Expect inputs to be a tuple (token_inputs, segment_inputs)\n",
    "        token_inputs, segment_inputs = inputs\n",
    "        tokens = self.token_embedding(token_inputs)  # Token embeddings\n",
    "        segments = self.segment_embedding(segment_inputs)  # Segment embeddings       \n",
    "        x = tokens + segments\n",
    "        # Scale the embeddings by the square root of the embedding dimension size\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        # Add positional encoding to the combined embeddings, sliced to match the input length\n",
    "        length = tf.shape(x)[1]\n",
    "        # pos_encodings = tf.reshape(self.pos_encoding, (1, -1, self.d_model))[:, :length, :]\n",
    "        pos_encodings = tf.cast(tf.reshape(self.pos_encoding, (1, -1, self.d_model))[:, :tf.shape(x)[1], :], tf.float32)\n",
    "        x += pos_encodings\n",
    "        x = self.dropout(x, training=training)\n",
    "        return x\n",
    "\n",
    "embedding_layer = PositionalAndSegmentEmbedding(vocab_size=vocab_size, segment_size=2, d_model=256)\n",
    "\n",
    "# Extract a single batch from the dataset\n",
    "for inputs, labels in train_dataset.take(1):\n",
    "    # The inputs dictionary contains 'input_ids' and 'segment_ids'\n",
    "    input_ids = inputs['input_ids']\n",
    "    segment_ids = inputs['segment_ids']\n",
    "\n",
    "    # Call the embedding layer\n",
    "    embeddings = embedding_layer((input_ids, segment_ids))\n",
    "\n",
    "    # Print the output shape\n",
    "    print(\"Output shape:\", embeddings.shape)\n",
    "    print(\"Output shape:\", embeddings._keras_mask)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a0ee2176-1307-4de3-b836-7f4762537010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 32, 768)\n"
     ]
    }
   ],
   "source": [
    "class TransformerEncoderV4(TransformerEncoderV3):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, vocab_size, segment_size, max_pos=2048, pos_dropout=0.1, **kwargs):\n",
    "        super(TransformerEncoderV4, self).__init__(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff, vocab_size=vocab_size, max_pos=max_pos, **kwargs)\n",
    "        # Use the custom embedding layer that handles tokens, segments, and positional encodings\n",
    "        self.embedding_layer = PositionalAndSegmentEmbedding(vocab_size, segment_size, d_model, max_pos, pos_dropout)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_ids, segment_ids = inputs\n",
    "        # The embedding layer now handles everything including token, segment, and positional embeddings\n",
    "        x = self.embedding_layer((input_ids, segment_ids), training=training)\n",
    "        x = self.enc_layers_0(x, training=training)\n",
    "        for i in range(self.remaining_layers):\n",
    "            x = self.enc_layers[i](x, training=training)\n",
    "        return x\n",
    "tren = TransformerEncoderV4(num_layers, d_model, num_heads, dff, vocab_size, segment_size, max_pos=max_seq_length)\n",
    "\n",
    "encoder_out = tren(single_input_tuple)\n",
    "print(encoder_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1750f3c4-7f2b-40c3-9cf2-ab166ae0a695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "028da786-f0f6-4280-8228-4d3b80ce27cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mlm_output': <tf.Tensor: shape=(16, 32, 30522), dtype=float32, numpy=\n",
      "array([[[2.9132796e-05, 4.1347856e-05, 4.0508101e-05, ...,\n",
      "         3.1462605e-05, 2.2753575e-05, 2.6322496e-05],\n",
      "        [2.2634018e-05, 4.1381449e-05, 3.8698367e-05, ...,\n",
      "         3.4007335e-05, 2.3044084e-05, 3.1454834e-05],\n",
      "        [2.6919319e-05, 4.7942154e-05, 4.1582123e-05, ...,\n",
      "         3.2779881e-05, 2.1349722e-05, 3.0214544e-05],\n",
      "        ...,\n",
      "        [3.4179207e-05, 4.2355263e-05, 3.5524554e-05, ...,\n",
      "         2.5386791e-05, 2.6338388e-05, 2.3977718e-05],\n",
      "        [4.3079119e-05, 3.3040353e-05, 3.4941444e-05, ...,\n",
      "         2.5725280e-05, 2.5267344e-05, 2.2713668e-05],\n",
      "        [3.5298261e-05, 4.1672167e-05, 3.6357946e-05, ...,\n",
      "         2.6349733e-05, 2.5629966e-05, 2.6083797e-05]],\n",
      "\n",
      "       [[2.7661485e-05, 4.0958759e-05, 4.0110586e-05, ...,\n",
      "         3.1388656e-05, 2.4603614e-05, 2.8394028e-05],\n",
      "        [2.2942872e-05, 4.1769668e-05, 3.7569400e-05, ...,\n",
      "         3.5041659e-05, 2.2891305e-05, 2.5415140e-05],\n",
      "        [2.4703879e-05, 4.3949010e-05, 3.4265020e-05, ...,\n",
      "         3.0140607e-05, 1.9860938e-05, 2.8716671e-05],\n",
      "        ...,\n",
      "        [3.1411924e-05, 3.6475543e-05, 3.1956610e-05, ...,\n",
      "         3.1407497e-05, 2.9663388e-05, 2.6796972e-05],\n",
      "        [3.3284996e-05, 3.9211300e-05, 3.2958709e-05, ...,\n",
      "         2.7635808e-05, 3.1431740e-05, 2.6842012e-05],\n",
      "        [3.2301123e-05, 4.2544743e-05, 3.2541222e-05, ...,\n",
      "         2.8642611e-05, 2.6917418e-05, 3.0224206e-05]],\n",
      "\n",
      "       [[2.8221473e-05, 4.1982919e-05, 3.7710844e-05, ...,\n",
      "         3.0410029e-05, 2.5162522e-05, 3.1122756e-05],\n",
      "        [2.5094920e-05, 4.2071606e-05, 3.2474261e-05, ...,\n",
      "         2.7507711e-05, 2.0302770e-05, 3.1209853e-05],\n",
      "        [1.9722944e-05, 3.7088474e-05, 3.3495638e-05, ...,\n",
      "         2.7804168e-05, 2.4976394e-05, 3.4946213e-05],\n",
      "        ...,\n",
      "        [3.2868957e-05, 4.7842292e-05, 3.6091398e-05, ...,\n",
      "         3.6412202e-05, 2.3965378e-05, 3.2851822e-05],\n",
      "        [3.3382239e-05, 4.2975393e-05, 4.0258077e-05, ...,\n",
      "         3.1920506e-05, 2.3477780e-05, 2.7066864e-05],\n",
      "        [3.2137552e-05, 4.3990363e-05, 3.3651242e-05, ...,\n",
      "         2.6422735e-05, 2.4163306e-05, 2.8039349e-05]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[2.7440423e-05, 4.0022729e-05, 4.0622715e-05, ...,\n",
      "         3.0626419e-05, 2.3056531e-05, 2.8009614e-05],\n",
      "        [3.4377023e-05, 4.1065337e-05, 3.8352318e-05, ...,\n",
      "         2.5147345e-05, 2.1862277e-05, 2.7280163e-05],\n",
      "        [2.4778677e-05, 4.9206348e-05, 5.1738924e-05, ...,\n",
      "         2.9149611e-05, 1.8338278e-05, 2.8116485e-05],\n",
      "        ...,\n",
      "        [3.5523695e-05, 3.5600624e-05, 3.7006612e-05, ...,\n",
      "         2.4431747e-05, 2.6108681e-05, 2.3747349e-05],\n",
      "        [3.6892747e-05, 3.2028587e-05, 3.4015295e-05, ...,\n",
      "         2.7991240e-05, 2.7986580e-05, 2.9327442e-05],\n",
      "        [3.4873527e-05, 3.6769459e-05, 3.6534144e-05, ...,\n",
      "         2.5541081e-05, 2.6003445e-05, 2.6357066e-05]],\n",
      "\n",
      "       [[2.9680426e-05, 4.4968860e-05, 4.0145052e-05, ...,\n",
      "         3.0769454e-05, 2.4505614e-05, 3.0640691e-05],\n",
      "        [2.7710312e-05, 5.1967381e-05, 4.6505735e-05, ...,\n",
      "         3.3821882e-05, 2.4504236e-05, 2.5827398e-05],\n",
      "        [2.4713259e-05, 4.6489360e-05, 4.5432062e-05, ...,\n",
      "         3.7867463e-05, 2.2835073e-05, 3.0643536e-05],\n",
      "        ...,\n",
      "        [3.7482750e-05, 3.7437036e-05, 3.7144906e-05, ...,\n",
      "         3.0657855e-05, 2.4857094e-05, 2.5249121e-05],\n",
      "        [3.8156009e-05, 3.2343214e-05, 2.7883072e-05, ...,\n",
      "         3.1366351e-05, 2.1735073e-05, 2.4801509e-05],\n",
      "        [3.2837324e-05, 3.8656541e-05, 3.3536813e-05, ...,\n",
      "         2.6740416e-05, 2.5470468e-05, 2.6017678e-05]],\n",
      "\n",
      "       [[2.8631459e-05, 3.9177670e-05, 4.4035027e-05, ...,\n",
      "         3.3664750e-05, 2.3628652e-05, 2.8441456e-05],\n",
      "        [3.2478438e-05, 3.8246675e-05, 3.9756513e-05, ...,\n",
      "         3.6011450e-05, 2.1260405e-05, 2.5706961e-05],\n",
      "        [2.4976422e-05, 4.1365271e-05, 4.4045875e-05, ...,\n",
      "         3.3924345e-05, 2.4078226e-05, 3.1722080e-05],\n",
      "        ...,\n",
      "        [3.7924503e-05, 4.4565280e-05, 3.2882046e-05, ...,\n",
      "         2.9549748e-05, 2.6991003e-05, 2.7231245e-05],\n",
      "        [3.1955988e-05, 4.2311538e-05, 3.0820793e-05, ...,\n",
      "         2.8016662e-05, 2.7995564e-05, 3.0042653e-05],\n",
      "        [3.0928801e-05, 4.0872164e-05, 3.1351807e-05, ...,\n",
      "         2.3651037e-05, 2.4519475e-05, 3.1010379e-05]]], dtype=float32)>, 'nsp_output': <tf.Tensor: shape=(16, 1), dtype=float32, numpy=\n",
      "array([[0.7912486 ],\n",
      "       [0.9061719 ],\n",
      "       [0.92429197],\n",
      "       [0.8110436 ],\n",
      "       [0.8779576 ],\n",
      "       [0.88730544],\n",
      "       [0.9095709 ],\n",
      "       [0.8719838 ],\n",
      "       [0.94691294],\n",
      "       [0.8275354 ],\n",
      "       [0.77987105],\n",
      "       [0.86241525],\n",
      "       [0.91664517],\n",
      "       [0.85568523],\n",
      "       [0.8565828 ],\n",
      "       [0.7978895 ]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "# class BERT(tf.keras.Model):\n",
    "#     def __init__(self, num_layers, d_model, num_heads, dff, vocab_size, segment_size, max_seq_length=128, rate=0.1):\n",
    "#         super(BERT, self).__init__()\n",
    "#         self.encoder = TransformerEncoderV4(num_layers=num_layers, d_model=d_model, num_heads=num_heads,\n",
    "#                                             dff=dff, vocab_size=vocab_size, segment_size=segment_size,\n",
    "#                                             max_pos=max_seq_length, pos_dropout=rate)\n",
    "#         self.mlm_dense = tf.keras.layers.Dense(vocab_size, activation='softmax')  # Ensures output shape is [batch, seq_length, vocab_size]\n",
    "#         self.nsp_dense = tf.keras.layers.Dense(1, activation='sigmoid')   \n",
    "    \n",
    "#     def call(self, inputs, training=False):\n",
    "#         x = self.encoder((inputs['input_ids'], inputs['segment_ids']), training=training)\n",
    "#         mlm_output = self.mlm_dense(x)  # Check shapes here\n",
    "#         nsp_output = self.nsp_dense(x[:, 0, :])\n",
    "#         return {'mlm_output': mlm_output, 'nsp_output': nsp_output}\n",
    "\n",
    "# bert_model = BERT(num_layers, d_model, num_heads, \n",
    "#                   dff, vocab_size, segment_size)\n",
    "# bert_out = bert_model(single_test_instance[0])\n",
    "# print(bert_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "76776e25-982b-4097-b2f2-30ee248feeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 32, 30522)\n",
      "(16, 1)\n"
     ]
    }
   ],
   "source": [
    "class BERT(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, vocab_size, segment_size, max_seq_length=128, rate=0.1):\n",
    "        super(BERT, self).__init__()\n",
    "        self.encoder = TransformerEncoderV4(num_layers=num_layers, d_model=d_model, num_heads=num_heads,\n",
    "                                            dff=dff, vocab_size=vocab_size, segment_size=segment_size,\n",
    "                                            max_pos=max_seq_length, pos_dropout=rate)\n",
    "        self.mlm_dense_transform = tf.keras.layers.Dense(d_model, activation='gelu')  # Transform layer for MLM\n",
    "        self.mlm_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-12)  # Layer normalization for MLM\n",
    "        self.mlm_dense = tf.keras.layers.Dense(vocab_size)  # Ensures output shape is [batch, seq_length, vocab_size]\n",
    "        self.nsp_dense = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.encoder((inputs['input_ids'], inputs['segment_ids']), training=training)\n",
    "        \n",
    "        # Apply dense transformation and layer normalization for MLM\n",
    "        mlm_output = self.mlm_dense_transform(x)\n",
    "        mlm_output = self.mlm_layer_norm(mlm_output)\n",
    "        mlm_output = self.mlm_dense(mlm_output)\n",
    "        \n",
    "        nsp_output = self.nsp_dense(x[:, 0, :])\n",
    "        return {'mlm_output': mlm_output, 'nsp_output': nsp_output}\n",
    "\n",
    "# Instantiate and compile the BERT model\n",
    "bert_model = BERT(num_layers, d_model, num_heads, \n",
    "                  dff, vocab_size, segment_size)\n",
    "bert_out = bert_model(single_test_instance[0])\n",
    "print(bert_out['mlm_output'].shape  )\n",
    "print(bert_out['nsp_output'].shape  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "197c1629-42be-477e-89bd-b82b5c95ae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss_object_nsp = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "\n",
    "def compute_mlm_loss(masked_positions, masked_labels, logits):\n",
    "    # Gather the logits at the masked positions\n",
    "    masked_logits = tf.gather(logits, masked_positions, batch_dims=1, axis=1) \n",
    "    # tf.print(\"masked_positions:\", masked_positions[0], summarize=-1)\n",
    "    # tf.print(\"masked_labels:\", masked_labels[0], summarize=-1)\n",
    "    # tf.print(\"masked_logits:\", masked_logits[0].shape, summarize=-1)\n",
    "    # Ensure that the masked_labels used here are the correct length and match the number of masked_positions\n",
    "    mlm_loss = tf.keras.losses.sparse_categorical_crossentropy(masked_labels, masked_logits, from_logits=True)\n",
    "    # tf.print(\"mlm_loss:\", mlm_loss[0], summarize=-1)\n",
    "    # Reduce mean across batches if needed or sum as appropriate\n",
    "    return tf.reduce_mean(mlm_loss)\n",
    "    \n",
    "def compute_nsp_loss(labels, logits):\n",
    "    return tf.keras.losses.binary_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "bfe9d7d7-b7a2-433a-a67e-4909874953c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0, Total Loss: 10.3523, MLM Loss: 10.3523, NSP Loss: 0.7588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:05:27.820836: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Step 0, Total Loss: 10.3740, MLM Loss: 10.3740, NSP Loss: 0.8065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:05:29.159169: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Step 0, Total Loss: 10.3871, MLM Loss: 10.3871, NSP Loss: 0.8478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:05:30.473423: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Step 0, Total Loss: 10.3338, MLM Loss: 10.3338, NSP Loss: 0.8719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:05:31.768384: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Step 0, Total Loss: 10.3394, MLM Loss: 10.3394, NSP Loss: 0.8040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:05:33.041288: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Step 0, Total Loss: 10.3431, MLM Loss: 10.3431, NSP Loss: 0.7065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:05:34.386915: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Step 0, Total Loss: 10.3434, MLM Loss: 10.3434, NSP Loss: 0.7179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:05:35.781340: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Step 0, Total Loss: 10.3589, MLM Loss: 10.3589, NSP Loss: 0.8624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:05:37.093158: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Step 0, Total Loss: 10.3311, MLM Loss: 10.3311, NSP Loss: 0.7160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:05:38.403152: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Step 0, Total Loss: 10.3145, MLM Loss: 10.3145, NSP Loss: 0.7148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:05:39.710706: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Step 0, Total Loss: 10.3115, MLM Loss: 10.3115, NSP Loss: 0.6065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:05:41.017094: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Step 0, Total Loss: 10.2941, MLM Loss: 10.2941, NSP Loss: 0.8228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:05:42.326403: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Step 0, Total Loss: 10.2844, MLM Loss: 10.2844, NSP Loss: 0.7140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:05:43.662081: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Step 0, Total Loss: 10.2663, MLM Loss: 10.2663, NSP Loss: 0.7763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:05:45.000540: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Step 0, Total Loss: 10.2994, MLM Loss: 10.2994, NSP Loss: 0.8444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:05:46.370642: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Step 0, Total Loss: 10.2889, MLM Loss: 10.2889, NSP Loss: 0.7690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:05:47.786073: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Step 0, Total Loss: 10.2409, MLM Loss: 10.2409, NSP Loss: 0.7088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:05:49.127251: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Step 0, Total Loss: 10.2258, MLM Loss: 10.2258, NSP Loss: 0.8141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:05:50.400261: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Step 0, Total Loss: 10.2558, MLM Loss: 10.2558, NSP Loss: 0.7559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:05:51.758108: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Step 0, Total Loss: 10.1724, MLM Loss: 10.1724, NSP Loss: 0.7898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:05:53.203768: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Step 0, Total Loss: 10.1910, MLM Loss: 10.1910, NSP Loss: 0.8164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:05:54.603510: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Step 0, Total Loss: 10.1616, MLM Loss: 10.1616, NSP Loss: 0.7280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:05:55.943943: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Step 0, Total Loss: 10.1623, MLM Loss: 10.1623, NSP Loss: 0.9647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:05:57.250937: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Step 0, Total Loss: 10.1477, MLM Loss: 10.1477, NSP Loss: 0.8506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:05:58.543883: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Step 0, Total Loss: 10.1425, MLM Loss: 10.1425, NSP Loss: 0.8144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:05:59.837754: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Step 0, Total Loss: 10.1049, MLM Loss: 10.1049, NSP Loss: 0.4734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:01.123319: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Step 0, Total Loss: 10.0800, MLM Loss: 10.0800, NSP Loss: 0.8051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:02.462482: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Step 0, Total Loss: 10.0795, MLM Loss: 10.0795, NSP Loss: 0.8810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:03.781568: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Step 0, Total Loss: 10.0402, MLM Loss: 10.0402, NSP Loss: 0.7338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:05.084095: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Step 0, Total Loss: 10.0395, MLM Loss: 10.0395, NSP Loss: 0.7840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:06.370703: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Step 0, Total Loss: 10.0148, MLM Loss: 10.0148, NSP Loss: 0.8483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:07.730416: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Step 0, Total Loss: 9.9909, MLM Loss: 9.9909, NSP Loss: 0.7342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:09.144301: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Step 0, Total Loss: 9.9647, MLM Loss: 9.9647, NSP Loss: 0.7196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:10.588495: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Step 0, Total Loss: 9.9498, MLM Loss: 9.9498, NSP Loss: 0.7199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:11.978126: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Step 0, Total Loss: 9.9367, MLM Loss: 9.9367, NSP Loss: 0.6957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:13.326427: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Step 0, Total Loss: 9.9059, MLM Loss: 9.9059, NSP Loss: 0.8324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:14.662159: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Step 0, Total Loss: 9.8886, MLM Loss: 9.8886, NSP Loss: 0.7913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:15.966932: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Step 0, Total Loss: 9.9162, MLM Loss: 9.9162, NSP Loss: 0.8017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:17.294502: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Step 0, Total Loss: 9.8396, MLM Loss: 9.8396, NSP Loss: 0.7087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:18.577463: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40, Step 0, Total Loss: 9.8344, MLM Loss: 9.8344, NSP Loss: 0.8255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:19.867774: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41, Step 0, Total Loss: 9.8138, MLM Loss: 9.8138, NSP Loss: 0.8037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:21.167325: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42, Step 0, Total Loss: 9.8025, MLM Loss: 9.8025, NSP Loss: 0.6386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:22.520093: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43, Step 0, Total Loss: 9.7586, MLM Loss: 9.7586, NSP Loss: 0.7449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:23.929167: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44, Step 0, Total Loss: 9.7457, MLM Loss: 9.7457, NSP Loss: 0.5881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:25.287374: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45, Step 0, Total Loss: 9.7618, MLM Loss: 9.7618, NSP Loss: 0.7520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:26.666982: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46, Step 0, Total Loss: 9.7157, MLM Loss: 9.7157, NSP Loss: 0.7484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:28.173975: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47, Step 0, Total Loss: 9.7121, MLM Loss: 9.7121, NSP Loss: 0.7630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:29.887557: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48, Step 0, Total Loss: 9.6765, MLM Loss: 9.6765, NSP Loss: 0.6966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:31.580456: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49, Step 0, Total Loss: 9.6340, MLM Loss: 9.6340, NSP Loss: 0.7328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:33.208940: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Step 0, Total Loss: 9.6577, MLM Loss: 9.6577, NSP Loss: 0.7028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:34.781596: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51, Step 0, Total Loss: 9.6225, MLM Loss: 9.6225, NSP Loss: 0.7240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:36.317621: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52, Step 0, Total Loss: 9.5863, MLM Loss: 9.5863, NSP Loss: 0.7805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:37.851868: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53, Step 0, Total Loss: 9.5837, MLM Loss: 9.5837, NSP Loss: 0.9090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:39.358865: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54, Step 0, Total Loss: 9.5552, MLM Loss: 9.5552, NSP Loss: 0.9720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:40.806480: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55, Step 0, Total Loss: 9.5236, MLM Loss: 9.5236, NSP Loss: 0.7683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:42.168718: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56, Step 0, Total Loss: 9.4820, MLM Loss: 9.4820, NSP Loss: 0.9051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:43.553318: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57, Step 0, Total Loss: 9.5084, MLM Loss: 9.5084, NSP Loss: 0.7577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:44.935489: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58, Step 0, Total Loss: 9.4902, MLM Loss: 9.4902, NSP Loss: 0.7844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:46.355484: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59, Step 0, Total Loss: 9.4766, MLM Loss: 9.4766, NSP Loss: 0.7764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:47.715680: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60, Step 0, Total Loss: 9.4409, MLM Loss: 9.4409, NSP Loss: 0.7934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:49.051299: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61, Step 0, Total Loss: 9.4356, MLM Loss: 9.4356, NSP Loss: 0.8621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:50.368268: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62, Step 0, Total Loss: 9.3979, MLM Loss: 9.3979, NSP Loss: 0.6562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:51.747969: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63, Step 0, Total Loss: 9.3804, MLM Loss: 9.3804, NSP Loss: 0.7982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:53.056440: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64, Step 0, Total Loss: 9.3805, MLM Loss: 9.3805, NSP Loss: 0.7430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:54.423165: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65, Step 0, Total Loss: 9.3652, MLM Loss: 9.3652, NSP Loss: 0.6402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:55.748900: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66, Step 0, Total Loss: 9.3036, MLM Loss: 9.3036, NSP Loss: 0.8558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:57.055296: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67, Step 0, Total Loss: 9.3270, MLM Loss: 9.3270, NSP Loss: 0.8807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:58.372897: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68, Step 0, Total Loss: 9.2914, MLM Loss: 9.2914, NSP Loss: 0.8570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:06:59.695285: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69, Step 0, Total Loss: 9.2588, MLM Loss: 9.2588, NSP Loss: 0.6241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:07:01.019280: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70, Step 0, Total Loss: 9.2118, MLM Loss: 9.2118, NSP Loss: 0.7096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:07:02.338508: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71, Step 0, Total Loss: 9.2204, MLM Loss: 9.2204, NSP Loss: 0.7138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:07:03.646075: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72, Step 0, Total Loss: 9.2043, MLM Loss: 9.2043, NSP Loss: 0.6461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:07:04.953253: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73, Step 0, Total Loss: 9.1764, MLM Loss: 9.1764, NSP Loss: 0.8302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:07:06.284394: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74, Step 0, Total Loss: 9.1492, MLM Loss: 9.1492, NSP Loss: 0.7267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:07:07.596203: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Step 0, Total Loss: 9.1446, MLM Loss: 9.1446, NSP Loss: 0.8544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:07:08.900148: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76, Step 0, Total Loss: 9.1102, MLM Loss: 9.1102, NSP Loss: 0.7594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:07:10.218968: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77, Step 0, Total Loss: 9.0945, MLM Loss: 9.0945, NSP Loss: 0.8876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:07:11.527941: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78, Step 0, Total Loss: 9.0711, MLM Loss: 9.0711, NSP Loss: 0.7874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:07:12.854529: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79, Step 0, Total Loss: 9.0417, MLM Loss: 9.0417, NSP Loss: 0.7179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:07:14.210091: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80, Step 0, Total Loss: 8.9940, MLM Loss: 8.9940, NSP Loss: 1.0353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:07:15.682850: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81, Step 0, Total Loss: 9.0018, MLM Loss: 9.0018, NSP Loss: 0.9501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:07:17.267686: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82, Step 0, Total Loss: 9.0022, MLM Loss: 9.0022, NSP Loss: 0.7825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:07:18.753736: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83, Step 0, Total Loss: 8.9456, MLM Loss: 8.9456, NSP Loss: 0.6976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:07:20.315559: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84, Step 0, Total Loss: 8.9386, MLM Loss: 8.9386, NSP Loss: 0.6444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:07:21.829121: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85, Step 0, Total Loss: 8.9409, MLM Loss: 8.9409, NSP Loss: 0.7415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:07:23.351961: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86, Step 0, Total Loss: 8.8871, MLM Loss: 8.8871, NSP Loss: 0.8380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:07:24.804257: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87, Step 0, Total Loss: 8.8399, MLM Loss: 8.8399, NSP Loss: 0.9388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:07:26.272296: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88, Step 0, Total Loss: 8.8390, MLM Loss: 8.8390, NSP Loss: 0.7943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:07:27.688584: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89, Step 0, Total Loss: 8.8104, MLM Loss: 8.8104, NSP Loss: 0.8526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:07:29.099037: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90, Step 0, Total Loss: 8.7807, MLM Loss: 8.7807, NSP Loss: 0.8083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:07:30.482832: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91, Step 0, Total Loss: 8.7379, MLM Loss: 8.7379, NSP Loss: 0.9789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:07:31.877849: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92, Step 0, Total Loss: 8.7275, MLM Loss: 8.7275, NSP Loss: 0.8325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:07:33.358974: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93, Step 0, Total Loss: 8.7004, MLM Loss: 8.7004, NSP Loss: 0.7062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:07:34.835102: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94, Step 0, Total Loss: 8.6888, MLM Loss: 8.6888, NSP Loss: 0.8646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:07:36.354653: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95, Step 0, Total Loss: 8.6691, MLM Loss: 8.6691, NSP Loss: 0.7720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:07:37.897035: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96, Step 0, Total Loss: 8.6194, MLM Loss: 8.6194, NSP Loss: 0.7994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:07:39.474798: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97, Step 0, Total Loss: 8.5966, MLM Loss: 8.5966, NSP Loss: 0.8384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:07:40.912835: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98, Step 0, Total Loss: 8.5884, MLM Loss: 8.5884, NSP Loss: 0.7938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:07:42.547898: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99, Step 0, Total Loss: 8.5649, MLM Loss: 8.5649, NSP Loss: 0.7803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:07:44.111065: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Step 0, Total Loss: 8.5479, MLM Loss: 8.5479, NSP Loss: 0.7651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:07:45.505561: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def train_step(inputs, labels, task='both'):\n",
    "    with tf.GradientTape() as tape:        \n",
    "        predictions = bert_model(inputs, training=True)  # Predictions will have 'mlm_output' and 'nsp_output'\n",
    "        # Compute the MLM loss using the positions and labels\n",
    "        loss_mlm = compute_mlm_loss(labels['masked_lm_positions'], \n",
    "                                    labels['mlm_labels'], predictions['mlm_output'])\n",
    "        # NSP loss remains the same\n",
    "        loss_nsp = loss_object_nsp(labels['nsp_labels'], predictions['nsp_output'])\n",
    "        if task == 'nsp':\n",
    "            total_loss = loss_nsp\n",
    "        elif task == 'mlm':\n",
    "            total_loss = loss_mlm\n",
    "        else:\n",
    "            total_loss = loss_mlm + loss_nsp\n",
    "    gradients = tape.gradient(total_loss, bert_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, bert_model.trainable_variables))\n",
    "    # tf.print(\"Total Loss:\", total_loss, \"MLM Loss:\", loss_mlm, \"NSP Loss:\", loss_nsp)\n",
    "    return total_loss, loss_mlm, loss_nsp\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.repeat()\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    step = 0\n",
    "    for inputs, labels in train_dataset.take(1):\n",
    "        # tf.print(\"input_ids:\", inputs['input_ids'][0], summarize=-1)\n",
    "        loss_values = train_step(inputs, labels, task='mlm')\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, Step {step}, Total Loss: {loss_values[0].numpy():.4f}, MLM Loss: {loss_values[1].numpy():.4f}, NSP Loss: {loss_values[2].numpy():.4f}\")\n",
    "        step += 1\n",
    "# for epoch in range(epochs):\n",
    "#     step = 0\n",
    "#     # Create a new iterator for each epoch\n",
    "#     dataset_iter = iter(train_dataset.take(10))\n",
    "#     while True:\n",
    "#         try:\n",
    "#             inputs, labels = next(dataset_iter)\n",
    "#             loss_values = train_step(inputs, labels, task='mlm')\n",
    "#             if step % 10 == 0:\n",
    "#                 print(f\"Epoch {epoch + 1}, Step {step}, Total Loss: {loss_values[0].numpy():.4f}, MLM Loss: {loss_values[1].numpy():.4f}, NSP Loss: {loss_values[2].numpy():.4f}\")\n",
    "#             step += 1\n",
    "#             # Break after one batch to simulate take(1)\n",
    "#             # break\n",
    "#         except StopIteration:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163078fe-11be-4884-8047-57c33ed6e995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9853e7-5925-4be9-86fa-8894c328c08d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d92931-6e8a-49b5-80d0-bc4e7d1c8aff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a02476d-ffc8-421e-afed-a24be67803ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca49da24-88db-4251-b719-8a4930f4076a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe41b431-f4e8-41a5-a62e-55beb9cca0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbc6730-41c2-4d66-9f32-4d7f219a754e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfae548a-3a80-4994-8f0a-d572b47969ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513b7327-059e-4fa8-bf8a-ac238cc62c85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87fabc4-8221-48b3-92e2-b2f5906e321d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422c3408-9a6a-4958-a2c8-b1c1f05c5c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e54f7b-6cc7-4b0b-9904-ecff00f33890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0b9ee814-a624-41e5-a3e4-3bd2f5b1d416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_tfrecord(serialized_example):\n",
    "#     feature_description = {\n",
    "#         'input_ids': tf.io.VarLenFeature(tf.int64),\n",
    "#         'segment_ids': tf.io.VarLenFeature(tf.int64),\n",
    "#         'masked_lm_positions': tf.io.VarLenFeature(tf.int64),\n",
    "#         'masked_lm_labels': tf.io.VarLenFeature(tf.int64),\n",
    "#         'next_sentence_labels': tf.io.FixedLenFeature([], tf.int64)\n",
    "#     }\n",
    "#     # print(serialized_example)\n",
    "#     example = tf.io.parse_single_example(serialized_example, feature_description)\n",
    "#     # print(example)\n",
    "#     input_ids = tf.cast(example['input_ids'], tf.int32)\n",
    "#     input_ids = tf.sparse.to_dense(input_ids)\n",
    "#     segment_ids = tf.cast(example['segment_ids'], tf.int32)\n",
    "#     segment_ids = tf.sparse.to_dense(segment_ids)\n",
    "#     masked_lm_positions = tf.sparse.to_dense(example['masked_lm_positions'])\n",
    "#     masked_lm_labels = tf.sparse.to_dense(example['masked_lm_labels'])\n",
    "#     next_sentence_labels = tf.cast(example['next_sentence_labels'], tf.int32)\n",
    "#     inputs = {'input_ids': input_ids, 'segment_ids': segment_ids}\n",
    "#     labels = {'masked_lm_positions': masked_lm_positions,\n",
    "#               'mlm_labels': masked_lm_labels, 'nsp_labels': next_sentence_labels}\n",
    "#     return (inputs, labels)\n",
    "\n",
    "# def load_dataset(filepath, batch_size):\n",
    "#     raw_dataset = tf.data.TFRecordDataset(filepath)\n",
    "#     parsed_dataset = raw_dataset.map(parse_tfrecord)\n",
    "#     # Define padding shapes for each component of the dataset\n",
    "#     padded_shapes = ({\n",
    "#         'input_ids': [None],  # Dynamic padding for input_ids\n",
    "#         'segment_ids': [None]  # Dynamic padding for segment_ids\n",
    "#     }, {\n",
    "#         'masked_lm_positions': [None],  # Dynamic padding for positions\n",
    "#         'mlm_labels': [None],  # Dynamic padding for mlm labels\n",
    "#         'nsp_labels': []  # No padding needed for scalar labels\n",
    "#     })\n",
    "\n",
    "#     # Use padded_batch to handle variable sequence lengths\n",
    "#     batched_dataset = parsed_dataset.padded_batch(batch_size, padded_shapes=padded_shapes)\n",
    "#     # batched_dataset = parsed_dataset.batch(batch_size)    \n",
    "#     return batched_dataset\n",
    "\n",
    "# # Usage\n",
    "# batch_size = 32\n",
    "# train_dataset = load_dataset('output.tfrecord', batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8669e1e8-12af-429a-93db-afd3470d8205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_and_print_dataset(filepath):\n",
    "#     raw_dataset = tf.data.TFRecordDataset(filepath)\n",
    "#     for i, raw_record in enumerate(raw_dataset):  # Adjust the number based on how many you want to check\n",
    "#         # print(\"Raw record:\", raw_record.numpy())\n",
    "#         try:\n",
    "#             example = tf.io.parse_single_example(\n",
    "#                 raw_record,\n",
    "#                 {\n",
    "#                     # 'input_ids': tf.io.FixedLenFeature([128], tf.int64),\n",
    "#                     'input_ids': tf.io.VarLenFeature(tf.int64),\n",
    "#                     'segment_ids': tf.io.VarLenFeature(tf.int64),\n",
    "#                     'masked_lm_positions': tf.io.VarLenFeature(tf.int64),\n",
    "#                     'masked_lm_labels': tf.io.VarLenFeature(tf.int64),\n",
    "#                     'next_sentence_labels': tf.io.FixedLenFeature([], tf.int64)\n",
    "#                 }\n",
    "#             )\n",
    "#             # print(\"Parsed example:\", example)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Failed to parse record {i}: {e}\")\n",
    "#             print(\"Raw record:\", raw_record.numpy())\n",
    "#             break  # Or continue based on how you want to handle errors\n",
    "\n",
    "# # Example usage\n",
    "# load_and_print_dataset('output.tfrecord')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8c508e2c-06f7-4f84-9e1b-1ae4c0d8f143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_bert_pretraining_instances_in_chunks(file_path, chunk_size=1048576, \n",
    "#                           doc_boundary_pattern=r'ARTICLE-\\d+-\\d+-https:\\/\\/\\S+',\n",
    "#                          test_print=2):   \n",
    "#     '''\n",
    "#     the chunk can read more than one documents and hence the buffer can hold more that one doc . Hence the documents can also hold more than one doc\n",
    "#     but when chunk is smaller than the smallest  doc in file, the documents is essentially end up hoding one doc at a time \n",
    "#     '''\n",
    "#     tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#     buffer = ''\n",
    "#     with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#         while True:\n",
    "#             chunk = file.read(chunk_size)\n",
    "#             if not chunk:\n",
    "#                 break\n",
    "#             buffer += chunk\n",
    "#             # Split buffer by document boundary and keep the content only\n",
    "#             documents = re.split(doc_boundary_pattern, buffer, flags=re.MULTILINE)            \n",
    "#             # If the last part might not be a complete document, keep it in the buffer\n",
    "#             if documents and not re.match(doc_boundary_pattern, documents[-1]):\n",
    "#                 buffer = documents.pop()\n",
    "#             else:\n",
    "#                 buffer = ''            \n",
    "#             # Process each document found in this chunk\n",
    "#             for i, doc in enumerate(documents):\n",
    "#                 if not doc.strip():  # Skip any empty results from split\n",
    "#                     continue\n",
    "#                 sentences = nltk.sent_tokenize(doc)\n",
    "#                 tokens = tokenizer.tokenize(doc)\n",
    "#                 if test_print > 0:\n",
    "#                     print(f\"First few tokens: {tokens[:10]}\") \n",
    "#                     print(f\"First sentence: {sentences[:3] if sentences else 'No content'}\\n\")\n",
    "#                 if i == test_print: return\n",
    "# file_path = 'wiki_articles_with_seperator.txt'\n",
    "# create_bert_pretraining_instances_in_chunks(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a55b2de0-9fe0-4bea-988a-4284b91408fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def create_bert_pretraining_instances_in_chunks(file_path, chunk_size=1048576, \n",
    "#                           doc_boundary_pattern=r'ARTICLE-\\d+-\\d+-https:\\/\\/\\S+',\n",
    "#                          test_print=10, max_seq_length=128, max_predictions_per_seq=20, \n",
    "#                          dupe_factor=5, random_seed=12345, nsp_enabled=True):\n",
    "#     tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#     buffer = ''\n",
    "#     rng = random.Random(random_seed)\n",
    "#     instances = []\n",
    "#     with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#         while True:\n",
    "#             chunk = file.read(chunk_size)\n",
    "#             if not chunk:\n",
    "#                 break\n",
    "#             buffer += chunk\n",
    "#             documents = re.split(doc_boundary_pattern, buffer, flags=re.MULTILINE)\n",
    "#             if documents and not re.match(doc_boundary_pattern, documents[-1]):\n",
    "#                 buffer = documents.pop()\n",
    "#             else:\n",
    "#                 buffer = ''\n",
    "            \n",
    "#             for i, doc in enumerate(documents):\n",
    "#                 if not doc.strip():\n",
    "#                     continue\n",
    "#                 tokenized_doc = tokenizer.tokenize(doc)\n",
    "#                 # Split tokenized document into sequences, not implemented here, assumes tokenized_doc is already split\n",
    "#                 sequences = [tokenized_doc[i:i+max_seq_length] for i in range(0, len(tokenized_doc), max_seq_length)]\n",
    "#                 for j in range(len(sequences) - 1):\n",
    "#                     tokens_a = sequences[j]\n",
    "#                     if rng.random() > 0.5 or not nsp_enabled:\n",
    "#                         is_random_next = True\n",
    "#                         # Randomly pick any sequence from any document\n",
    "#                         tokens_b = sequences[rng.randint(0, len(sequences) - 1)]\n",
    "#                     else:\n",
    "#                         is_random_next = False\n",
    "#                         # Ensure tokens_b is the next sequence in the same document\n",
    "#                         tokens_b = sequences[j + 1]\n",
    "                    \n",
    "#                     if test_print > 0:\n",
    "#                         print(f\"Tokens A: {tokens_a}, len:: {len(tokens_a) }\")\n",
    "#                         print(f\"Tokens B: {tokens_b[:10]}\")\n",
    "#                         print(f\"Is random next: {is_random_next}\\n\")\n",
    "#                         test_print -= 1\n",
    "#                         if test_print == 0:\n",
    "#                             return  # Early exit for test purposes\n",
    "\n",
    "# file_path = 'wiki_articles_with_seperator.txt'\n",
    "# res = create_bert_pretraining_instances_in_chunks(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b4409844-790b-4695-946a-51f6127b8a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Human:\n",
    "#     def __init__(self, name, legs=2):\n",
    "#         self.name = name\n",
    "#         self.legs = legs\n",
    "\n",
    "# class MaleHuman(Human):\n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         super(MaleHuman, self).__init__(*args, **kwargs)\n",
    "#         # self.beard = beard\n",
    "\n",
    "# m1 = MaleHuman('Bhujay')\n",
    "# m1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848e17e7-f516-4e0e-85de-66c66c143682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7ed3be30-03bc-46fa-86d3-c2c719bfb359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.padded_batch(\n",
    "    #     batch_size, \n",
    "    #     padded_shapes={'input_ids': [None], \n",
    "    #                    'segment_ids': [128], \n",
    "    #                    'masked_lm_positions': [None], \n",
    "    #                    'masked_lm_labels': [None], \n",
    "    #                    'next_sentence_labels': []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "eaa8e482-617d-43f2-9ea9-63076c6546fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# try:\n",
    "#     for idx, (inputs, labels) in enumerate(train_dataset):\n",
    "#         print(f\"instance {idx + 1}:\")\n",
    "#         # print(f\"Inputs: {inputs}\")\n",
    "#         # print(f\"Labels: {labels}\")\n",
    "#         if idx == 10:  # Limit to 2 batches for demonstration\n",
    "#             break\n",
    "# except Exception as e:\n",
    "#     print(f\"Error while processing the dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9da8a988-88a4-419f-aaac-03af4ace604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_dataset(filepath):\n",
    "#     # dataset = load_dataset(filepath, batch_size=16)  # Using batch size of 1 for simplicity\n",
    "#     try:\n",
    "#         for i, (inputs, labels) in enumerate(train_dataset):        \n",
    "#             print(f\"Successfully parsed entry {i+1}\")        \n",
    "#             if i == 100:  # Optionally limit the number of entries to check\n",
    "#                 break\n",
    "#     except Exception as e:\n",
    "#             print(f'record: {i}  , Err: {e}')\n",
    "\n",
    "# test_dataset('output.tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3488dd32-cf4e-46b5-88d9-880fae1b1069",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def parse_tfrecord(serialized_example):\n",
    "#     feature_description = {\n",
    "#         'input_ids': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "#         'segment_ids': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "#         'masked_lm_positions': tf.io.VarLenFeature(tf.int64),\n",
    "#         'masked_lm_labels': tf.io.VarLenFeature(tf.int64),\n",
    "#         'next_sentence_labels': tf.io.FixedLenFeature([], tf.int64)\n",
    "#     }\n",
    "#     try:\n",
    "#         example = tf.io.parse_single_example(serialized_example, feature_description)    \n",
    "#         input_ids = tf.cast(example['input_ids'], tf.int32)\n",
    "#         segment_ids = tf.cast(example['segment_ids'], tf.int32)\n",
    "#         # segment_ids = tf.sparse.to_dense(segment_ids, default_value=0)  # Default to 0\n",
    "#         # segment_ids = tf.reshape(segment_ids, [max_seq_length]) \n",
    "#         masked_lm_positions = tf.sparse.to_dense(example['masked_lm_positions'])\n",
    "#         masked_lm_labels = tf.sparse.to_dense(example['masked_lm_labels'])   \n",
    "#         next_sentence_labels = tf.cast(example['next_sentence_labels'], tf.int32)\n",
    "#         inputs = {'input_ids': input_ids, 'segment_ids': segment_ids, }\n",
    "#         labels = {'masked_lm_positions': masked_lm_positions, \n",
    "#                   'mlm_labels': masked_lm_labels, 'nsp_labels': next_sentence_labels}\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to parse example: {e}\")\n",
    "#         # Return dummy/default data to allow pipeline to continue\n",
    "#         inputs = {\n",
    "#             'input_ids': tf.constant([0]*args.max_seq_length, dtype=tf.int32),\n",
    "#             'segment_ids': tf.constant([0]*args.max_seq_length, dtype=tf.int32)\n",
    "#             }\n",
    "#         labels = {\n",
    "#             'masked_lm_positions': tf.constant([-1]*20, dtype=tf.int32),  # Adjust 20 to your max_predictions_per_seq\n",
    "#             'masked_lm_labels': tf.constant([-1]*20, dtype=tf.int32),\n",
    "#             'nsp_labels': tf.constant([0], dtype=tf.int32)\n",
    "#             } \n",
    "#         return (None, None)\n",
    "#     return inputs, labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def load_dataset(filepath, batch_size):\n",
    "#     raw_dataset = tf.data.TFRecordDataset(filepath)\n",
    "#     parsed_dataset = raw_dataset.map(parse_tfrecord)\n",
    "#     filtered_dataset = parsed_dataset.filter(lambda x: x[0] is not None and x[1] is not None)\n",
    "#     batched_dataset = filtered_dataset.batch(batch_size)\n",
    "#     return batched_dataset\n",
    "# # Usage\n",
    "# batch_size = 16\n",
    "# train_dataset = load_dataset('output.tfrecord', batch_size)\n",
    "\n",
    "# single_test_instance = iter(train_dataset.take(1)).next()\n",
    "# single_input_tuple = single_test_instance[0]['input_ids'], single_test_instance[0]['segment_ids']\n",
    "# print(single_test_instance)\n",
    "# print()\n",
    "# # print(single_input_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1b52b52f-f946-4704-9bc0-51cc52a8ba95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def _parse_function(proto):\n",
    "#     # Define your tfrecord again. It must be the same as the one used for saving your data.\n",
    "#     feature_description = {\n",
    "#         'input_ids': tf.io.FixedLenFeature([128], tf.int64),  # Assuming input_ids are of length 128\n",
    "#         'segment_ids': tf.io.FixedLenFeature([128], tf.int64),  # Assuming segment_ids are of length 128\n",
    "#         'masked_lm_positions': tf.io.VarLenFeature(tf.int64),\n",
    "#         'masked_lm_labels': tf.io.VarLenFeature(tf.int64),\n",
    "#         'next_sentence_labels': tf.io.FixedLenFeature([], tf.int64),\n",
    "#     }\n",
    "\n",
    "#     # Load one example\n",
    "#     parsed_features = tf.io.parse_single_example(proto, feature_description)\n",
    "    \n",
    "#     # Turn your sparse array into a dense array with default values as 0\n",
    "#     parsed_features['masked_lm_positions'] = tf.sparse.to_dense(parsed_features['masked_lm_positions'], default_value=0)\n",
    "#     parsed_features['masked_lm_labels'] = tf.sparse.to_dense(parsed_features['masked_lm_labels'], default_value=0)\n",
    "\n",
    "#     return parsed_features\n",
    "# # Read the TFRecord file\n",
    "# def load_dataset(file_path):\n",
    "#     dataset = tf.data.TFRecordDataset(file_path)\n",
    "#     dataset = dataset.map(_parse_function)  # Parse the record into tensors.\n",
    "#     return dataset\n",
    "\n",
    "# # Path to the TFRecord file\n",
    "# tfrecord_file_path = 'output.tfrecord'\n",
    "\n",
    "# # Load the dataset\n",
    "# parsed_dataset = load_dataset(tfrecord_file_path)\n",
    "# # Display a few examples from the dataset\n",
    "# for parsed_record in parsed_dataset.take(2):  # Only take first 5 examples\n",
    "#     print('Input IDs:', parsed_record['input_ids'].numpy())\n",
    "#     print('Segment IDs:', parsed_record['segment_ids'].numpy())\n",
    "#     print('Masked LM Positions:', parsed_record['masked_lm_positions'].numpy())\n",
    "#     print('Masked LM Labels:', parsed_record['masked_lm_labels'].numpy())\n",
    "#     print('Next Sentence Label:', parsed_record['next_sentence_labels'].numpy())\n",
    "#     print('---')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "39969389-584a-4602-8847-1472a4155e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_tfrecord(serialized_example):\n",
    "#     feature_description = {\n",
    "#         'input_ids': tf.io.FixedLenFeature([128], tf.int64),\n",
    "#         'segment_ids': tf.io.FixedLenFeature([128], tf.int64),\n",
    "#         'masked_lm_positions': tf.io.VarLenFeature(tf.int64),\n",
    "#         'masked_lm_labels': tf.io.VarLenFeature(tf.int64),\n",
    "#         'next_sentence_labels': tf.io.FixedLenFeature([], tf.int64)\n",
    "#     }\n",
    "#     example = tf.io.parse_single_example(serialized_example, feature_description)\n",
    "\n",
    "#     input_ids = tf.cast(example['input_ids'], tf.int32)\n",
    "#     segment_ids = tf.cast(example['segment_ids'], tf.int32)\n",
    "#     masked_lm_positions = tf.sparse.to_dense(example['masked_lm_positions'])\n",
    "#     masked_lm_labels = tf.sparse.to_dense(example['masked_lm_labels'])\n",
    "#     next_sentence_labels = tf.cast(example['next_sentence_labels'], tf.int32)\n",
    "\n",
    "#     inputs = {'input_ids': input_ids, 'segment_ids': segment_ids}\n",
    "#     labels = {'mlm_output': masked_lm_labels, 'nsp_output': next_sentence_labels}\n",
    "\n",
    "#     return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4058d182-6444-4054-87c9-81511b0ebbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = Adam(learning_rate=2e-5)\n",
    "# loss = {\n",
    "#     'mlm_output': SparseCategoricalCrossentropy(from_logits=True),\n",
    "#     'nsp_output': BinaryCrossentropy(from_logits=True)\n",
    "# }\n",
    "# metrics = {\n",
    "#     'mlm_output': 'accuracy',\n",
    "#     'nsp_output': 'accuracy'\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# def masked_sparse_categorical_crossentropy(y_true, y_pred):\n",
    "#     # Create a mask to ignore `-1` in labels for loss calculation\n",
    "#     mask = tf.cast(tf.not_equal(y_true, -1), tf.float32)\n",
    "#     # Compute sparse categorical crossentropy loss\n",
    "#     loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
    "#     # Apply the mask\n",
    "#     loss *= mask\n",
    "#     # Calculate mean loss only over non-masked elements\n",
    "#     return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "# # Ensure the custom loss function is used when compiling the model\n",
    "# bert_model.compile(\n",
    "#     optimizer=optimizer,\n",
    "#     loss={'mlm_output': masked_sparse_categorical_crossentropy, 'nsp_output': BinaryCrossentropy(from_logits=True)},\n",
    "#     metrics=metrics\n",
    "# )\n",
    "# # epochs = 3 \n",
    "# # bert_model.fit(train_dataset, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d9bfe004-16cb-4a1d-9991-fcb75ff85137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SmallBERT(Model):\n",
    "#     def __init__(self, vocab_size, num_layers, d_model, num_heads, dff, max_pos=512, rate=0.1):\n",
    "#         super(SmallBERT, self).__init__()\n",
    "#         self.token_embedding = Embedding(vocab_size, d_model)\n",
    "#         self.position_embedding = Embedding(max_pos, d_model)\n",
    "#         self.segment_embedding = Embedding(2, d_model)  # Only 2 segments assumed\n",
    "        \n",
    "#         self.enc_layers = [TransformerEncoderV3(num_layers=1, d_model=d_model, num_heads=num_heads, dff=dff,\n",
    "#                                                 vocab_size=vocab_size, max_pos=max_pos) for _ in range(num_layers)]\n",
    "        \n",
    "#         self.dropout = Dropout(rate)\n",
    "#         self.final_layer = Dense(vocab_size)  # Prediction layer for MLM\n",
    "#         self.nsp_classifier = Dense(2, activation='softmax')  # NSP output\n",
    "\n",
    "#     def call(self, input_ids, segment_ids, training=False):\n",
    "#         seq_length = tf.shape(input_ids)[1]\n",
    "#         position_ids = tf.range(seq_length)\n",
    "        \n",
    "#         x = self.token_embedding(input_ids) + self.position_embedding(position_ids) + self.segment_embedding(segment_ids)\n",
    "#         x = self.dropout(x, training=training)\n",
    "        \n",
    "#         for encoder in self.enc_layers:\n",
    "#             x = encoder(x)\n",
    "        \n",
    "#         logits = self.final_layer(x)\n",
    "#         pooled_output = self.dropout(sequence_output[:, 0, :], training=training)  # Use the output of the [CLS] token\n",
    "#         nsp_output = self.nsp_classifier(pooled_output)\n",
    "#         return logits, nsp_output  \n",
    "\n",
    "# vocab_size = 10000  # Smaller vocabulary size for simplicity\n",
    "# num_layers = 2  # Fewer layers\n",
    "# d_model = 128  # Smaller dimensionality\n",
    "# num_heads = 4\n",
    "# dff = 512\n",
    "# small_bert = SmallBERT(vocab_size, num_layers, d_model, num_heads, dff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e381b9-5878-4250-b99e-8b853640fc3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3dfa73-323e-4930-b9c2-c1e2080a8acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7227c722-9c49-4bdb-926e-7091b49e5f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69ca148-1c24-45e2-904c-94a6c99df05b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a51f6c-f36c-4bf1-97cb-948e5bebaef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d198953-53b2-47b1-bb1a-9d243bf52c02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
