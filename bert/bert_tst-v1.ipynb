{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "086051b2-2d8f-4bb7-bf24-0e4dd05fdc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-12 17:23:43.350212: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-12 17:23:43.373187: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-12 17:23:44.021458: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-12 17:23:46.403992: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-12 17:23:55.499345: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from transformer_encoder import TransformerEncoderV3  \n",
    "from positional_encoding import encode_pos_sin_cosine\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Input, Dense, Dropout, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, BinaryCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63c535dd-000d-42b6-904a-4f2d3faba92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how the dataset looks: Dataset({\n",
      "    features: ['id', 'url', 'title', 'text'],\n",
      "    num_rows: 6458670\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# Load an example dataset, 'wikipedia' for English, 2020-03-01 version\n",
    "dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=[\"train\"])\n",
    "print('how the dataset looks:', dataset[0])\n",
    "article_texts_dataset = dataset[0]['text'][:1000]\n",
    "# # Extract text and write to a file\n",
    "# with open('input_text.txt', 'w', encoding='utf-8') as f:\n",
    "#     for article  in article_texts_dataset:\n",
    "#         # Write each Wikipedia article on a new line\n",
    "#         f.write(article.replace('\\n', ' ') + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eec8e7b0-449e-4c0f-9c78-d050fc6c4344",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words in first_article: 46773\n",
      "how the article look:\n",
      " Autism is a neurodevelopmental disorder characterized by difficulties with social interaction and communication, and by restricted and repetitive behavior. Parents often notice signs during the first three years of their child's life. These signs often develop gradually, though some autistic children experience regression in their communication and social skills after reaching developmental milestones at a normal pace.\n",
      "\n",
      "Autism is associated with a combination of genetic and environmental factors. Risk factors during pregnancy include certain infections, such as rubella, toxins including valproic acid, alcohol, cocaine, pesticides, lead, and air pollution, fetal growth restriction, and autoimmune diseases. Controversies surround other proposed environmental causes; for example, the vaccine hypothesis, which has been disproven. Autism affects information processing in the brain and how nerve cells and their synapses connect and organize; how this occurs is not well understood. The Diagnostic and Statistical Manual of Mental Disorders (DSM-5) combines forms of the condition, including Asperger syndrome and pervasive developmental disorder not otherwise specified (PDD-NOS) into the diagnosis of autism spectrum disorder (ASD).\n",
      "\n",
      "Several interventions have been shown to reduce symptoms and improve the ability of autistic people to function and participate independently in the community. Behavioral, psychological, education, and/or skill-building interventions may be used to assist a\n",
      "..........................................\n",
      "........................\n",
      "ory and detail orientation as well as a high regard for rules and procedure in autistic employees. A majority of the economic burden of autism is caused by decreased earnings in the job market. Some studies also find decreased earning among parents who care for autistic children.\n",
      "\n",
      "References\n",
      "\n",
      "External links\n",
      "\n",
      " \n",
      "1910s neologisms\n",
      "Articles containing video clips\n",
      "Communication disorders\n",
      "Neurological disorders in children\n",
      "Pervasive developmental disorders\n",
      "Wikipedia medicine articles ready to translate\n"
     ]
    }
   ],
   "source": [
    "first_article = article_texts_dataset[1]\n",
    "print('words in first_article:', len(first_article))\n",
    "print('how the article look:\\n', article_texts_dataset[1][:1500])\n",
    "print('..........................................\\n........................')\n",
    "print(article_texts_dataset[1][-500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327a007e-1361-41a0-80ad-cc75d8426a38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd859cf-88b2-4da8-9eaa-b71c0cb1ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert these articles to pretraining data \n",
    "# !python create_pretraining_data.py --vocab_file vocab.txt --input_text input_text.txt --output_tfrecord output.tfrecord --do_lower_case --nsp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d246b53-8a4b-4e74-b802-da85d3f58f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 30000  # Smaller vocabulary size for simplicity\n",
    "num_layers = 2  # Fewer layers\n",
    "d_model = 256  # Smaller dimensionality\n",
    "num_heads = 4\n",
    "dff = 512\n",
    "segment_size = 2\n",
    "max_seq_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8669e1e8-12af-429a-93db-afd3470d8205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-12 17:40:22.878046: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "def load_and_print_dataset(filepath):\n",
    "    raw_dataset = tf.data.TFRecordDataset(filepath)\n",
    "    for i, raw_record in enumerate(raw_dataset):  # Adjust the number based on how many you want to check\n",
    "        # print(\"Raw record:\", raw_record.numpy())\n",
    "        try:\n",
    "            example = tf.io.parse_single_example(\n",
    "                raw_record,\n",
    "                {\n",
    "                    # 'input_ids': tf.io.FixedLenFeature([128], tf.int64),\n",
    "                    'input_ids': tf.io.VarLenFeature(tf.int64),\n",
    "                    'segment_ids': tf.io.VarLenFeature(tf.int64),\n",
    "                    'masked_lm_positions': tf.io.VarLenFeature(tf.int64),\n",
    "                    'masked_lm_labels': tf.io.VarLenFeature(tf.int64),\n",
    "                    'next_sentence_labels': tf.io.FixedLenFeature([], tf.int64)\n",
    "                }\n",
    "            )\n",
    "            # print(\"Parsed example:\", example)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to parse record {i}: {e}\")\n",
    "            print(\"Raw record:\", raw_record.numpy())\n",
    "            break  # Or continue based on how you want to handle errors\n",
    "\n",
    "# Example usage\n",
    "load_and_print_dataset('output.tfrecord')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b9ee814-a624-41e5-a3e4-3bd2f5b1d416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfrecord(serialized_example):\n",
    "    feature_description = {\n",
    "        'input_ids': tf.io.VarLenFeature(tf.int64),\n",
    "        'segment_ids': tf.io.VarLenFeature(tf.int64),\n",
    "        'masked_lm_positions': tf.io.VarLenFeature(tf.int64),\n",
    "        'masked_lm_labels': tf.io.VarLenFeature(tf.int64),\n",
    "        'next_sentence_labels': tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    # print(serialized_example)\n",
    "    example = tf.io.parse_single_example(serialized_example, feature_description)\n",
    "    # print(example)\n",
    "    input_ids = tf.cast(example['input_ids'], tf.int32)\n",
    "    input_ids = tf.sparse.to_dense(input_ids)\n",
    "    segment_ids = tf.cast(example['segment_ids'], tf.int32)\n",
    "    segment_ids = tf.sparse.to_dense(segment_ids)\n",
    "    masked_lm_positions = tf.sparse.to_dense(example['masked_lm_positions'])\n",
    "    masked_lm_labels = tf.sparse.to_dense(example['masked_lm_labels'])\n",
    "    next_sentence_labels = tf.cast(example['next_sentence_labels'], tf.int32)\n",
    "    inputs = {'input_ids': input_ids, 'segment_ids': segment_ids}\n",
    "    labels = {'masked_lm_positions': masked_lm_positions,\n",
    "              'mlm_labels': masked_lm_labels, 'nsp_labels': next_sentence_labels}\n",
    "    return (inputs, labels)\n",
    "\n",
    "def load_dataset(filepath, batch_size):\n",
    "    raw_dataset = tf.data.TFRecordDataset(filepath)\n",
    "    parsed_dataset = raw_dataset.map(parse_tfrecord)\n",
    "    # Define padding shapes for each component of the dataset\n",
    "    padded_shapes = ({\n",
    "        'input_ids': [None],  # Dynamic padding for input_ids\n",
    "        'segment_ids': [None]  # Dynamic padding for segment_ids\n",
    "    }, {\n",
    "        'masked_lm_positions': [None],  # Dynamic padding for positions\n",
    "        'mlm_labels': [None],  # Dynamic padding for mlm labels\n",
    "        'nsp_labels': []  # No padding needed for scalar labels\n",
    "    })\n",
    "\n",
    "    # Use padded_batch to handle variable sequence lengths\n",
    "    batched_dataset = parsed_dataset.padded_batch(batch_size, padded_shapes=padded_shapes)\n",
    "    # batched_dataset = parsed_dataset.batch(batch_size)    \n",
    "    return batched_dataset\n",
    "\n",
    "# Usage\n",
    "batch_size = 32\n",
    "train_dataset = load_dataset('output.tfrecord', batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84d75159-e176-4d89-80a9-fa6b52aeec18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(32, 128), dtype=int32, numpy=\n",
      "array([[  101,  9617, 11140, ...,  2591,  4813,   102],\n",
      "       [  101, 19465,  2003, ...,   103,  1997,   102],\n",
      "       [  101,  2632, 28759, ...,  3774,  1997,   102],\n",
      "       ...,\n",
      "       [  101,   103, 11332, ..., 12626,  1997,   102],\n",
      "       [  101,  1996, 16951, ...,  1996,  7842,   102],\n",
      "       [  101, 17694, 15396, ...,   103, 24988,   102]], dtype=int32)>, 'segment_ids': <tf.Tensor: shape=(32, 128), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 1, 1, 1],\n",
      "       [0, 0, 0, ..., 1, 1, 1],\n",
      "       [0, 0, 0, ..., 1, 1, 1],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 1, 1, 1],\n",
      "       [0, 0, 0, ..., 1, 1, 1],\n",
      "       [0, 0, 0, ..., 1, 1, 1]], dtype=int32)>}, {'masked_lm_positions': <tf.Tensor: shape=(32, 18), dtype=int64, numpy=\n",
      "array([[  5,   8,  18,  26,  40,  47,  51,  61,  63,  75,  79,  86,  87,\n",
      "         89,  93,  94, 110, 123],\n",
      "       [ 19,  22,  31,  33,  37,  45,  46,  49,  56,  61,  64,  85,  95,\n",
      "         98, 108, 111, 112, 125],\n",
      "       [  3,   4,  12,  14,  18,  37,  51,  58,  60,  70,  76,  91, 106,\n",
      "        107, 111, 112, 117, 120],\n",
      "       [ 23,  26,  34,  38,  44,  47,  52,  60,  66,  71,  76,  77,  79,\n",
      "         89,  98,  99, 106, 116],\n",
      "       [  2,   5,   6,  11,  30,  35,  39,  45,  56,  82,  83,  86,  91,\n",
      "         95,  99, 113, 114, 125],\n",
      "       [  5,   8,  13,  15,  17,  23,  33,  36,  38,  39,  44,  51,  77,\n",
      "         78,  80,  87, 101, 102],\n",
      "       [  7,   9,  27,  34,  39,  52,  59,  69,  74,  77,  85,  94,  96,\n",
      "        107, 112, 113, 114, 125],\n",
      "       [  3,   4,   6,  23,  29,  34,  35,  50,  51,  69,  72,  74,  75,\n",
      "         81,  86, 107, 108, 113],\n",
      "       [  4,   5,  14,  23,  29,  48,  54,  69,  75,  82,  94, 100, 104,\n",
      "        108, 109, 117, 120, 126],\n",
      "       [ 13,  15,  16,  29,  31,  34,  47,  66,  76,  79,  85,  89, 100,\n",
      "        104, 105, 107, 108, 109],\n",
      "       [ 12,  17,  35,  48,  49,  59,  63,  70,  72,  84,  95,  98, 100,\n",
      "        102, 116, 118, 121, 123],\n",
      "       [  2,   7,  15,  16,  27,  44,  51,  73,  80,  84,  90, 101, 102,\n",
      "        106, 114, 119, 121, 122],\n",
      "       [ 16,  20,  21,  22,  45,  48,  49,  58,  77,  80,  81,  84,  85,\n",
      "         91, 104, 115, 116, 118],\n",
      "       [ 12,  23,  33,  37,  40,  42,  49,  50,  61,  62,  72,  74,  79,\n",
      "         83,  92, 105, 116, 117],\n",
      "       [ 21,  33,  44,  54,  58,  62,  63,  65,  71,  72,  73,  87,  92,\n",
      "         93, 108, 113, 119, 125],\n",
      "       [  6,  10,  11,  21,  31,  36,  39,  40,  47,  56,  67,  71,  74,\n",
      "         88,  95, 112, 116, 118],\n",
      "       [  9,  12,  19,  20,  27,  30,  39,  46,  49,  57,  79,  83,  84,\n",
      "         87,  98, 104, 109, 119],\n",
      "       [ 21,  27,  35,  43,  49,  50,  52,  57,  69,  81,  84, 102, 105,\n",
      "        108, 111, 113, 118, 124],\n",
      "       [ 17,  18,  28,  44,  46,  48,  56,  62,  66,  69,  77,  81,  85,\n",
      "         88,  92, 106, 123, 126],\n",
      "       [  2,  12,  17,  19,  20,  28,  34,  40,  42,  60,  77,  81,  93,\n",
      "         99, 101, 107, 114, 115],\n",
      "       [ 16,  30,  37,  45,  51,  54,  65,  71,  83,  84,  95,  97,  99,\n",
      "        103, 107, 110, 113, 126],\n",
      "       [  9,  11,  13,  20,  38,  48,  54,  66,  67,  72,  85,  86,  92,\n",
      "         97, 103, 110, 111, 113],\n",
      "       [  6,   7,  33,  45,  48,  52,  56,  58,  63,  67,  71,  74,  78,\n",
      "         88, 112, 114, 115, 119],\n",
      "       [  2,   4,  17,  20,  30,  32,  36,  38,  39,  40,  43,  50,  67,\n",
      "         70,  87,  99, 115, 118],\n",
      "       [  3,   6,   7,  14,  15,  21,  48,  52,  56,  60,  62,  66,  69,\n",
      "         71,  78, 108, 111, 122],\n",
      "       [  2,  12,  19,  32,  41,  43,  46,  47,  69,  73,  77,  83,  88,\n",
      "         94, 103, 107, 118, 120],\n",
      "       [  1,   2,   4,   5,   7,  20,  50,  57,  63,  85,  86,  98,  99,\n",
      "        107, 113, 115, 121, 125],\n",
      "       [  6,   8,  14,  16,  17,  23,  32,  40,  54,  73,  87,  96,  99,\n",
      "        100, 102, 104, 106, 111],\n",
      "       [  5,   8,   9,  15,  16,  17,  19,  24,  48,  53,  60,  64,  66,\n",
      "         75,  79,  80,  90, 117],\n",
      "       [  1,  12,  14,  20,  21,  26,  27,  38,  52,  59,  84,  88,  92,\n",
      "        100, 101, 108, 115, 121],\n",
      "       [  3,   4,  12,  20,  37,  38,  48,  50,  58,  60,  66,  84,  86,\n",
      "         87,  94, 110, 114, 117],\n",
      "       [  5,   9,  13,  25,  32,  49,  52,  59,  70,  74,  85,  90, 108,\n",
      "        117, 118, 119, 125, 126]])>, 'mlm_labels': <tf.Tensor: shape=(32, 18), dtype=int64, numpy=\n",
      "array([[ 1037,  1998, 19164,  1997,  2029,  6151,  1010,  2929,  2872,\n",
      "         7356,  2591,  7775,  1998,  5248,  5060,  5751,  4503,  4807],\n",
      "       [ 1010,  7775,  2076,  2034,  2037,  2411,  4503,  2295, 26237,\n",
      "         2591,  2632,  1998,  1037,  2008,  7978,  2303,  2008,  6463],\n",
      "       [ 1006,  1025,  9185,  5943,  1996,  2015,  2035,  2003,  2004,\n",
      "         2003,  2034,  2171,  2714,  1999,  3418,  3306,  2029,  1012],\n",
      "       [ 3763,  2049,  1007,  2015,  4338,  3418,  2013,  2544,  1997,\n",
      "         1516,  2124,  2036,  2632, 23848,  2923,  2013,  2573, 24268],\n",
      "       [ 1006,  1037,  2110,  1997,  1996,  1996,  5900,  2003, 20151,\n",
      "         2006,  1996,  3167,  7526,  2019,  3656,  1997,  1996,  4322],\n",
      "       [23167,  2030,  1007,  1037,  1997,  4602,  2430, 11525,  1055,\n",
      "         6335,  1996,  1996,  6725,  1007,  2019,  2004,  6725,  1012],\n",
      "       [ 1010,  1516,  1997,  2010,  5367,  1996,  1010,  9834,  2549,\n",
      "         4647, 18900,  1012,  2011,  2819,  3686,  4588,  2082,  1012],\n",
      "       [ 1025, 10488,  9834,  2232,  3418, 18858,  1010,  4588,  2082,\n",
      "         1024,  2012,  1010,  2019,  2070,  1996,  1037,  2373,  1006],\n",
      "       [ 3000,  2003,  4543,  2001, 25600,  4776, 25600,  2190,  2396,\n",
      "         1005,  2000,  2005,  2914,  2689,  4504,  1997,  1997,  1006],\n",
      "       [ 1999,  1012,  1996,  2904,  2049,  1999,  1996,  2982,  2982,\n",
      "         1998,  3068,  5240,  1996,  1012,  2445,  2011,  1996,  2914],\n",
      "       [ 2024,  4087,  2982,  4367,  3861,  2982,  5038, 13245,  2003,\n",
      "        21151,  2377,  1054,  2011,  2361,  1010,  2035,  2011,  1012],\n",
      "       [ 1006,  1007,  2143,  2550,  2400, 26573,  5889,  2001,  2001,\n",
      "         1010,  2429, 17365,  2581,  4242,  2003,  3273,  1998,  4695],\n",
      "       [ 2761,  1010,  2628,  2011,  4969,  2569,  8597,  2727,  1037,\n",
      "        11718,  9593,  3115,  2241,  1997,  7142,  1996,  4054,  6648],\n",
      "       [ 2003,  1996,  1055,  2009,  7142,  1997,  2009,  2003,  4964,\n",
      "        16396,  3218,  5142,  2529,  4176,  3430,  1037,  1012,  2174],\n",
      "       [ 1010,  2009,  7814,  2174,  1006,  5142,  8137,  1999,  2019,\n",
      "         9896,  1006,  2000,  3563,  3471,  1998,  2437, 13792,  2015],\n",
      "       [ 2141,  5740, 10736,  1020,  2171,  1007,  1037,  2845,  1012,\n",
      "         4975,  2003,  7621, 16744,  1037,  2181,  2030,  4066,  4198],\n",
      "       [ 4006,  1037, 13702,  1010,  2817, 11208,  2002,  2139,  1045,\n",
      "         2118,  4053,  4655,  1012,  5320,  2036, 21434,  2030,  3732],\n",
      "       [ 3261,  1011,  1010,  2632,  4361,  1010,  1010,  1010,  2111,\n",
      "         1996,  2497,  1999,  2088,  2003,  1996,  2011,  2264,  4643],\n",
      "       [ 1996, 23848,  2003,  2003,  2000,  4794, 12917, 16842,  1037,\n",
      "         3494,  3117,  2350,  2024,  2013, 21989,  3117, 15758,  2014],\n",
      "       [ 2003,  1055,  1012,  3494,  1996,  1012,  4830, 10191,  1996,\n",
      "        15758,  1010,  8578,  2627,  1012, 12795,  2096,  2164, 17606],\n",
      "       [ 1010,  1010, 12795, 12795, 17606,  1012, 22703,  3567,  2658,\n",
      "         5093,  3570,  2014,  1997,  5093,  2012,  1997,  1010,  2028],\n",
      "       [ 2005,  1007,  1037,  2492,  2109,  1997,  4910,  9593,  2051,\n",
      "         1996,  2241,  2006,  5372,  1055,  1037, 11679,  3823,  2009],\n",
      "       [ 5640,  1024, 11020,  1010,  2088,  1012,  2433,  2632, 18470,\n",
      "         1006,  2137,  1007,  4800, 15187,  2891,  2003,  2241,  4174],\n",
      "       [ 3952,  2000,  2003,  2120,  2335,  2162, 14482,  1010,  2166,\n",
      "         2029, 21754,  3334,  2080,  1025,  3619,  1006,  9516,  3417],\n",
      "       [ 2003,  1999,  1996,  2913,  2006,  2648,  2011,  1007,  1012,\n",
      "         2030, 26357,  6895,  1007, 12066,  8989,  1011,  2024,  3176],\n",
      "       [ 6895,  2005, 17181,  7588,  2087,  2839, 11683,  2024,  3146,\n",
      "         2163,  2036,  2660,  2710,  4561,  4904,  5899,  1040,  1051],\n",
      "       [ 5899,  2003,  3007,  1997,  1999,  2660,  2226,  1051,  4290,\n",
      "        10808,  1037,  1007,  1010, 12702,  3430,  4776, 13729, 23569],\n",
      "       [ 2029,  2024,  4871,  1999,  3151,  2030,  8697,  1012,  1039,\n",
      "         4556,  1997,  1037, 21383,  1010,  1998,  1010,  1998,  7870],\n",
      "       [ 1996,  1999,  4556,  3306,  1998,  3142,  1012,  1996,  1010,\n",
      "         1010,  1997,  7213, 12943,  3359,  2137,  2280,  2019, 12943],\n",
      "       [ 7213,  3359,  2003,  1012,  1015,  2003,  2019, 12968,  8504,\n",
      "         2034,  2549,  1997, 16444,  4958, 26013,  1007,  3656, 13710],\n",
      "       [15396,  4588, 19472,  8240,  1010,  7269,  2105,  2454,  1997,\n",
      "         4155, 15396, 20604,  4100,  3406, 22123,  9197,  1010,  1010],\n",
      "       [17694,  1010, 21358, 10654, 11219,  4021,  3088,  3033,  1997,\n",
      "         2003, 21988,  2789,  2042,  2011,  1996,  4175,  2620,  1010]])>, 'nsp_labels': <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
      "array([0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,\n",
      "       1, 1, 0, 0, 1, 0, 0, 1, 0, 0], dtype=int32)>})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "single_test_instance = iter(train_dataset.take(1)).next()\n",
    "single_input_tuple = single_test_instance[0]['input_ids'], single_test_instance[0]['segment_ids']\n",
    "print(single_test_instance)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed3d1efc-c514-4806-9707-3be670c4fc30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (32, 128, 256)\n",
      "Output shape: tf.Tensor(\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]], shape=(32, 128), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "class PositionalAndSegmentEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, segment_size, d_model, max_pos=2048, pos_dropout=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)  # Initialize the superclass (Layer)\n",
    "        self.d_model = d_model  # Store the dimensionality of the model embeddings\n",
    "        self.token_embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.segment_embedding = tf.keras.layers.Embedding(segment_size, d_model)\n",
    "        self.pos_encoding = encode_pos_sin_cosine(max_pos, d_model, debug=False)\n",
    "        self.dropout = tf.keras.layers.Dropout(pos_dropout)\n",
    "\n",
    "    def compute_mask(self, inputs, *args, **kwargs):\n",
    "        # Assuming the input structure is a tuple of (tokens, segments)\n",
    "        token_inputs, _ = inputs\n",
    "        return self.token_embedding.compute_mask(token_inputs, *args, **kwargs)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # Expect inputs to be a tuple (token_inputs, segment_inputs)\n",
    "        token_inputs, segment_inputs = inputs\n",
    "        tokens = self.token_embedding(token_inputs)  # Token embeddings\n",
    "        segments = self.segment_embedding(segment_inputs)  # Segment embeddings       \n",
    "        x = tokens + segments\n",
    "        # Scale the embeddings by the square root of the embedding dimension size\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        # Add positional encoding to the combined embeddings, sliced to match the input length\n",
    "        length = tf.shape(x)[1]\n",
    "        # pos_encodings = tf.reshape(self.pos_encoding, (1, -1, self.d_model))[:, :length, :]\n",
    "        pos_encodings = tf.cast(tf.reshape(self.pos_encoding, (1, -1, self.d_model))[:, :tf.shape(x)[1], :], tf.float32)\n",
    "        x += pos_encodings\n",
    "        x = self.dropout(x, training=training)\n",
    "        return x\n",
    "\n",
    "embedding_layer = PositionalAndSegmentEmbedding(vocab_size=vocab_size, segment_size=2, d_model=256)\n",
    "\n",
    "# Extract a single batch from the dataset\n",
    "for inputs, labels in train_dataset.take(1):\n",
    "    # The inputs dictionary contains 'input_ids' and 'segment_ids'\n",
    "    input_ids = inputs['input_ids']\n",
    "    segment_ids = inputs['segment_ids']\n",
    "\n",
    "    # Call the embedding layer\n",
    "    embeddings = embedding_layer((input_ids, segment_ids))\n",
    "\n",
    "    # Print the output shape\n",
    "    print(\"Output shape:\", embeddings.shape)\n",
    "    print(\"Output shape:\", embeddings._keras_mask)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0ee2176-1307-4de3-b836-7f4762537010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 128, 256)\n"
     ]
    }
   ],
   "source": [
    "class TransformerEncoderV4(TransformerEncoderV3):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, vocab_size, segment_size, max_pos=2048, pos_dropout=0.1, **kwargs):\n",
    "        super(TransformerEncoderV4, self).__init__(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff, vocab_size=vocab_size, max_pos=max_pos, **kwargs)\n",
    "        # Use the custom embedding layer that handles tokens, segments, and positional encodings\n",
    "        self.embedding_layer = PositionalAndSegmentEmbedding(vocab_size, segment_size, d_model, max_pos, pos_dropout)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_ids, segment_ids = inputs\n",
    "        # The embedding layer now handles everything including token, segment, and positional embeddings\n",
    "        x = self.embedding_layer((input_ids, segment_ids), training=training)\n",
    "        x = self.enc_layers_0(x, training=training)\n",
    "        for i in range(self.remaining_layers):\n",
    "            x = self.enc_layers[i](x, training=training)\n",
    "        return x\n",
    "tren = TransformerEncoderV4(num_layers, d_model, num_heads, dff, vocab_size, segment_size, max_pos=max_seq_length)\n",
    "\n",
    "encoder_out = tren(single_input_tuple)\n",
    "print(encoder_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "028da786-f0f6-4280-8228-4d3b80ce27cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mlm_output': <tf.Tensor: shape=(32, 128, 30000), dtype=float32, numpy=\n",
      "array([[[3.8340859e-05, 3.7446305e-05, 2.8395381e-05, ...,\n",
      "         2.6639331e-05, 3.7298159e-05, 3.4331169e-05],\n",
      "        [3.5365043e-05, 3.5282104e-05, 3.7169062e-05, ...,\n",
      "         2.2325734e-05, 3.5428879e-05, 3.0616156e-05],\n",
      "        [3.5435303e-05, 3.2537922e-05, 3.5552261e-05, ...,\n",
      "         2.3641700e-05, 3.7744659e-05, 3.3079072e-05],\n",
      "        ...,\n",
      "        [3.8897797e-05, 3.2174790e-05, 3.5057983e-05, ...,\n",
      "         2.0151998e-05, 3.3009997e-05, 3.7184676e-05],\n",
      "        [3.8864127e-05, 2.8284470e-05, 3.2879325e-05, ...,\n",
      "         2.0593974e-05, 3.2062126e-05, 4.0074581e-05],\n",
      "        [4.0870749e-05, 3.0497569e-05, 3.2746706e-05, ...,\n",
      "         1.9698917e-05, 3.7209582e-05, 3.7094494e-05]],\n",
      "\n",
      "       [[3.8267041e-05, 3.6553694e-05, 2.8442262e-05, ...,\n",
      "         2.6696673e-05, 3.8556202e-05, 3.5093311e-05],\n",
      "        [3.3343262e-05, 3.7650851e-05, 3.7500002e-05, ...,\n",
      "         2.4165498e-05, 3.2304510e-05, 3.7988360e-05],\n",
      "        [3.5531571e-05, 3.4948869e-05, 3.2075324e-05, ...,\n",
      "         2.1441134e-05, 3.7092130e-05, 3.3637887e-05],\n",
      "        ...,\n",
      "        [3.8908034e-05, 3.2001451e-05, 3.0565996e-05, ...,\n",
      "         2.1982078e-05, 3.3278833e-05, 3.5821147e-05],\n",
      "        [4.1329899e-05, 3.2053867e-05, 3.2572167e-05, ...,\n",
      "         2.2254239e-05, 3.4770706e-05, 3.6197362e-05],\n",
      "        [4.1736821e-05, 3.1131854e-05, 3.3270349e-05, ...,\n",
      "         1.9692614e-05, 3.6545713e-05, 3.7423240e-05]],\n",
      "\n",
      "       [[3.8384944e-05, 3.7300528e-05, 2.7708664e-05, ...,\n",
      "         2.6305384e-05, 3.7537651e-05, 3.3667657e-05],\n",
      "        [4.1632200e-05, 3.9185899e-05, 3.0581661e-05, ...,\n",
      "         2.4036859e-05, 3.3238783e-05, 3.0437408e-05],\n",
      "        [3.4502766e-05, 3.4785950e-05, 3.1502605e-05, ...,\n",
      "         2.3415003e-05, 3.5155812e-05, 3.2552554e-05],\n",
      "        ...,\n",
      "        [4.3554519e-05, 3.0483128e-05, 2.9599249e-05, ...,\n",
      "         2.2188186e-05, 3.4779139e-05, 4.1666441e-05],\n",
      "        [4.0897674e-05, 3.1536409e-05, 3.1543219e-05, ...,\n",
      "         2.1879205e-05, 3.4441269e-05, 3.5778747e-05],\n",
      "        [4.2222469e-05, 3.0489366e-05, 3.2068663e-05, ...,\n",
      "         1.9387504e-05, 3.6191577e-05, 3.7251746e-05]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[4.0369618e-05, 3.6432561e-05, 2.7565522e-05, ...,\n",
      "         2.5920044e-05, 3.7305486e-05, 3.3638342e-05],\n",
      "        [3.7296966e-05, 3.4359044e-05, 2.9409130e-05, ...,\n",
      "         2.5062487e-05, 3.4500139e-05, 2.9945128e-05],\n",
      "        [4.2680727e-05, 3.1275235e-05, 2.8359695e-05, ...,\n",
      "         2.5282194e-05, 3.9887571e-05, 3.1117946e-05],\n",
      "        ...,\n",
      "        [4.0022253e-05, 3.1457053e-05, 3.2297379e-05, ...,\n",
      "         2.2535862e-05, 3.2225715e-05, 3.7428927e-05],\n",
      "        [4.1111543e-05, 3.1459767e-05, 3.1317588e-05, ...,\n",
      "         2.2015029e-05, 3.4947137e-05, 3.5476398e-05],\n",
      "        [4.2125215e-05, 3.0098117e-05, 3.1983309e-05, ...,\n",
      "         1.9385750e-05, 3.6863250e-05, 3.7321373e-05]],\n",
      "\n",
      "       [[3.9081766e-05, 3.6956069e-05, 2.7707294e-05, ...,\n",
      "         2.6533280e-05, 3.8100632e-05, 3.3457203e-05],\n",
      "        [4.0066912e-05, 3.3975768e-05, 3.0732012e-05, ...,\n",
      "         2.5087022e-05, 3.5351644e-05, 3.1470379e-05],\n",
      "        [3.9770755e-05, 3.3331078e-05, 3.2376174e-05, ...,\n",
      "         2.5248119e-05, 3.1461521e-05, 2.8341643e-05],\n",
      "        ...,\n",
      "        [4.1766787e-05, 2.9246972e-05, 3.1107375e-05, ...,\n",
      "         2.1901249e-05, 3.4004643e-05, 3.9108039e-05],\n",
      "        [4.0607236e-05, 3.0588966e-05, 3.2712895e-05, ...,\n",
      "         2.0346504e-05, 3.3356078e-05, 3.9278093e-05],\n",
      "        [4.1660398e-05, 3.0389723e-05, 3.2573927e-05, ...,\n",
      "         1.9765024e-05, 3.7451824e-05, 3.6718247e-05]],\n",
      "\n",
      "       [[3.9453593e-05, 3.7783735e-05, 2.8366981e-05, ...,\n",
      "         2.6708973e-05, 3.7036432e-05, 3.3726810e-05],\n",
      "        [3.4346514e-05, 3.1883330e-05, 3.0936513e-05, ...,\n",
      "         2.3902670e-05, 3.4056371e-05, 3.3554687e-05],\n",
      "        [3.6541449e-05, 3.6453835e-05, 3.5595604e-05, ...,\n",
      "         2.4008288e-05, 3.7813763e-05, 3.4381275e-05],\n",
      "        ...,\n",
      "        [3.8372822e-05, 3.0996394e-05, 2.9803323e-05, ...,\n",
      "         2.1743881e-05, 3.3167766e-05, 3.6474266e-05],\n",
      "        [3.7692094e-05, 3.1488395e-05, 3.5858080e-05, ...,\n",
      "         1.8802806e-05, 3.5655325e-05, 3.8000591e-05],\n",
      "        [4.1824500e-05, 3.0451160e-05, 3.2356467e-05, ...,\n",
      "         1.9408011e-05, 3.6730191e-05, 3.7507070e-05]]], dtype=float32)>, 'nsp_output': <tf.Tensor: shape=(32, 1), dtype=float32, numpy=\n",
      "array([[0.355136  ],\n",
      "       [0.3213798 ],\n",
      "       [0.35239238],\n",
      "       [0.314295  ],\n",
      "       [0.36076358],\n",
      "       [0.33442888],\n",
      "       [0.32500687],\n",
      "       [0.34099397],\n",
      "       [0.35146984],\n",
      "       [0.33107892],\n",
      "       [0.32160512],\n",
      "       [0.30575266],\n",
      "       [0.3073671 ],\n",
      "       [0.42170575],\n",
      "       [0.35086858],\n",
      "       [0.31313616],\n",
      "       [0.33483458],\n",
      "       [0.3903778 ],\n",
      "       [0.3403566 ],\n",
      "       [0.31576318],\n",
      "       [0.34178865],\n",
      "       [0.36401507],\n",
      "       [0.37044027],\n",
      "       [0.37676662],\n",
      "       [0.3459438 ],\n",
      "       [0.3838137 ],\n",
      "       [0.37607267],\n",
      "       [0.34416476],\n",
      "       [0.34559724],\n",
      "       [0.37360963],\n",
      "       [0.313964  ],\n",
      "       [0.30483413]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "class BERT(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, vocab_size, segment_size, max_seq_length=128, rate=0.1):\n",
    "        super(BERT, self).__init__()\n",
    "        self.encoder = TransformerEncoderV4(num_layers=num_layers, d_model=d_model, num_heads=num_heads,\n",
    "                                            dff=dff, vocab_size=vocab_size, segment_size=segment_size,\n",
    "                                            max_pos=max_seq_length, pos_dropout=rate)\n",
    "        self.mlm_dense = tf.keras.layers.Dense(vocab_size, activation='softmax')  # Ensures output shape is [batch, seq_length, vocab_size]\n",
    "        self.nsp_dense = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.encoder((inputs['input_ids'], inputs['segment_ids']), training=training)\n",
    "        mlm_output = self.mlm_dense(x)  # Check shapes here\n",
    "        nsp_output = self.nsp_dense(x[:, 0, :])\n",
    "        return {'mlm_output': mlm_output, 'nsp_output': nsp_output}\n",
    "\n",
    "\n",
    "bert_model = BERT(num_layers, d_model, num_heads, \n",
    "                  dff, vocab_size, segment_size)\n",
    "bert_out = bert_model(single_test_instance[0])\n",
    "print(bert_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bfe9d7d7-b7a2-433a-a67e-4909874953c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0, Total Loss: 11.0329, MLM Loss: 10.3090, NSP Loss: 0.7240\n",
      "Epoch 1, Step 10, Total Loss: 11.0875, MLM Loss: 10.3090, NSP Loss: 0.7785\n",
      "Epoch 1, Step 20, Total Loss: 11.0651, MLM Loss: 10.3090, NSP Loss: 0.7562\n",
      "Epoch 1, Step 30, Total Loss: 10.9832, MLM Loss: 10.3090, NSP Loss: 0.6742\n",
      "Epoch 1, Step 40, Total Loss: 11.0547, MLM Loss: 10.3090, NSP Loss: 0.7458\n",
      "Epoch 1, Step 50, Total Loss: 11.1379, MLM Loss: 10.3090, NSP Loss: 0.8290\n",
      "Epoch 1, Step 60, Total Loss: 11.2094, MLM Loss: 10.3090, NSP Loss: 0.9005\n",
      "Epoch 1, Step 70, Total Loss: 11.1596, MLM Loss: 10.3090, NSP Loss: 0.8507\n",
      "Epoch 1, Step 80, Total Loss: 11.0163, MLM Loss: 10.3090, NSP Loss: 0.7074\n",
      "Epoch 1, Step 90, Total Loss: 11.0615, MLM Loss: 10.3090, NSP Loss: 0.7526\n",
      "Epoch 1, Step 100, Total Loss: 11.0120, MLM Loss: 10.3090, NSP Loss: 0.7030\n",
      "Epoch 1, Step 110, Total Loss: 11.0890, MLM Loss: 10.3089, NSP Loss: 0.7801\n",
      "Epoch 1, Step 120, Total Loss: 11.0855, MLM Loss: 10.3089, NSP Loss: 0.7765\n",
      "Epoch 1, Step 130, Total Loss: 11.1073, MLM Loss: 10.3089, NSP Loss: 0.7984\n",
      "Epoch 1, Step 140, Total Loss: 11.1070, MLM Loss: 10.3089, NSP Loss: 0.7981\n",
      "Epoch 1, Step 150, Total Loss: 11.1046, MLM Loss: 10.3089, NSP Loss: 0.7956\n",
      "Epoch 1, Step 160, Total Loss: 10.9980, MLM Loss: 10.3089, NSP Loss: 0.6891\n",
      "Epoch 1, Step 170, Total Loss: 11.0680, MLM Loss: 10.3089, NSP Loss: 0.7591\n",
      "Epoch 1, Step 180, Total Loss: 11.0447, MLM Loss: 10.3089, NSP Loss: 0.7357\n",
      "Epoch 1, Step 190, Total Loss: 11.1043, MLM Loss: 10.3089, NSP Loss: 0.7953\n",
      "Epoch 1, Step 200, Total Loss: 11.0274, MLM Loss: 10.3089, NSP Loss: 0.7184\n",
      "Epoch 1, Step 210, Total Loss: 10.9339, MLM Loss: 10.3089, NSP Loss: 0.6249\n",
      "Epoch 1, Step 220, Total Loss: 11.1278, MLM Loss: 10.3089, NSP Loss: 0.8189\n",
      "Epoch 1, Step 230, Total Loss: 11.0738, MLM Loss: 10.3089, NSP Loss: 0.7648\n",
      "Epoch 1, Step 240, Total Loss: 11.1705, MLM Loss: 10.3089, NSP Loss: 0.8616\n",
      "Epoch 1, Step 250, Total Loss: 11.0642, MLM Loss: 10.3089, NSP Loss: 0.7553\n",
      "Epoch 1, Step 260, Total Loss: 11.2062, MLM Loss: 10.3089, NSP Loss: 0.8973\n",
      "Epoch 1, Step 270, Total Loss: 10.9137, MLM Loss: 10.3089, NSP Loss: 0.6048\n",
      "Epoch 1, Step 280, Total Loss: 10.9877, MLM Loss: 10.3089, NSP Loss: 0.6788\n",
      "Epoch 1, Step 290, Total Loss: 10.9139, MLM Loss: 10.3089, NSP Loss: 0.6049\n",
      "Epoch 1, Step 300, Total Loss: 11.0796, MLM Loss: 10.3089, NSP Loss: 0.7707\n",
      "Epoch 1, Step 310, Total Loss: 11.0568, MLM Loss: 10.3089, NSP Loss: 0.7479\n",
      "Epoch 1, Step 320, Total Loss: 11.0881, MLM Loss: 10.3089, NSP Loss: 0.7792\n",
      "Epoch 1, Step 330, Total Loss: 10.9293, MLM Loss: 10.3089, NSP Loss: 0.6204\n",
      "Epoch 1, Step 340, Total Loss: 11.1032, MLM Loss: 10.3089, NSP Loss: 0.7942\n",
      "Epoch 1, Step 350, Total Loss: 11.0516, MLM Loss: 10.3089, NSP Loss: 0.7427\n",
      "Epoch 1, Step 360, Total Loss: 11.0182, MLM Loss: 10.3089, NSP Loss: 0.7092\n",
      "Epoch 1, Step 370, Total Loss: 11.0871, MLM Loss: 10.3089, NSP Loss: 0.7781\n",
      "Epoch 1, Step 380, Total Loss: 10.9865, MLM Loss: 10.3089, NSP Loss: 0.6775\n",
      "Epoch 1, Step 390, Total Loss: 10.9752, MLM Loss: 10.3089, NSP Loss: 0.6663\n",
      "Epoch 1, Step 400, Total Loss: 11.1608, MLM Loss: 10.3089, NSP Loss: 0.8518\n",
      "Epoch 1, Step 410, Total Loss: 10.9989, MLM Loss: 10.3089, NSP Loss: 0.6900\n",
      "Epoch 1, Step 420, Total Loss: 10.9988, MLM Loss: 10.3089, NSP Loss: 0.6898\n",
      "Epoch 1, Step 430, Total Loss: 11.1358, MLM Loss: 10.3089, NSP Loss: 0.8268\n",
      "Epoch 1, Step 440, Total Loss: 11.0617, MLM Loss: 10.3089, NSP Loss: 0.7528\n",
      "Epoch 1, Step 450, Total Loss: 11.0223, MLM Loss: 10.3089, NSP Loss: 0.7134\n",
      "Epoch 1, Step 460, Total Loss: 11.0444, MLM Loss: 10.3089, NSP Loss: 0.7355\n",
      "Epoch 1, Step 470, Total Loss: 10.9692, MLM Loss: 10.3089, NSP Loss: 0.6603\n",
      "Epoch 1, Step 480, Total Loss: 11.0644, MLM Loss: 10.3089, NSP Loss: 0.7555\n",
      "Epoch 1, Step 490, Total Loss: 11.0689, MLM Loss: 10.3089, NSP Loss: 0.7600\n",
      "Epoch 1, Step 500, Total Loss: 11.0517, MLM Loss: 10.3089, NSP Loss: 0.7428\n",
      "Epoch 1, Step 510, Total Loss: 11.0791, MLM Loss: 10.3089, NSP Loss: 0.7702\n",
      "Epoch 1, Step 520, Total Loss: 10.9969, MLM Loss: 10.3089, NSP Loss: 0.6880\n",
      "Epoch 1, Step 530, Total Loss: 11.0088, MLM Loss: 10.3089, NSP Loss: 0.6999\n",
      "Epoch 1, Step 540, Total Loss: 11.0130, MLM Loss: 10.3089, NSP Loss: 0.7042\n",
      "Epoch 1, Step 550, Total Loss: 11.0350, MLM Loss: 10.3089, NSP Loss: 0.7262\n",
      "Epoch 1, Step 560, Total Loss: 10.9638, MLM Loss: 10.3089, NSP Loss: 0.6549\n",
      "Epoch 1, Step 570, Total Loss: 11.0500, MLM Loss: 10.3089, NSP Loss: 0.7412\n",
      "Epoch 1, Step 580, Total Loss: 11.0538, MLM Loss: 10.3089, NSP Loss: 0.7449\n",
      "Epoch 1, Step 590, Total Loss: 11.0548, MLM Loss: 10.3089, NSP Loss: 0.7459\n",
      "Epoch 1, Step 600, Total Loss: 10.9581, MLM Loss: 10.3089, NSP Loss: 0.6492\n",
      "Epoch 1, Step 610, Total Loss: 10.9503, MLM Loss: 10.3089, NSP Loss: 0.6414\n",
      "Epoch 1, Step 620, Total Loss: 11.0426, MLM Loss: 10.3088, NSP Loss: 0.7337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-12 18:16:21.953513: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Step 0, Total Loss: 11.0036, MLM Loss: 10.3088, NSP Loss: 0.6948\n",
      "Epoch 2, Step 10, Total Loss: 10.9292, MLM Loss: 10.3088, NSP Loss: 0.6204\n",
      "Epoch 2, Step 20, Total Loss: 11.0623, MLM Loss: 10.3088, NSP Loss: 0.7535\n",
      "Epoch 2, Step 30, Total Loss: 11.0720, MLM Loss: 10.3087, NSP Loss: 0.7633\n",
      "Epoch 2, Step 40, Total Loss: 10.9847, MLM Loss: 10.3088, NSP Loss: 0.6759\n",
      "Epoch 2, Step 50, Total Loss: 10.9817, MLM Loss: 10.3087, NSP Loss: 0.6730\n",
      "Epoch 2, Step 60, Total Loss: 11.0598, MLM Loss: 10.3087, NSP Loss: 0.7510\n",
      "Epoch 2, Step 70, Total Loss: 10.9881, MLM Loss: 10.3087, NSP Loss: 0.6794\n",
      "Epoch 2, Step 80, Total Loss: 11.0788, MLM Loss: 10.3086, NSP Loss: 0.7701\n",
      "Epoch 2, Step 90, Total Loss: 11.0113, MLM Loss: 10.3086, NSP Loss: 0.7026\n",
      "Epoch 2, Step 100, Total Loss: 11.0149, MLM Loss: 10.3086, NSP Loss: 0.7062\n",
      "Epoch 2, Step 110, Total Loss: 11.1322, MLM Loss: 10.3085, NSP Loss: 0.8236\n",
      "Epoch 2, Step 120, Total Loss: 10.9869, MLM Loss: 10.3085, NSP Loss: 0.6784\n",
      "Epoch 2, Step 130, Total Loss: 10.9497, MLM Loss: 10.3084, NSP Loss: 0.6413\n",
      "Epoch 2, Step 140, Total Loss: 10.9953, MLM Loss: 10.3083, NSP Loss: 0.6870\n",
      "Epoch 2, Step 150, Total Loss: 11.0428, MLM Loss: 10.3083, NSP Loss: 0.7346\n",
      "Epoch 2, Step 160, Total Loss: 11.0303, MLM Loss: 10.3080, NSP Loss: 0.7223\n",
      "Epoch 2, Step 170, Total Loss: 11.0163, MLM Loss: 10.3081, NSP Loss: 0.7082\n",
      "Epoch 2, Step 180, Total Loss: 11.0238, MLM Loss: 10.3080, NSP Loss: 0.7157\n",
      "Epoch 2, Step 190, Total Loss: 10.9171, MLM Loss: 10.3080, NSP Loss: 0.6091\n",
      "Epoch 2, Step 200, Total Loss: 11.0049, MLM Loss: 10.3076, NSP Loss: 0.6973\n",
      "Epoch 2, Step 210, Total Loss: 11.0074, MLM Loss: 10.3071, NSP Loss: 0.7002\n",
      "Epoch 2, Step 220, Total Loss: 11.0259, MLM Loss: 10.3069, NSP Loss: 0.7190\n",
      "Epoch 2, Step 230, Total Loss: 11.0738, MLM Loss: 10.3070, NSP Loss: 0.7668\n",
      "Epoch 2, Step 240, Total Loss: 10.9660, MLM Loss: 10.3066, NSP Loss: 0.6594\n",
      "Epoch 2, Step 250, Total Loss: 11.0027, MLM Loss: 10.3064, NSP Loss: 0.6963\n",
      "Epoch 2, Step 260, Total Loss: 11.1325, MLM Loss: 10.3053, NSP Loss: 0.8272\n",
      "Epoch 2, Step 270, Total Loss: 10.9468, MLM Loss: 10.3053, NSP Loss: 0.6415\n",
      "Epoch 2, Step 280, Total Loss: 11.0030, MLM Loss: 10.3041, NSP Loss: 0.6989\n",
      "Epoch 2, Step 290, Total Loss: 11.0061, MLM Loss: 10.3037, NSP Loss: 0.7024\n",
      "Epoch 2, Step 300, Total Loss: 11.0518, MLM Loss: 10.3029, NSP Loss: 0.7489\n",
      "Epoch 2, Step 310, Total Loss: 11.1253, MLM Loss: 10.3045, NSP Loss: 0.8207\n",
      "Epoch 2, Step 320, Total Loss: 10.9808, MLM Loss: 10.3021, NSP Loss: 0.6787\n",
      "Epoch 2, Step 330, Total Loss: 10.9593, MLM Loss: 10.2999, NSP Loss: 0.6595\n",
      "Epoch 2, Step 340, Total Loss: 10.9825, MLM Loss: 10.2968, NSP Loss: 0.6856\n",
      "Epoch 2, Step 350, Total Loss: 11.0242, MLM Loss: 10.2984, NSP Loss: 0.7259\n",
      "Epoch 2, Step 360, Total Loss: 11.0186, MLM Loss: 10.2967, NSP Loss: 0.7219\n",
      "Epoch 2, Step 370, Total Loss: 11.0294, MLM Loss: 10.2934, NSP Loss: 0.7360\n",
      "Epoch 2, Step 380, Total Loss: 11.0346, MLM Loss: 10.2868, NSP Loss: 0.7478\n",
      "Epoch 2, Step 390, Total Loss: 10.9668, MLM Loss: 10.2828, NSP Loss: 0.6840\n",
      "Epoch 2, Step 400, Total Loss: 10.9713, MLM Loss: 10.2830, NSP Loss: 0.6882\n",
      "Epoch 2, Step 410, Total Loss: 10.8833, MLM Loss: 10.2805, NSP Loss: 0.6028\n",
      "Epoch 2, Step 420, Total Loss: 11.0774, MLM Loss: 10.2715, NSP Loss: 0.8059\n",
      "Epoch 2, Step 430, Total Loss: 11.0091, MLM Loss: 10.2854, NSP Loss: 0.7237\n",
      "Epoch 2, Step 440, Total Loss: 10.9285, MLM Loss: 10.2796, NSP Loss: 0.6489\n",
      "Epoch 2, Step 450, Total Loss: 11.0459, MLM Loss: 10.2870, NSP Loss: 0.7589\n",
      "Epoch 2, Step 460, Total Loss: 10.9219, MLM Loss: 10.2767, NSP Loss: 0.6452\n",
      "Epoch 2, Step 470, Total Loss: 11.0034, MLM Loss: 10.2694, NSP Loss: 0.7340\n",
      "Epoch 2, Step 480, Total Loss: 10.9572, MLM Loss: 10.2770, NSP Loss: 0.6802\n",
      "Epoch 2, Step 490, Total Loss: 11.0023, MLM Loss: 10.2757, NSP Loss: 0.7266\n",
      "Epoch 2, Step 500, Total Loss: 10.9840, MLM Loss: 10.2651, NSP Loss: 0.7190\n",
      "Epoch 2, Step 510, Total Loss: 10.9555, MLM Loss: 10.2684, NSP Loss: 0.6871\n",
      "Epoch 2, Step 520, Total Loss: 11.0657, MLM Loss: 10.2735, NSP Loss: 0.7922\n",
      "Epoch 2, Step 530, Total Loss: 11.0366, MLM Loss: 10.2647, NSP Loss: 0.7719\n",
      "Epoch 2, Step 540, Total Loss: 10.9272, MLM Loss: 10.2626, NSP Loss: 0.6647\n",
      "Epoch 2, Step 550, Total Loss: 11.0820, MLM Loss: 10.2542, NSP Loss: 0.8278\n",
      "Epoch 2, Step 560, Total Loss: 10.9919, MLM Loss: 10.2699, NSP Loss: 0.7220\n",
      "Epoch 2, Step 570, Total Loss: 11.0179, MLM Loss: 10.2509, NSP Loss: 0.7670\n",
      "Epoch 2, Step 580, Total Loss: 11.0619, MLM Loss: 10.2680, NSP Loss: 0.7940\n",
      "Epoch 2, Step 590, Total Loss: 10.9614, MLM Loss: 10.2707, NSP Loss: 0.6906\n",
      "Epoch 2, Step 600, Total Loss: 10.9571, MLM Loss: 10.2636, NSP Loss: 0.6935\n",
      "Epoch 2, Step 610, Total Loss: 11.0292, MLM Loss: 10.2726, NSP Loss: 0.7565\n",
      "Epoch 2, Step 620, Total Loss: 10.9644, MLM Loss: 10.2655, NSP Loss: 0.6989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-12 18:22:21.464438: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Step 0, Total Loss: 10.9019, MLM Loss: 10.2695, NSP Loss: 0.6324\n",
      "Epoch 3, Step 10, Total Loss: 10.9768, MLM Loss: 10.2712, NSP Loss: 0.7056\n",
      "Epoch 3, Step 20, Total Loss: 10.9484, MLM Loss: 10.2651, NSP Loss: 0.6833\n",
      "Epoch 3, Step 30, Total Loss: 10.9837, MLM Loss: 10.2471, NSP Loss: 0.7367\n",
      "Epoch 3, Step 40, Total Loss: 10.9918, MLM Loss: 10.2748, NSP Loss: 0.7170\n",
      "Epoch 3, Step 50, Total Loss: 10.9657, MLM Loss: 10.2524, NSP Loss: 0.7132\n",
      "Epoch 3, Step 60, Total Loss: 11.0250, MLM Loss: 10.2734, NSP Loss: 0.7517\n",
      "Epoch 3, Step 70, Total Loss: 10.9326, MLM Loss: 10.2591, NSP Loss: 0.6735\n",
      "Epoch 3, Step 80, Total Loss: 10.9131, MLM Loss: 10.2483, NSP Loss: 0.6648\n",
      "Epoch 3, Step 90, Total Loss: 10.9206, MLM Loss: 10.2701, NSP Loss: 0.6505\n",
      "Epoch 3, Step 100, Total Loss: 10.9700, MLM Loss: 10.2643, NSP Loss: 0.7057\n",
      "Epoch 3, Step 110, Total Loss: 10.9996, MLM Loss: 10.2656, NSP Loss: 0.7340\n",
      "Epoch 3, Step 120, Total Loss: 10.9620, MLM Loss: 10.2717, NSP Loss: 0.6904\n",
      "Epoch 3, Step 130, Total Loss: 11.0171, MLM Loss: 10.2615, NSP Loss: 0.7556\n",
      "Epoch 3, Step 140, Total Loss: 10.9329, MLM Loss: 10.2453, NSP Loss: 0.6875\n",
      "Epoch 3, Step 150, Total Loss: 10.9487, MLM Loss: 10.2584, NSP Loss: 0.6903\n",
      "Epoch 3, Step 160, Total Loss: 10.9469, MLM Loss: 10.2526, NSP Loss: 0.6943\n",
      "Epoch 3, Step 170, Total Loss: 10.9261, MLM Loss: 10.2617, NSP Loss: 0.6643\n",
      "Epoch 3, Step 180, Total Loss: 10.9333, MLM Loss: 10.2662, NSP Loss: 0.6671\n",
      "Epoch 3, Step 190, Total Loss: 10.9843, MLM Loss: 10.2705, NSP Loss: 0.7138\n",
      "Epoch 3, Step 200, Total Loss: 10.9847, MLM Loss: 10.2549, NSP Loss: 0.7299\n",
      "Epoch 3, Step 210, Total Loss: 10.9032, MLM Loss: 10.2636, NSP Loss: 0.6395\n",
      "Epoch 3, Step 220, Total Loss: 10.9784, MLM Loss: 10.2527, NSP Loss: 0.7257\n",
      "Epoch 3, Step 230, Total Loss: 11.0387, MLM Loss: 10.2650, NSP Loss: 0.7737\n",
      "Epoch 3, Step 240, Total Loss: 10.9764, MLM Loss: 10.2708, NSP Loss: 0.7056\n",
      "Epoch 3, Step 250, Total Loss: 11.0098, MLM Loss: 10.2537, NSP Loss: 0.7561\n",
      "Epoch 3, Step 260, Total Loss: 11.1120, MLM Loss: 10.2623, NSP Loss: 0.8496\n",
      "Epoch 3, Step 270, Total Loss: 10.9147, MLM Loss: 10.2543, NSP Loss: 0.6604\n",
      "Epoch 3, Step 280, Total Loss: 10.9389, MLM Loss: 10.2494, NSP Loss: 0.6895\n",
      "Epoch 3, Step 290, Total Loss: 10.9244, MLM Loss: 10.2634, NSP Loss: 0.6609\n",
      "Epoch 3, Step 300, Total Loss: 10.9279, MLM Loss: 10.2485, NSP Loss: 0.6794\n",
      "Epoch 3, Step 310, Total Loss: 10.9774, MLM Loss: 10.2765, NSP Loss: 0.7009\n",
      "Epoch 3, Step 320, Total Loss: 10.8862, MLM Loss: 10.2575, NSP Loss: 0.6288\n",
      "Epoch 3, Step 330, Total Loss: 10.9334, MLM Loss: 10.2631, NSP Loss: 0.6703\n",
      "Epoch 3, Step 340, Total Loss: 10.9771, MLM Loss: 10.2589, NSP Loss: 0.7182\n",
      "Epoch 3, Step 350, Total Loss: 10.9391, MLM Loss: 10.2593, NSP Loss: 0.6798\n",
      "Epoch 3, Step 360, Total Loss: 10.9336, MLM Loss: 10.2635, NSP Loss: 0.6701\n",
      "Epoch 3, Step 370, Total Loss: 10.9689, MLM Loss: 10.2516, NSP Loss: 0.7172\n",
      "Epoch 3, Step 380, Total Loss: 11.0232, MLM Loss: 10.2485, NSP Loss: 0.7748\n",
      "Epoch 3, Step 390, Total Loss: 10.9455, MLM Loss: 10.2370, NSP Loss: 0.7084\n",
      "Epoch 3, Step 400, Total Loss: 10.9446, MLM Loss: 10.2464, NSP Loss: 0.6982\n",
      "Epoch 3, Step 410, Total Loss: 10.9063, MLM Loss: 10.2493, NSP Loss: 0.6570\n",
      "Epoch 3, Step 420, Total Loss: 10.9082, MLM Loss: 10.2330, NSP Loss: 0.6752\n",
      "Epoch 3, Step 430, Total Loss: 11.0506, MLM Loss: 10.2660, NSP Loss: 0.7846\n",
      "Epoch 3, Step 440, Total Loss: 10.9346, MLM Loss: 10.2623, NSP Loss: 0.6723\n",
      "Epoch 3, Step 450, Total Loss: 11.0613, MLM Loss: 10.2725, NSP Loss: 0.7888\n",
      "Epoch 3, Step 460, Total Loss: 10.9621, MLM Loss: 10.2548, NSP Loss: 0.7073\n",
      "Epoch 3, Step 470, Total Loss: 10.9573, MLM Loss: 10.2580, NSP Loss: 0.6993\n",
      "Epoch 3, Step 480, Total Loss: 11.0402, MLM Loss: 10.2549, NSP Loss: 0.7853\n",
      "Epoch 3, Step 490, Total Loss: 10.9425, MLM Loss: 10.2563, NSP Loss: 0.6862\n",
      "Epoch 3, Step 500, Total Loss: 10.9839, MLM Loss: 10.2459, NSP Loss: 0.7379\n",
      "Epoch 3, Step 510, Total Loss: 10.8951, MLM Loss: 10.2483, NSP Loss: 0.6468\n",
      "Epoch 3, Step 520, Total Loss: 10.9914, MLM Loss: 10.2636, NSP Loss: 0.7279\n",
      "Epoch 3, Step 530, Total Loss: 10.9257, MLM Loss: 10.2518, NSP Loss: 0.6739\n",
      "Epoch 3, Step 540, Total Loss: 10.8916, MLM Loss: 10.2534, NSP Loss: 0.6382\n",
      "Epoch 3, Step 550, Total Loss: 10.9290, MLM Loss: 10.2350, NSP Loss: 0.6939\n",
      "Epoch 3, Step 560, Total Loss: 10.9207, MLM Loss: 10.2649, NSP Loss: 0.6558\n",
      "Epoch 3, Step 570, Total Loss: 10.9277, MLM Loss: 10.2418, NSP Loss: 0.6859\n",
      "Epoch 3, Step 580, Total Loss: 11.1227, MLM Loss: 10.2626, NSP Loss: 0.8601\n",
      "Epoch 3, Step 590, Total Loss: 10.9641, MLM Loss: 10.2666, NSP Loss: 0.6976\n",
      "Epoch 3, Step 600, Total Loss: 10.9998, MLM Loss: 10.2558, NSP Loss: 0.7439\n",
      "Epoch 3, Step 610, Total Loss: 10.9327, MLM Loss: 10.2674, NSP Loss: 0.6652\n",
      "Epoch 3, Step 620, Total Loss: 10.9683, MLM Loss: 10.2573, NSP Loss: 0.7109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-12 18:28:38.523818: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss_object_mlm = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, \n",
    "                                                                reduction=tf.keras.losses.Reduction.NONE)\n",
    "loss_object_nsp = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "\n",
    "def compute_mlm_loss(masked_positions, masked_labels, logits):\n",
    "    # Gather the logits at the masked positions\n",
    "    masked_logits = tf.gather(logits, masked_positions, batch_dims=1, axis=1)\n",
    "    \n",
    "    # Ensure that the masked_labels used here are the correct length and match the number of masked_positions\n",
    "    mlm_loss = tf.keras.losses.sparse_categorical_crossentropy(masked_labels, masked_logits, from_logits=True)\n",
    "\n",
    "    # Reduce mean across batches if needed or sum as appropriate\n",
    "    return tf.reduce_mean(mlm_loss)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = bert_model(inputs, training=True)  # Predictions will have 'mlm_output' and 'nsp_output'\n",
    "        # Compute the MLM loss using the positions and labels\n",
    "        loss_mlm = compute_mlm_loss(labels['masked_lm_positions'], \n",
    "                                    labels['mlm_labels'], predictions['mlm_output'])\n",
    "        # NSP loss remains the same\n",
    "        loss_nsp = loss_object_nsp(labels['nsp_labels'], predictions['nsp_output'])\n",
    "        total_loss = loss_mlm + loss_nsp\n",
    "\n",
    "    gradients = tape.gradient(total_loss, bert_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, bert_model.trainable_variables))\n",
    "    return total_loss, loss_mlm, loss_nsp\n",
    "\n",
    "# Ensure that your dataset loading function is correctly parsing and returning 'masked_lm_positions' and 'masked_lm_labels'\n",
    "\n",
    "batch_size = 16\n",
    "train_dataset = load_dataset('output.tfrecord', batch_size)\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    step = 0\n",
    "    for inputs, labels in train_dataset:\n",
    "        loss_values = train_step(inputs, labels)\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, Step {step}, Total Loss: {loss_values[0].numpy():.4f}, MLM Loss: {loss_values[1].numpy():.4f}, NSP Loss: {loss_values[2].numpy():.4f}\")\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163078fe-11be-4884-8047-57c33ed6e995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513b7327-059e-4fa8-bf8a-ac238cc62c85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87fabc4-8221-48b3-92e2-b2f5906e321d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422c3408-9a6a-4958-a2c8-b1c1f05c5c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e54f7b-6cc7-4b0b-9904-ecff00f33890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4409844-790b-4695-946a-51f6127b8a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Human:\n",
    "#     def __init__(self, name, legs=2):\n",
    "#         self.name = name\n",
    "#         self.legs = legs\n",
    "\n",
    "# class MaleHuman(Human):\n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         super(MaleHuman, self).__init__(*args, **kwargs)\n",
    "#         # self.beard = beard\n",
    "\n",
    "# m1 = MaleHuman('Bhujay')\n",
    "# m1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848e17e7-f516-4e0e-85de-66c66c143682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed3be30-03bc-46fa-86d3-c2c719bfb359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.padded_batch(\n",
    "    #     batch_size, \n",
    "    #     padded_shapes={'input_ids': [None], \n",
    "    #                    'segment_ids': [128], \n",
    "    #                    'masked_lm_positions': [None], \n",
    "    #                    'masked_lm_labels': [None], \n",
    "    #                    'next_sentence_labels': []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa8e482-617d-43f2-9ea9-63076c6546fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# try:\n",
    "#     for idx, (inputs, labels) in enumerate(train_dataset):\n",
    "#         print(f\"instance {idx + 1}:\")\n",
    "#         # print(f\"Inputs: {inputs}\")\n",
    "#         # print(f\"Labels: {labels}\")\n",
    "#         if idx == 10:  # Limit to 2 batches for demonstration\n",
    "#             break\n",
    "# except Exception as e:\n",
    "#     print(f\"Error while processing the dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da8a988-88a4-419f-aaac-03af4ace604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_dataset(filepath):\n",
    "#     # dataset = load_dataset(filepath, batch_size=16)  # Using batch size of 1 for simplicity\n",
    "#     try:\n",
    "#         for i, (inputs, labels) in enumerate(train_dataset):        \n",
    "#             print(f\"Successfully parsed entry {i+1}\")        \n",
    "#             if i == 100:  # Optionally limit the number of entries to check\n",
    "#                 break\n",
    "#     except Exception as e:\n",
    "#             print(f'record: {i}  , Err: {e}')\n",
    "\n",
    "# test_dataset('output.tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3488dd32-cf4e-46b5-88d9-880fae1b1069",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def parse_tfrecord(serialized_example):\n",
    "#     feature_description = {\n",
    "#         'input_ids': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "#         'segment_ids': tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "#         'masked_lm_positions': tf.io.VarLenFeature(tf.int64),\n",
    "#         'masked_lm_labels': tf.io.VarLenFeature(tf.int64),\n",
    "#         'next_sentence_labels': tf.io.FixedLenFeature([], tf.int64)\n",
    "#     }\n",
    "#     try:\n",
    "#         example = tf.io.parse_single_example(serialized_example, feature_description)    \n",
    "#         input_ids = tf.cast(example['input_ids'], tf.int32)\n",
    "#         segment_ids = tf.cast(example['segment_ids'], tf.int32)\n",
    "#         # segment_ids = tf.sparse.to_dense(segment_ids, default_value=0)  # Default to 0\n",
    "#         # segment_ids = tf.reshape(segment_ids, [max_seq_length]) \n",
    "#         masked_lm_positions = tf.sparse.to_dense(example['masked_lm_positions'])\n",
    "#         masked_lm_labels = tf.sparse.to_dense(example['masked_lm_labels'])   \n",
    "#         next_sentence_labels = tf.cast(example['next_sentence_labels'], tf.int32)\n",
    "#         inputs = {'input_ids': input_ids, 'segment_ids': segment_ids, }\n",
    "#         labels = {'masked_lm_positions': masked_lm_positions, \n",
    "#                   'mlm_labels': masked_lm_labels, 'nsp_labels': next_sentence_labels}\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to parse example: {e}\")\n",
    "#         # Return dummy/default data to allow pipeline to continue\n",
    "#         inputs = {\n",
    "#             'input_ids': tf.constant([0]*args.max_seq_length, dtype=tf.int32),\n",
    "#             'segment_ids': tf.constant([0]*args.max_seq_length, dtype=tf.int32)\n",
    "#             }\n",
    "#         labels = {\n",
    "#             'masked_lm_positions': tf.constant([-1]*20, dtype=tf.int32),  # Adjust 20 to your max_predictions_per_seq\n",
    "#             'masked_lm_labels': tf.constant([-1]*20, dtype=tf.int32),\n",
    "#             'nsp_labels': tf.constant([0], dtype=tf.int32)\n",
    "#             } \n",
    "#         return (None, None)\n",
    "#     return inputs, labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def load_dataset(filepath, batch_size):\n",
    "#     raw_dataset = tf.data.TFRecordDataset(filepath)\n",
    "#     parsed_dataset = raw_dataset.map(parse_tfrecord)\n",
    "#     filtered_dataset = parsed_dataset.filter(lambda x: x[0] is not None and x[1] is not None)\n",
    "#     batched_dataset = filtered_dataset.batch(batch_size)\n",
    "#     return batched_dataset\n",
    "# # Usage\n",
    "# batch_size = 16\n",
    "# train_dataset = load_dataset('output.tfrecord', batch_size)\n",
    "\n",
    "# single_test_instance = iter(train_dataset.take(1)).next()\n",
    "# single_input_tuple = single_test_instance[0]['input_ids'], single_test_instance[0]['segment_ids']\n",
    "# print(single_test_instance)\n",
    "# print()\n",
    "# # print(single_input_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b52b52f-f946-4704-9bc0-51cc52a8ba95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def _parse_function(proto):\n",
    "#     # Define your tfrecord again. It must be the same as the one used for saving your data.\n",
    "#     feature_description = {\n",
    "#         'input_ids': tf.io.FixedLenFeature([128], tf.int64),  # Assuming input_ids are of length 128\n",
    "#         'segment_ids': tf.io.FixedLenFeature([128], tf.int64),  # Assuming segment_ids are of length 128\n",
    "#         'masked_lm_positions': tf.io.VarLenFeature(tf.int64),\n",
    "#         'masked_lm_labels': tf.io.VarLenFeature(tf.int64),\n",
    "#         'next_sentence_labels': tf.io.FixedLenFeature([], tf.int64),\n",
    "#     }\n",
    "\n",
    "#     # Load one example\n",
    "#     parsed_features = tf.io.parse_single_example(proto, feature_description)\n",
    "    \n",
    "#     # Turn your sparse array into a dense array with default values as 0\n",
    "#     parsed_features['masked_lm_positions'] = tf.sparse.to_dense(parsed_features['masked_lm_positions'], default_value=0)\n",
    "#     parsed_features['masked_lm_labels'] = tf.sparse.to_dense(parsed_features['masked_lm_labels'], default_value=0)\n",
    "\n",
    "#     return parsed_features\n",
    "# # Read the TFRecord file\n",
    "# def load_dataset(file_path):\n",
    "#     dataset = tf.data.TFRecordDataset(file_path)\n",
    "#     dataset = dataset.map(_parse_function)  # Parse the record into tensors.\n",
    "#     return dataset\n",
    "\n",
    "# # Path to the TFRecord file\n",
    "# tfrecord_file_path = 'output.tfrecord'\n",
    "\n",
    "# # Load the dataset\n",
    "# parsed_dataset = load_dataset(tfrecord_file_path)\n",
    "# # Display a few examples from the dataset\n",
    "# for parsed_record in parsed_dataset.take(2):  # Only take first 5 examples\n",
    "#     print('Input IDs:', parsed_record['input_ids'].numpy())\n",
    "#     print('Segment IDs:', parsed_record['segment_ids'].numpy())\n",
    "#     print('Masked LM Positions:', parsed_record['masked_lm_positions'].numpy())\n",
    "#     print('Masked LM Labels:', parsed_record['masked_lm_labels'].numpy())\n",
    "#     print('Next Sentence Label:', parsed_record['next_sentence_labels'].numpy())\n",
    "#     print('---')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39969389-584a-4602-8847-1472a4155e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_tfrecord(serialized_example):\n",
    "#     feature_description = {\n",
    "#         'input_ids': tf.io.FixedLenFeature([128], tf.int64),\n",
    "#         'segment_ids': tf.io.FixedLenFeature([128], tf.int64),\n",
    "#         'masked_lm_positions': tf.io.VarLenFeature(tf.int64),\n",
    "#         'masked_lm_labels': tf.io.VarLenFeature(tf.int64),\n",
    "#         'next_sentence_labels': tf.io.FixedLenFeature([], tf.int64)\n",
    "#     }\n",
    "#     example = tf.io.parse_single_example(serialized_example, feature_description)\n",
    "\n",
    "#     input_ids = tf.cast(example['input_ids'], tf.int32)\n",
    "#     segment_ids = tf.cast(example['segment_ids'], tf.int32)\n",
    "#     masked_lm_positions = tf.sparse.to_dense(example['masked_lm_positions'])\n",
    "#     masked_lm_labels = tf.sparse.to_dense(example['masked_lm_labels'])\n",
    "#     next_sentence_labels = tf.cast(example['next_sentence_labels'], tf.int32)\n",
    "\n",
    "#     inputs = {'input_ids': input_ids, 'segment_ids': segment_ids}\n",
    "#     labels = {'mlm_output': masked_lm_labels, 'nsp_output': next_sentence_labels}\n",
    "\n",
    "#     return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4058d182-6444-4054-87c9-81511b0ebbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = Adam(learning_rate=2e-5)\n",
    "# loss = {\n",
    "#     'mlm_output': SparseCategoricalCrossentropy(from_logits=True),\n",
    "#     'nsp_output': BinaryCrossentropy(from_logits=True)\n",
    "# }\n",
    "# metrics = {\n",
    "#     'mlm_output': 'accuracy',\n",
    "#     'nsp_output': 'accuracy'\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# def masked_sparse_categorical_crossentropy(y_true, y_pred):\n",
    "#     # Create a mask to ignore `-1` in labels for loss calculation\n",
    "#     mask = tf.cast(tf.not_equal(y_true, -1), tf.float32)\n",
    "#     # Compute sparse categorical crossentropy loss\n",
    "#     loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
    "#     # Apply the mask\n",
    "#     loss *= mask\n",
    "#     # Calculate mean loss only over non-masked elements\n",
    "#     return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "# # Ensure the custom loss function is used when compiling the model\n",
    "# bert_model.compile(\n",
    "#     optimizer=optimizer,\n",
    "#     loss={'mlm_output': masked_sparse_categorical_crossentropy, 'nsp_output': BinaryCrossentropy(from_logits=True)},\n",
    "#     metrics=metrics\n",
    "# )\n",
    "# # epochs = 3 \n",
    "# # bert_model.fit(train_dataset, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bfe004-16cb-4a1d-9991-fcb75ff85137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SmallBERT(Model):\n",
    "#     def __init__(self, vocab_size, num_layers, d_model, num_heads, dff, max_pos=512, rate=0.1):\n",
    "#         super(SmallBERT, self).__init__()\n",
    "#         self.token_embedding = Embedding(vocab_size, d_model)\n",
    "#         self.position_embedding = Embedding(max_pos, d_model)\n",
    "#         self.segment_embedding = Embedding(2, d_model)  # Only 2 segments assumed\n",
    "        \n",
    "#         self.enc_layers = [TransformerEncoderV3(num_layers=1, d_model=d_model, num_heads=num_heads, dff=dff,\n",
    "#                                                 vocab_size=vocab_size, max_pos=max_pos) for _ in range(num_layers)]\n",
    "        \n",
    "#         self.dropout = Dropout(rate)\n",
    "#         self.final_layer = Dense(vocab_size)  # Prediction layer for MLM\n",
    "#         self.nsp_classifier = Dense(2, activation='softmax')  # NSP output\n",
    "\n",
    "#     def call(self, input_ids, segment_ids, training=False):\n",
    "#         seq_length = tf.shape(input_ids)[1]\n",
    "#         position_ids = tf.range(seq_length)\n",
    "        \n",
    "#         x = self.token_embedding(input_ids) + self.position_embedding(position_ids) + self.segment_embedding(segment_ids)\n",
    "#         x = self.dropout(x, training=training)\n",
    "        \n",
    "#         for encoder in self.enc_layers:\n",
    "#             x = encoder(x)\n",
    "        \n",
    "#         logits = self.final_layer(x)\n",
    "#         pooled_output = self.dropout(sequence_output[:, 0, :], training=training)  # Use the output of the [CLS] token\n",
    "#         nsp_output = self.nsp_classifier(pooled_output)\n",
    "#         return logits, nsp_output  \n",
    "\n",
    "# vocab_size = 10000  # Smaller vocabulary size for simplicity\n",
    "# num_layers = 2  # Fewer layers\n",
    "# d_model = 128  # Smaller dimensionality\n",
    "# num_heads = 4\n",
    "# dff = 512\n",
    "# small_bert = SmallBERT(vocab_size, num_layers, d_model, num_heads, dff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e381b9-5878-4250-b99e-8b853640fc3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3dfa73-323e-4930-b9c2-c1e2080a8acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7227c722-9c49-4bdb-926e-7091b49e5f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69ca148-1c24-45e2-904c-94a6c99df05b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a51f6c-f36c-4bf1-97cb-948e5bebaef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d198953-53b2-47b1-bb1a-9d243bf52c02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
